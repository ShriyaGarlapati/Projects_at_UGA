{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Pg-GpqwF2mIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "gdO7YQ7-5S7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3iEUG4gbSwBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade torch torchvision opacus\n"
      ],
      "metadata": {
        "id": "BTsENNGtTR9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18821ed-16c0-4aa4-85b0-9143196e6402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Collecting opacus\n",
            "  Downloading opacus-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.13.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading opacus-1.5.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.9/239.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opacus\n",
            "Successfully installed opacus-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "YfUMIoLF5XPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAOWUPjKqAra"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MiUOM_W9Fd6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
        "100%|██████████| 170M/170M [00:08<00:00, 20.4MB/s]\n",
        "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
        "Files already downloaded and verified\n",
        "Epoch: 1, Train Loss: 1.5723, Train Accuracy: 41.47%\n",
        "Epoch: 1, Test Loss: 1.3598, Test Accuracy: 50.90%\n",
        "End of Epoch 1:\n",
        "Train Loss: 1.5723, Train Accuracy: 41.47%\n",
        "Test Loss: 1.3598, Test Accuracy: 50.90%\n",
        "Epoch: 2, Train Loss: 1.0846, Train Accuracy: 61.08%\n",
        "Epoch: 2, Test Loss: 1.2472, Test Accuracy: 58.09%\n",
        "End of Epoch 2:\n",
        "Train Loss: 1.0846, Train Accuracy: 61.08%\n",
        "Test Loss: 1.2472, Test Accuracy: 58.09%"
      ],
      "metadata": {
        "id": "UHyNuvxH7zRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*DP*- ADDED.   256, 0.1-->no group normalization\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1xqIrqwV5pX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x), inplace=False)  # Explicitly ensure inplace=False\n",
        "        out = F.relu(self.conv2(out), inplace=False)\n",
        "        out = out + self.shortcut(x)  # Avoid inplace modification\n",
        "        return out\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize SGD optimizer from Opacus\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,  # Adjust this value based on your privacy needs\n",
        "    max_grad_norm=1.0,\n",
        "    epochs= 20\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, 20):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}')\n",
        "    # Print statistics for the epoch\n",
        "    print(f'End of Epoch {epoch}:')\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4EcNyEzbHUHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lRGRMcqhkJ6m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEFWfXW2HT9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLTCGYmIFdQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2O3_C4R5iKq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADDED GROUP NORMALIZATION.          256, 0.1"
      ],
      "metadata": {
        "id": "yXy9kloBHUhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)  # Replacing BatchNorm with GroupNorm\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)  # Replacing BatchNorm with GroupNorm\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])  # Replacing BatchNorm with GroupNorm\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize SGD optimizer from Opacus\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,  # Adjust this value based on your privacy needs\n",
        "    max_grad_norm=1.0,\n",
        "    epochs= 20\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, 20):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}')\n",
        "    # Print statistics for the epoch\n",
        "    print(f'End of Epoch {epoch}:')\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qJ2Op3885qzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Privacy Budget: Epsilon = 2.1299\n",
        "End of Epoch 8:\n",
        "Train Loss: 1.8109, Train Accuracy: 32.17%\n",
        "Test Loss: 1.7563, Test Accuracy: 32.31%"
      ],
      "metadata": {
        "id": "x2HForQAc2t2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oa7nNQ0Uq9nV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "driHCKeZDX2X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUOtiz23EUoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increased batch size to 512, 0.1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1tmX8Q_lEVdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn .functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)  # Replacing BatchNorm with GroupNorm\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)  # Replacing BatchNorm with GroupNorm\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])  # Replacing BatchNorm with GroupNorm\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize SGD optimizer from Opacus\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=20,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=0.3,  # Adjust this value based on your privacy needs\n",
        "    max_grad_norm=1.0,     # Set max gradient norm as required\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, 20):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    # Print statistics for the epoch\n",
        "    print(f'End of Epoch {epoch}:')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gpUxrSzAEUc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DVMJT5c6hx0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qxkOvJ3crJGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "BATCH SIZE=256, 0.01"
      ],
      "metadata": {
        "id": "IU9Zw9S5F_K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn .functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)  # Replacing BatchNorm with GroupNorm\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)  # Replacing BatchNorm with GroupNorm\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])  # Replacing BatchNorm with GroupNorm\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize SGD optimizer from Opacus\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=20,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=0.3,  # Adjust this value based on your privacy needs\n",
        "    max_grad_norm=1.0,     # Set max gradient norm as required\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, 20):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    # Print statistics for the epoch\n",
        "    print(f'End of Epoch {epoch}:')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZgYgwttFw4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eLcDDSack0rf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANWPj7xYFw1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7wVR3AFbUqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z07Azk3VbUGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADDED WEIGHT STANDARDIZATION- BATCH SZE: 256, 0.001------>(x,y)"
      ],
      "metadata": {
        "id": "kuZQoHIlFxfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        # Initialize weight_mean and weight_std with the correct dimensions\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))  # (1, in_channels, 1, 1)\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))   # (1, in_channels, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        # Normalize the weights\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        # Apply the learned scale and shift\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "\n",
        "\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Modify the DataLoader to drop the last batch if it's incomplete\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize SGD optimizer from Opacus\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=0.3,  # Adjust this value based on your privacy needs\n",
        "    max_grad_norm=1.0,     # Set max gradient norm as required\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, 50):\n",
        "    torch.cuda.empty_cache()\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    # Print statistics for the epoch\n",
        "    print(f'End of Epoch {epoch}:')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rxWJ3xtkFwy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "l-M9tetffo-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SAME PREVIOUS WEIGHT STANDARDIZATION CODE. BATCH SIZE=128, 0.001\n",
        "\n"
      ],
      "metadata": {
        "id": "KN79xaaizReC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        # Initialize weight_mean and weight_std with the correct dimensions\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))  # (1, in_channels, 1, 1)\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))   # (1, in_channels, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        # Normalize the weights\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        # Apply the learned scale and shift\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "\n",
        "\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Modify the DataLoader to drop the last batch if it's incomplete\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=True)\n",
        "\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize SGD optimizer from Opacus\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=20,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=0.3,  # Adjust this value based on your privacy needs\n",
        "    max_grad_norm=1.0,     # Set max gradient norm as required\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, 20):\n",
        "    torch.cuda.empty_cache()\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    # Print statistics for the epoch\n",
        "    print(f'End of Epoch {epoch}:')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UKAxX-PuwwIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADD PARAMETER AVERAGING"
      ],
      "metadata": {
        "id": "pQlkx-bafzDC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZAHlWeAFwvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADD PARAMETER AVERAGING- lr=0.1"
      ],
      "metadata": {
        "id": "R_hGQcUJzVaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%\\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\\n\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}:\\n')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}\\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\\n')\n",
        "\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "piW4kAJAFwl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VeaIkh6KsYKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rB5LpbQL8h0g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aiWzolQmzlEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARAMETER AVERAGING- LR=0.01"
      ],
      "metadata": {
        "id": "2CCWT1aXmnG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    # Print statistics for the epoch\n",
        "    print(f'End of Epoch {epoch}:')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OCJwvo5ZzlCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train Loss: 2.1033, Train Accuracy: 20.98%\n",
        "Epoch: 1, Test Loss: 1.9654, Test Accuracy: 26.69%\n",
        "End of Epoch 1:\n",
        "Privacy Budget: Epsilon = 0.4951\n",
        "Train Loss: 2.1033, Train Accuracy: 20.98%\n",
        "Test Loss: 1.9654, Test Accuracy: 26.69%\n",
        "Epoch: 2, Train Loss: 1.8857, Train Accuracy: 29.57%\n",
        "Epoch: 2, Test Loss: 1.8192, Test Accuracy: 31.95%\n",
        "End of Epoch 2:\n",
        "Privacy Budget: Epsilon = 0.6385\n",
        "Train Loss: 1.8857, Train Accuracy: 29.57%\n",
        "Test Loss: 1.8192, Test Accuracy: 31.95%\n",
        "Epoch: 3, Train Loss: 1.7737, Train Accuracy: 33.35%\n",
        "Epoch: 3, Test Loss: 1.7448, Test Accuracy: 34.95%\n",
        "End of Epoch 3:\n",
        "Privacy Budget: Epsilon = 0.7527\n",
        "Train Loss: 1.7737, Train Accuracy: 33.35%\n",
        "Test Loss: 1.7448, Test Accuracy: 34.95%\n",
        "Epoch: 4, Train Loss: 1.7244, Train Accuracy: 34.61%\n",
        "Epoch: 4, Test Loss: 1.7017, Test Accuracy: 35.83%\n",
        "End of Epoch 4:\n",
        "Privacy Budget: Epsilon = 0.8518\n",
        "Train Loss: 1.7244, Train Accuracy: 34.61%\n",
        "Test Loss: 1.7017, Test Accuracy: 35.83%\n",
        "Epoch: 5, Train Loss: 1.6948, Train Accuracy: 35.63%\n",
        "Epoch: 5, Test Loss: 1.6853, Test Accuracy: 37.67%\n",
        "End of Epoch 5:\n",
        "Privacy Budget: Epsilon = 0.9411\n",
        "Train Loss: 1.6948, Train Accuracy: 35.63%\n",
        "Test Loss: 1.6853, Test Accuracy: 37.67%\n",
        "Epoch: 6, Train Loss: 1.6733, Train Accuracy: 37.48%\n",
        "Epoch: 6, Test Loss: 1.6444, Test Accuracy: 39.04%\n",
        "End of Epoch 6:\n",
        "Privacy Budget: Epsilon = 1.0231\n",
        "Train Loss: 1.6733, Train Accuracy: 37.48%\n",
        "Test Loss: 1.6444, Test Accuracy: 39.04%\n",
        "Epoch: 7, Train Loss: 1.6260, Train Accuracy: 39.44%\n",
        "Epoch: 7, Test Loss: 1.6314, Test Accuracy: 39.70%\n",
        "End of Epoch 7:\n",
        "Privacy Budget: Epsilon = 1.0995\n",
        "Train Loss: 1.6260, Train Accuracy: 39.44%\n",
        "Test Loss: 1.6314, Test Accuracy: 39.70%\n",
        "Epoch: 8, Train Loss: 1.6026, Train Accuracy: 40.60%\n",
        "Epoch: 8, Test Loss: 1.5973, Test Accuracy: 40.53%\n",
        "End of Epoch 8:\n",
        "Privacy Budget: Epsilon = 1.1715\n",
        "Train Loss: 1.6026, Train Accuracy: 40.60%\n",
        "Test Loss: 1.5973, Test Accuracy: 40.53%\n",
        "Epoch: 9, Train Loss: 1.5734, Train Accuracy: 41.50%\n",
        "Epoch: 9, Test Loss: 1.5747, Test Accuracy: 41.48%\n",
        "End of Epoch 9:\n",
        "Privacy Budget: Epsilon = 1.2397\n",
        "Train Loss: 1.5734, Train Accuracy: 41.50%\n",
        "Test Loss: 1.5747, Test Accuracy: 41.48%\n",
        "Epoch: 10, Train Loss: 1.5472, Train Accuracy: 42.68%\n",
        "Epoch: 10, Test Loss: 1.5493, Test Accuracy: 42.29%\n",
        "Averaging model parameters...\n",
        "End of Epoch 10:\n",
        "Privacy Budget: Epsilon = 1.3049\n",
        "Train Loss: 1.5472, Train Accuracy: 42.68%\n",
        "Test Loss: 1.5493, Test Accuracy: 42.29%\n",
        "Epoch: 11, Train Loss: 1.5383, Train Accuracy: 43.20%\n",
        "Epoch: 11, Test Loss: 1.5379, Test Accuracy: 43.70%\n",
        "End of Epoch 11:\n",
        "Privacy Budget: Epsilon = 1.3673\n",
        "Train Loss: 1.5383, Train Accuracy: 43.20%\n",
        "Test Loss: 1.5379, Test Accuracy: 43.70%\n",
        "Epoch: 12, Train Loss: 1.5294, Train Accuracy: 43.69%\n",
        "Epoch: 12, Test Loss: 1.5338, Test Accuracy: 43.84%\n",
        "End of Epoch 12:\n",
        "Privacy Budget: Epsilon = 1.4274\n",
        "Train Loss: 1.5294, Train Accuracy: 43.69%\n",
        "Test Loss: 1.5338, Test Accuracy: 43.84%\n",
        "Epoch: 13, Train Loss: 1.5154, Train Accuracy: 44.49%\n",
        "Epoch: 13, Test Loss: 1.5147, Test Accuracy: 44.45%\n",
        "End of Epoch 13:\n",
        "Privacy Budget: Epsilon = 1.4854\n",
        "Train Loss: 1.5154, Train Accuracy: 44.49%\n",
        "Test Loss: 1.5147, Test Accuracy: 44.45%\n",
        "Epoch: 14, Train Loss: 1.5034, Train Accuracy: 45.14%\n",
        "Epoch: 14, Test Loss: 1.5295, Test Accuracy: 44.07%\n",
        "End of Epoch 14:\n",
        "Privacy Budget: Epsilon = 1.5415\n",
        "Train Loss: 1.5034, Train Accuracy: 45.14%\n",
        "Test Loss: 1.5295, Test Accuracy: 44.07%\n",
        "Epoch: 15, Train Loss: 1.5005, Train Accuracy: 45.34%\n",
        "Epoch: 15, Test Loss: 1.4931, Test Accuracy: 45.64%\n",
        "End of Epoch 15:\n",
        "Privacy Budget: Epsilon = 1.5959\n",
        "Train Loss: 1.5005, Train Accuracy: 45.34%\n",
        "Test Loss: 1.4931, Test Accuracy: 45.64%\n",
        "Epoch: 16, Train Loss: 1.4879, Train Accuracy: 45.65%\n",
        "Epoch: 16, Test Loss: 1.4916, Test Accuracy: 45.84%\n",
        "End of Epoch 16:\n",
        "Privacy Budget: Epsilon = 1.6489\n",
        "Train Loss: 1.4879, Train Accuracy: 45.65%\n",
        "Test Loss: 1.4916, Test Accuracy: 45.84%\n",
        "Epoch: 17, Train Loss: 1.4846, Train Accuracy: 45.96%\n",
        "Epoch: 17, Test Loss: 1.4768, Test Accuracy: 45.92%\n",
        "End of Epoch 17:\n",
        "Privacy Budget: Epsilon = 1.7004\n",
        "Train Loss: 1.4846, Train Accuracy: 45.96%\n",
        "Test Loss: 1.4768, Test Accuracy: 45.92%\n",
        "Epoch: 18, Train Loss: 1.4795, Train Accuracy: 46.10%\n",
        "Epoch: 18, Test Loss: 1.4799, Test Accuracy: 46.80%\n",
        "End of Epoch 18:\n",
        "Privacy Budget: Epsilon = 1.7507\n",
        "Train Loss: 1.4795, Train Accuracy: 46.10%\n",
        "Test Loss: 1.4799, Test Accuracy: 46.80%\n",
        "Epoch: 19, Train Loss: 1.4763, Train Accuracy: 46.70%\n",
        "Epoch: 19, Test Loss: 1.4994, Test Accuracy: 46.22%\n",
        "End of Epoch 19:\n",
        "Privacy Budget: Epsilon = 1.7998\n",
        "Train Loss: 1.4763, Train Accuracy: 46.70%\n",
        "Test Loss: 1.4994, Test Accuracy: 46.22%\n",
        "Epoch: 20, Train Loss: 1.4775, Train Accuracy: 46.73%\n",
        "Epoch: 20, Test Loss: 1.4746, Test Accuracy: 46.62%\n",
        "Averaging model parameters...\n",
        "End of Epoch 20:\n",
        "Privacy Budget: Epsilon = 1.8479\n",
        "Train Loss: 1.4775, Train Accuracy: 46.73%\n",
        "Test Loss: 1.4746, Test Accuracy: 46.62%\n",
        "Epoch: 21, Train Loss: 1.4610, Train Accuracy: 47.27%\n",
        "Epoch: 21, Test Loss: 1.4627, Test Accuracy: 46.97%\n",
        "End of Epoch 21:\n",
        "Privacy Budget: Epsilon = 1.8949\n",
        "Train Loss: 1.4610, Train Accuracy: 47.27%\n",
        "Test Loss: 1.4627, Test Accuracy: 46.97%\n",
        "Epoch: 22, Train Loss: 1.4689, Train Accuracy: 47.49%\n",
        "Epoch: 22, Test Loss: 1.4657, Test Accuracy: 47.36%\n",
        "End of Epoch 22:\n",
        "Privacy Budget: Epsilon = 1.9410\n",
        "Train Loss: 1.4689, Train Accuracy: 47.49%\n",
        "Test Loss: 1.4657, Test Accuracy: 47.36%\n",
        "Epoch: 23, Train Loss: 1.4579, Train Accuracy: 48.04%\n",
        "Epoch: 23, Test Loss: 1.4811, Test Accuracy: 48.05%\n",
        "End of Epoch 23:\n",
        "Privacy Budget: Epsilon = 1.9863\n",
        "Train Loss: 1.4579, Train Accuracy: 48.04%\n",
        "Test Loss: 1.4811, Test Accuracy: 48.05%\n",
        "Epoch: 24, Train Loss: 1.4636, Train Accuracy: 47.90%\n",
        "Epoch: 24, Test Loss: 1.4890, Test Accuracy: 46.90%\n",
        "End of Epoch 24:\n",
        "Privacy Budget: Epsilon = 2.0307\n",
        "Train Loss: 1.4636, Train Accuracy: 47.90%\n",
        "Test Loss: 1.4890, Test Accuracy: 46.90%\n",
        "Epoch: 25, Train Loss: 1.4640, Train Accuracy: 48.15%\n",
        "Epoch: 25, Test Loss: 1.4731, Test Accuracy: 48.25%\n",
        "End of Epoch 25:\n",
        "Privacy Budget: Epsilon = 2.0743\n",
        "Train Loss: 1.4640, Train Accuracy: 48.15%\n",
        "Test Loss: 1.4731, Test Accuracy: 48.25%\n",
        "Epoch: 26, Train Loss: 1.4610, Train Accuracy: 48.02%\n",
        "Epoch: 26, Test Loss: 1.4881, Test Accuracy: 48.18%\n",
        "End of Epoch 26:\n",
        "Privacy Budget: Epsilon = 2.1172\n",
        "Train Loss: 1.4610, Train Accuracy: 48.02%\n",
        "Test Loss: 1.4881, Test Accuracy: 48.18%\n",
        "Epoch: 27, Train Loss: 1.4693, Train Accuracy: 48.28%\n",
        "Epoch: 27, Test Loss: 1.4799, Test Accuracy: 48.19%\n",
        "End of Epoch 27:\n",
        "Privacy Budget: Epsilon = 2.1594\n",
        "Train Loss: 1.4693, Train Accuracy: 48.28%\n",
        "Test Loss: 1.4799, Test Accuracy: 48.19%\n",
        "Epoch: 28, Train Loss: 1.4669, Train Accuracy: 48.59%\n",
        "Epoch: 28, Test Loss: 1.4754, Test Accuracy: 48.41%\n",
        "End of Epoch 28:\n",
        "Privacy Budget: Epsilon = 2.2010\n",
        "Train Loss: 1.4669, Train Accuracy: 48.59%\n",
        "Test Loss: 1.4754, Test Accuracy: 48.41%\n",
        "Epoch: 29, Train Loss: 1.4600, Train Accuracy: 48.72%\n",
        "Epoch: 29, Test Loss: 1.4697, Test Accuracy: 48.64%\n",
        "End of Epoch 29:\n",
        "Privacy Budget: Epsilon = 2.2420\n",
        "Train Loss: 1.4600, Train Accuracy: 48.72%\n",
        "Test Loss: 1.4697, Test Accuracy: 48.64%\n",
        "Epoch: 30, Train Loss: 1.4612, Train Accuracy: 49.06%\n",
        "Epoch: 30, Test Loss: 1.4878, Test Accuracy: 48.44%\n",
        "Averaging model parameters...\n",
        "End of Epoch 30:\n",
        "Privacy Budget: Epsilon = 2.2823\n",
        "Train Loss: 1.4612, Train Accuracy: 49.06%\n",
        "Test Loss: 1.4878, Test Accuracy: 48.44%\n",
        "Epoch: 31, Train Loss: 1.4626, Train Accuracy: 49.10%\n",
        "Epoch: 31, Test Loss: 1.4844, Test Accuracy: 49.09%\n",
        "\n",
        "\n",
        "End of Epoch 31:\n",
        "Privacy Budget: Epsilon = 2.3222\n",
        "Train Loss: 1.4626, Train Accuracy: 49.10%\n",
        "Test Loss: 1.4844, Test Accuracy: 49.09%"
      ],
      "metadata": {
        "id": "DFrpxhht20Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzhEBJG4zk_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter averaging+ grouped grad clipp---- grouped_gradient_clipping.py"
      ],
      "metadata": {
        "id": "pV0dUo6fzmGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\\n\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    # Print statistics for the epoch\n",
        "    print(f'End of Epoch {epoch}:\\n')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f}\\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\\n')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "gNxCZyVXzk9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train Loss: 2.1057, Train Accuracy: 20.05%\n",
        "Epoch: 1, Test Loss: 1.9697, Test Accuracy: 26.04%\n",
        "End of Epoch 1:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.4951\n",
        "\n",
        "Train Loss: 2.1057, Train Accuracy: 20.05%\n",
        "\n",
        "Test Loss: 1.9697, Test Accuracy: 26.04%\n",
        "\n",
        "Epoch: 2, Train Loss: 1.9076, Train Accuracy: 27.74%\n",
        "Epoch: 2, Test Loss: 1.8368, Test Accuracy: 30.65%\n",
        "End of Epoch 2:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.6385\n",
        "\n",
        "Train Loss: 1.9076, Train Accuracy: 27.74%\n",
        "\n",
        "Test Loss: 1.8368, Test Accuracy: 30.65%\n",
        "\n",
        "Epoch: 3, Train Loss: 1.7983, Train Accuracy: 31.05%\n",
        "Epoch: 3, Test Loss: 1.7697, Test Accuracy: 31.61%\n",
        "End of Epoch 3:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.7527\n",
        "\n",
        "Train Loss: 1.7983, Train Accuracy: 31.05%\n",
        "\n",
        "Test Loss: 1.7697, Test Accuracy: 31.61%\n",
        "\n",
        "Epoch: 4, Train Loss: 1.7453, Train Accuracy: 32.75%\n",
        "Epoch: 4, Test Loss: 1.7251, Test Accuracy: 33.70%\n",
        "End of Epoch 4:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.8518\n",
        "\n",
        "Train Loss: 1.7453, Train Accuracy: 32.75%\n",
        "\n",
        "Test Loss: 1.7251, Test Accuracy: 33.70%\n",
        "\n",
        "Epoch: 5, Train Loss: 1.7041, Train Accuracy: 33.91%\n",
        "Epoch: 5, Test Loss: 1.6973, Test Accuracy: 34.39%\n",
        "End of Epoch 5:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.9411\n",
        "\n",
        "Train Loss: 1.7041, Train Accuracy: 33.91%\n",
        "\n",
        "Test Loss: 1.6973, Test Accuracy: 34.39%\n",
        "\n",
        "Epoch: 6, Train Loss: 1.6749, Train Accuracy: 35.61%\n",
        "Epoch: 6, Test Loss: 1.6673, Test Accuracy: 35.68%\n",
        "End of Epoch 6:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0231\n",
        "\n",
        "Train Loss: 1.6749, Train Accuracy: 35.61%\n",
        "\n",
        "Test Loss: 1.6673, Test Accuracy: 35.68%\n",
        "\n",
        "Epoch: 7, Train Loss: 1.6524, Train Accuracy: 36.98%\n",
        "Epoch: 7, Test Loss: 1.6389, Test Accuracy: 37.98%\n",
        "End of Epoch 7:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0995\n",
        "\n",
        "Train Loss: 1.6524, Train Accuracy: 36.98%\n",
        "\n",
        "Test Loss: 1.6389, Test Accuracy: 37.98%\n",
        "\n",
        "Epoch: 8, Train Loss: 1.6240, Train Accuracy: 39.02%\n",
        "Epoch: 8, Test Loss: 1.6263, Test Accuracy: 39.02%\n",
        "End of Epoch 8:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.1715\n",
        "\n",
        "Train Loss: 1.6240, Train Accuracy: 39.02%\n",
        "\n",
        "Test Loss: 1.6263, Test Accuracy: 39.02%\n",
        "\n",
        "Epoch: 9, Train Loss: 1.6102, Train Accuracy: 39.97%\n",
        "Epoch: 9, Test Loss: 1.6171, Test Accuracy: 40.11%\n",
        "End of Epoch 9:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2397\n",
        "\n",
        "Train Loss: 1.6102, Train Accuracy: 39.97%\n",
        "\n",
        "Test Loss: 1.6171, Test Accuracy: 40.11%\n",
        "\n",
        "Epoch: 10, Train Loss: 1.5966, Train Accuracy: 40.83%\n",
        "Epoch: 10, Test Loss: 1.6105, Test Accuracy: 40.57%\n",
        "Averaging model parameters...\n",
        "\n",
        "End of Epoch 10:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3049\n",
        "\n",
        "Train Loss: 1.5966, Train Accuracy: 40.83%\n",
        "\n",
        "Test Loss: 1.6105, Test Accuracy: 40.57%\n",
        "\n",
        "Epoch: 11, Train Loss: 1.5838, Train Accuracy: 41.31%\n",
        "Epoch: 11, Test Loss: 1.5903, Test Accuracy: 41.37%\n",
        "End of Epoch 11:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3673\n",
        "\n",
        "Train Loss: 1.5838, Train Accuracy: 41.31%\n",
        "\n",
        "Test Loss: 1.5903, Test Accuracy: 41.37%\n",
        "\n",
        "Epoch: 12, Train Loss: 1.5741, Train Accuracy: 41.72%\n",
        "Epoch: 12, Test Loss: 1.5984, Test Accuracy: 41.50%\n",
        "End of Epoch 12:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4274\n",
        "\n",
        "Train Loss: 1.5741, Train Accuracy: 41.72%\n",
        "\n",
        "Test Loss: 1.5984, Test Accuracy: 41.50%\n",
        "\n",
        "Epoch: 13, Train Loss: 1.5721, Train Accuracy: 42.09%\n",
        "Epoch: 13, Test Loss: 1.5668, Test Accuracy: 43.01%\n",
        "End of Epoch 13:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4854\n",
        "\n",
        "Train Loss: 1.5721, Train Accuracy: 42.09%\n",
        "\n",
        "Test Loss: 1.5668, Test Accuracy: 43.01%\n",
        "\n",
        "Epoch: 14, Train Loss: 1.5589, Train Accuracy: 43.09%\n",
        "Epoch: 14, Test Loss: 1.5826, Test Accuracy: 41.96%\n",
        "End of Epoch 14:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5415\n",
        "\n",
        "Train Loss: 1.5589, Train Accuracy: 43.09%\n",
        "\n",
        "Test Loss: 1.5826, Test Accuracy: 41.96%\n",
        "\n",
        "Epoch: 15, Train Loss: 1.5504, Train Accuracy: 42.61%\n",
        "Epoch: 15, Test Loss: 1.5769, Test Accuracy: 42.69%\n",
        "End of Epoch 15:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5959\n",
        "\n",
        "Train Loss: 1.5504, Train Accuracy: 42.61%\n",
        "\n",
        "Test Loss: 1.5769, Test Accuracy: 42.69%\n",
        "\n",
        "Epoch: 16, Train Loss: 1.5464, Train Accuracy: 43.40%\n",
        "Epoch: 16, Test Loss: 1.5572, Test Accuracy: 43.26%\n",
        "End of Epoch 16:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6489\n",
        "\n",
        "Train Loss: 1.5464, Train Accuracy: 43.40%\n",
        "\n",
        "Test Loss: 1.5572, Test Accuracy: 43.26%\n",
        "\n",
        "Epoch: 17, Train Loss: 1.5518, Train Accuracy: 43.31%\n",
        "Epoch: 17, Test Loss: 1.5848, Test Accuracy: 42.03%\n",
        "End of Epoch 17:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7004\n",
        "\n",
        "Train Loss: 1.5518, Train Accuracy: 43.31%\n",
        "\n",
        "Test Loss: 1.5848, Test Accuracy: 42.03%\n",
        "\n",
        "Epoch: 18, Train Loss: 1.5270, Train Accuracy: 44.06%\n",
        "Epoch: 18, Test Loss: 1.5354, Test Accuracy: 44.00%\n",
        "End of Epoch 18:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7507\n",
        "\n",
        "Train Loss: 1.5270, Train Accuracy: 44.06%\n",
        "\n",
        "Test Loss: 1.5354, Test Accuracy: 44.00%\n",
        "\n",
        "Epoch: 19, Train Loss: 1.5329, Train Accuracy: 44.10%\n",
        "Epoch: 19, Test Loss: 1.5592, Test Accuracy: 43.54%\n",
        "End of Epoch 19:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7998\n",
        "\n",
        "Train Loss: 1.5329, Train Accuracy: 44.10%\n",
        "\n",
        "Test Loss: 1.5592, Test Accuracy: 43.54%\n",
        "\n",
        "Epoch: 20, Train Loss: 1.5248, Train Accuracy: 44.20%\n",
        "Epoch: 20, Test Loss: 1.5413, Test Accuracy: 44.64%\n",
        "Averaging model parameters...\n",
        "\n",
        "End of Epoch 20:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8479\n",
        "\n",
        "Train Loss: 1.5248, Train Accuracy: 44.20%\n",
        "\n",
        "Test Loss: 1.5413, Test Accuracy: 44.64%\n",
        "\n",
        "Epoch: 21, Train Loss: 1.5124, Train Accuracy: 44.72%\n",
        "Epoch: 21, Test Loss: 1.5306, Test Accuracy: 44.66%\n",
        "End of Epoch 21:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8949\n",
        "\n",
        "Train Loss: 1.5124, Train Accuracy: 44.72%\n",
        "\n",
        "Test Loss: 1.5306, Test Accuracy: 44.66%\n",
        "\n",
        "Epoch: 22, Train Loss: 1.5166, Train Accuracy: 45.16%\n",
        "Epoch: 22, Test Loss: 1.5520, Test Accuracy: 43.68%\n",
        "End of Epoch 22:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9410\n",
        "\n",
        "Train Loss: 1.5166, Train Accuracy: 45.16%\n",
        "\n",
        "Test Loss: 1.5520, Test Accuracy: 43.68%\n",
        "\n",
        "Epoch: 23, Train Loss: 1.5130, Train Accuracy: 44.95%\n",
        "Epoch: 23, Test Loss: 1.5162, Test Accuracy: 45.63%\n",
        "End of Epoch 23:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9863\n",
        "\n",
        "Train Loss: 1.5130, Train Accuracy: 44.95%\n",
        "\n",
        "Test Loss: 1.5162, Test Accuracy: 45.63%\n",
        "\n",
        "Epoch: 24, Train Loss: 1.5086, Train Accuracy: 45.36%\n",
        "Epoch: 24, Test Loss: 1.5213, Test Accuracy: 45.35%\n",
        "End of Epoch 24:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0307\n",
        "\n",
        "Train Loss: 1.5086, Train Accuracy: 45.36%\n",
        "\n",
        "Test Loss: 1.5213, Test Accuracy: 45.35%\n",
        "\n",
        "Epoch: 25, Train Loss: 1.4998, Train Accuracy: 45.69%\n",
        "Epoch: 25, Test Loss: 1.5179, Test Accuracy: 45.69%\n",
        "End of Epoch 25:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0743\n",
        "\n",
        "Train Loss: 1.4998, Train Accuracy: 45.69%\n",
        "\n",
        "Test Loss: 1.5179, Test Accuracy: 45.69%\n",
        "\n",
        "Epoch: 26, Train Loss: 1.5084, Train Accuracy: 45.75%\n",
        "Epoch: 26, Test Loss: 1.5063, Test Accuracy: 45.81%\n",
        "End of Epoch 26:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1172\n",
        "\n",
        "Train Loss: 1.5084, Train Accuracy: 45.75%\n",
        "\n",
        "Test Loss: 1.5063, Test Accuracy: 45.81%\n",
        "\n",
        "Epoch: 27, Train Loss: 1.5057, Train Accuracy: 45.61%\n",
        "Epoch: 27, Test Loss: 1.5222, Test Accuracy: 45.56%\n",
        "End of Epoch 27:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1594\n",
        "\n",
        "Train Loss: 1.5057, Train Accuracy: 45.61%\n",
        "\n",
        "Test Loss: 1.5222, Test Accuracy: 45.56%\n",
        "\n",
        "Epoch: 28, Train Loss: 1.5023, Train Accuracy: 46.16%\n",
        "Epoch: 28, Test Loss: 1.5227, Test Accuracy: 45.65%\n",
        "End of Epoch 28:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2010\n",
        "\n",
        "Train Loss: 1.5023, Train Accuracy: 46.16%\n",
        "\n",
        "Test Loss: 1.5227, Test Accuracy: 45.65%\n",
        "\n",
        "Epoch: 29, Train Loss: 1.5129, Train Accuracy: 45.77%\n",
        "Epoch: 29, Test Loss: 1.5326, Test Accuracy: 45.57%\n",
        "End of Epoch 29:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2420\n",
        "\n",
        "Train Loss: 1.5129, Train Accuracy: 45.77%\n",
        "\n",
        "Test Loss: 1.5326, Test Accuracy: 45.57%\n",
        "\n",
        "Epoch: 30, Train Loss: 1.5024, Train Accuracy: 46.42%\n",
        "Epoch: 30, Test Loss: 1.5324, Test Accuracy: 46.20%\n",
        "Averaging model parameters...\n",
        "\n",
        "End of Epoch 30:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2823\n",
        "\n",
        "Train Loss: 1.5024, Train Accuracy: 46.42%\n",
        "\n",
        "Test Loss: 1.5324, Test Accuracy: 46.20%\n",
        "\n",
        "Epoch: 31, Train Loss: 1.5011, Train Accuracy: 46.52%\n",
        "Epoch: 31, Test Loss: 1.5103, Test Accuracy: 46.60%\n",
        "End of Epoch 31:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3222\n",
        "\n",
        "Train Loss: 1.5011, Train Accuracy: 46.52%\n",
        "\n",
        "Test Loss: 1.5103, Test Accuracy: 46.60%\n",
        "\n",
        "Epoch: 32, Train Loss: 1.4985, Train Accuracy: 46.79%\n",
        "Epoch: 32, Test Loss: 1.5118, Test Accuracy: 46.51%\n",
        "End of Epoch 32:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3614\n",
        "\n",
        "Train Loss: 1.4985, Train Accuracy: 46.79%\n",
        "\n",
        "Test Loss: 1.5118, Test Accuracy: 46.51%\n",
        "\n",
        "Epoch: 33, Train Loss: 1.5039, Train Accuracy: 46.77%\n",
        "Epoch: 33, Test Loss: 1.5345, Test Accuracy: 45.65%\n",
        "End of Epoch 33:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4002\n",
        "\n",
        "Train Loss: 1.5039, Train Accuracy: 46.77%\n",
        "\n",
        "Test Loss: 1.5345, Test Accuracy: 45.65%\n",
        "\n",
        "Epoch: 34, Train Loss: 1.4961, Train Accuracy: 46.84%\n",
        "Epoch: 34, Test Loss: 1.5206, Test Accuracy: 46.73%\n",
        "End of Epoch 34:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4385\n",
        "\n",
        "Train Loss: 1.4961, Train Accuracy: 46.84%\n",
        "\n",
        "Test Loss: 1.5206, Test Accuracy: 46.73%\n",
        "\n",
        "Epoch: 35, Train Loss: 1.4949, Train Accuracy: 47.12%\n",
        "Epoch: 35, Test Loss: 1.5377, Test Accuracy: 45.98%\n",
        "End of Epoch 35:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4763\n",
        "\n",
        "Train Loss: 1.4949, Train Accuracy: 47.12%\n",
        "\n",
        "Test Loss: 1.5377, Test Accuracy: 45.98%\n",
        "\n",
        "Epoch: 36, Train Loss: 1.4840, Train Accuracy: 47.02%\n",
        "Epoch: 36, Test Loss: 1.5434, Test Accuracy: 45.75%\n",
        "End of Epoch 36:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5137\n",
        "\n",
        "Train Loss: 1.4840, Train Accuracy: 47.02%\n",
        "\n",
        "Test Loss: 1.5434, Test Accuracy: 45.75%\n",
        "\n",
        "Epoch: 37, Train Loss: 1.4900, Train Accuracy: 47.14%\n",
        "Epoch: 37, Test Loss: 1.5276, Test Accuracy: 46.88%\n",
        "End of Epoch 37:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5507\n",
        "\n",
        "Train Loss: 1.4900, Train Accuracy: 47.14%\n",
        "\n",
        "Test Loss: 1.5276, Test Accuracy: 46.88%\n",
        "\n",
        "Epoch: 38, Train Loss: 1.4937, Train Accuracy: 47.13%\n",
        "Epoch: 38, Test Loss: 1.5133, Test Accuracy: 47.15%\n",
        "End of Epoch 38:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5872\n",
        "\n",
        "Train Loss: 1.4937, Train Accuracy: 47.13%\n",
        "\n",
        "Test Loss: 1.5133, Test Accuracy: 47.15%\n",
        "\n",
        "Epoch: 39, Train Loss: 1.4936, Train Accuracy: 47.28%\n",
        "Epoch: 39, Test Loss: 1.5017, Test Accuracy: 46.82%\n",
        "End of Epoch 39:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6233\n",
        "\n",
        "Train Loss: 1.4936, Train Accuracy: 47.28%\n",
        "\n",
        "Test Loss: 1.5017, Test Accuracy: 46.82%\n",
        "\n",
        "Epoch: 40, Train Loss: 1.4939, Train Accuracy: 47.55%\n",
        "Epoch: 40, Test Loss: 1.5395, Test Accuracy: 46.01%\n",
        "Averaging model parameters...\n",
        "\n",
        "End of Epoch 40:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6591\n",
        "\n",
        "Train Loss: 1.4939, Train Accuracy: 47.55%\n",
        "\n",
        "Test Loss: 1.5395, Test Accuracy: 46.01%\n",
        "\n",
        "Epoch: 41, Train Loss: 1.4973, Train Accuracy: 47.50%\n",
        "Epoch: 41, Test Loss: 1.5036, Test Accuracy: 47.48%\n",
        "End of Epoch 41:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6945\n",
        "\n",
        "Train Loss: 1.4973, Train Accuracy: 47.50%\n",
        "\n",
        "Test Loss: 1.5036, Test Accuracy: 47.48%\n",
        "\n",
        "Epoch: 42, Train Loss: 1.4920, Train Accuracy: 47.76%\n",
        "Epoch: 42, Test Loss: 1.5241, Test Accuracy: 47.16%\n",
        "End of Epoch 42:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7295\n",
        "\n",
        "Train Loss: 1.4920, Train Accuracy: 47.76%\n",
        "\n",
        "Test Loss: 1.5241, Test Accuracy: 47.16%\n",
        "\n",
        "Epoch: 43, Train Loss: 1.4929, Train Accuracy: 47.73%\n",
        "Epoch: 43, Test Loss: 1.5543, Test Accuracy: 47.28%\n",
        "End of Epoch 43:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7642\n",
        "\n",
        "Train Loss: 1.4929, Train Accuracy: 47.73%\n",
        "\n",
        "Test Loss: 1.5543, Test Accuracy: 47.28%\n",
        "\n",
        "Epoch: 44, Train Loss: 1.4950, Train Accuracy: 47.66%\n",
        "Epoch: 44, Test Loss: 1.5229, Test Accuracy: 46.97%\n",
        "End of Epoch 44:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7986\n",
        "\n",
        "Train Loss: 1.4950, Train Accuracy: 47.66%\n",
        "\n",
        "Test Loss: 1.5229, Test Accuracy: 46.97%\n",
        "\n",
        "Epoch: 45, Train Loss: 1.4940, Train Accuracy: 47.84%\n",
        "Epoch: 45, Test Loss: 1.5100, Test Accuracy: 47.15%\n",
        "End of Epoch 45:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8327\n",
        "\n",
        "Train Loss: 1.4940, Train Accuracy: 47.84%\n",
        "\n",
        "Test Loss: 1.5100, Test Accuracy: 47.15%\n",
        "\n",
        "Epoch: 46, Train Loss: 1.5048, Train Accuracy: 47.58%\n",
        "Epoch: 46, Test Loss: 1.5387, Test Accuracy: 46.64%\n",
        "End of Epoch 46:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8664\n",
        "\n",
        "Train Loss: 1.5048, Train Accuracy: 47.58%\n",
        "\n",
        "Test Loss: 1.5387, Test Accuracy: 46.64%\n",
        "\n",
        "Epoch: 47, Train Loss: 1.4996, Train Accuracy: 48.03%\n",
        "Epoch: 47, Test Loss: 1.5237, Test Accuracy: 47.14%\n",
        "End of Epoch 47:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8998\n",
        "\n",
        "Train Loss: 1.4996, Train Accuracy: 48.03%\n",
        "\n",
        "Test Loss: 1.5237, Test Accuracy: 47.14%\n",
        "\n",
        "Epoch: 48, Train Loss: 1.4999, Train Accuracy: 47.80%\n",
        "Epoch: 48, Test Loss: 1.5191, Test Accuracy: 47.54%\n",
        "End of Epoch 48:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9330\n",
        "\n",
        "Train Loss: 1.4999, Train Accuracy: 47.80%\n",
        "\n",
        "Test Loss: 1.5191, Test Accuracy: 47.54%\n",
        "\n",
        "Epoch: 49, Train Loss: 1.5010, Train Accuracy: 48.23%\n",
        "Epoch: 49, Test Loss: 1.5093, Test Accuracy: 47.89%\n",
        "End of Epoch 49:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9659\n",
        "\n",
        "Train Loss: 1.5010, Train Accuracy: 48.23%\n",
        "\n",
        "Test Loss: 1.5093, Test Accuracy: 47.89%"
      ],
      "metadata": {
        "id": "je2UykvqQuCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train Loss: 2.1057, Train Accuracy: 20.05%\n",
        "Epoch: 1, Test Loss: 1.9697, Test Accuracy: 26.04%\n",
        "End of Epoch 1:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.4951\n",
        "\n",
        "Train Loss: 2.1057, Train Accuracy: 20.05%\n",
        "\n",
        "Test Loss: 1.9697, Test Accuracy: 26.04%\n",
        "\n",
        "Epoch: 2, Train Loss: 1.9076, Train Accuracy: 27.74%\n",
        "Epoch: 2, Test Loss: 1.8368, Test Accuracy: 30.65%\n",
        "End of Epoch 2:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.6385\n",
        "\n",
        "Train Loss: 1.9076, Train Accuracy: 27.74%\n",
        "\n",
        "Test Loss: 1.8368, Test Accuracy: 30.65%\n",
        "\n",
        "Epoch: 3, Train Loss: 1.7983, Train Accuracy: 31.05%\n",
        "Epoch: 3, Test Loss: 1.7697, Test Accuracy: 31.61%\n",
        "End of Epoch 3:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.7527\n",
        "\n",
        "Train Loss: 1.7983, Train Accuracy: 31.05%\n",
        "\n",
        "Test Loss: 1.7697, Test Accuracy: 31.61%\n",
        "\n",
        "Epoch: 4, Train Loss: 1.7453, Train Accuracy: 32.75%\n",
        "Epoch: 4, Test Loss: 1.7251, Test Accuracy: 33.70%\n",
        "End of Epoch 4:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.8518\n",
        "\n",
        "Train Loss: 1.7453, Train Accuracy: 32.75%\n",
        "\n",
        "Test Loss: 1.7251, Test Accuracy: 33.70%\n",
        "\n",
        "Epoch: 5, Train Loss: 1.7041, Train Accuracy: 33.91%\n",
        "Epoch: 5, Test Loss: 1.6973, Test Accuracy: 34.39%\n",
        "End of Epoch 5:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.9411\n",
        "\n",
        "Train Loss: 1.7041, Train Accuracy: 33.91%\n",
        "\n",
        "Test Loss: 1.6973, Test Accuracy: 34.39%\n",
        "\n",
        "Epoch: 6, Train Loss: 1.6749, Train Accuracy: 35.61%\n",
        "Epoch: 6, Test Loss: 1.6673, Test Accuracy: 35.68%\n",
        "End of Epoch 6:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0231\n",
        "\n",
        "Train Loss: 1.6749, Train Accuracy: 35.61%\n",
        "\n",
        "Test Loss: 1.6673, Test Accuracy: 35.68%\n",
        "\n",
        "Epoch: 7, Train Loss: 1.6524, Train Accuracy: 36.98%\n",
        "Epoch: 7, Test Loss: 1.6389, Test Accuracy: 37.98%\n",
        "End of Epoch 7:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0995\n",
        "\n",
        "Train Loss: 1.6524, Train Accuracy: 36.98%\n",
        "\n",
        "Test Loss: 1.6389, Test Accuracy: 37.98%\n",
        "\n",
        "Epoch: 8, Train Loss: 1.6240, Train Accuracy: 39.02%\n",
        "Epoch: 8, Test Loss: 1.6263, Test Accuracy: 39.02%\n",
        "End of Epoch 8:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.1715\n",
        "\n",
        "Train Loss: 1.6240, Train Accuracy: 39.02%\n",
        "\n",
        "Test Loss: 1.6263, Test Accuracy: 39.02%\n",
        "\n",
        "Epoch: 9, Train Loss: 1.6102, Train Accuracy: 39.97%\n",
        "Epoch: 9, Test Loss: 1.6171, Test Accuracy: 40.11%\n",
        "End of Epoch 9:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2397\n",
        "\n",
        "Train Loss: 1.6102, Train Accuracy: 39.97%\n",
        "\n",
        "Test Loss: 1.6171, Test Accuracy: 40.11%\n",
        "\n",
        "Epoch: 10, Train Loss: 1.5966, Train Accuracy: 40.83%\n",
        "Epoch: 10, Test Loss: 1.6105, Test Accuracy: 40.57%\n",
        "Averaging model parameters...\n",
        "\n",
        "End of Epoch 10:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3049\n",
        "\n",
        "Train Loss: 1.5966, Train Accuracy: 40.83%\n",
        "\n",
        "Test Loss: 1.6105, Test Accuracy: 40.57%\n",
        "\n",
        "Epoch: 11, Train Loss: 1.5838, Train Accuracy: 41.31%\n",
        "Epoch: 11, Test Loss: 1.5903, Test Accuracy: 41.37%\n",
        "End of Epoch 11:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3673\n",
        "\n",
        "Train Loss: 1.5838, Train Accuracy: 41.31%\n",
        "\n",
        "Test Loss: 1.5903, Test Accuracy: 41.37%\n",
        "\n",
        "Epoch: 12, Train Loss: 1.5741, Train Accuracy: 41.72%\n",
        "Epoch: 12, Test Loss: 1.5984, Test Accuracy: 41.50%\n",
        "End of Epoch 12:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4274\n",
        "\n",
        "Train Loss: 1.5741, Train Accuracy: 41.72%\n",
        "\n",
        "Test Loss: 1.5984, Test Accuracy: 41.50%\n",
        "\n",
        "Epoch: 13, Train Loss: 1.5721, Train Accuracy: 42.09%\n",
        "Epoch: 13, Test Loss: 1.5668, Test Accuracy: 43.01%\n",
        "End of Epoch 13:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4854\n",
        "\n",
        "Train Loss: 1.5721, Train Accuracy: 42.09%\n",
        "\n",
        "Test Loss: 1.5668, Test Accuracy: 43.01%\n",
        "\n",
        "Epoch: 14, Train Loss: 1.5589, Train Accuracy: 43.09%\n",
        "Epoch: 14, Test Loss: 1.5826, Test Accuracy: 41.96%\n",
        "End of Epoch 14:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5415\n",
        "\n",
        "Train Loss: 1.5589, Train Accuracy: 43.09%\n",
        "\n",
        "Test Loss: 1.5826, Test Accuracy: 41.96%\n",
        "\n",
        "Epoch: 15, Train Loss: 1.5504, Train Accuracy: 42.61%\n",
        "Epoch: 15, Test Loss: 1.5769, Test Accuracy: 42.69%\n",
        "End of Epoch 15:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5959\n",
        "\n",
        "Train Loss: 1.5504, Train Accuracy: 42.61%\n",
        "\n",
        "Test Loss: 1.5769, Test Accuracy: 42.69%\n",
        "\n",
        "Epoch: 16, Train Loss: 1.5464, Train Accuracy: 43.40%\n",
        "Epoch: 16, Test Loss: 1.5572, Test Accuracy: 43.26%\n",
        "End of Epoch 16:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6489\n",
        "\n",
        "Train Loss: 1.5464, Train Accuracy: 43.40%\n",
        "\n",
        "Test Loss: 1.5572, Test Accuracy: 43.26%\n",
        "\n",
        "Epoch: 17, Train Loss: 1.5518, Train Accuracy: 43.31%\n",
        "Epoch: 17, Test Loss: 1.5848, Test Accuracy: 42.03%\n",
        "End of Epoch 17:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7004\n",
        "\n",
        "Train Loss: 1.5518, Train Accuracy: 43.31%\n",
        "\n",
        "Test Loss: 1.5848, Test Accuracy: 42.03%\n",
        "\n",
        "Epoch: 18, Train Loss: 1.5270, Train Accuracy: 44.06%\n",
        "Epoch: 18, Test Loss: 1.5354, Test Accuracy: 44.00%\n",
        "End of Epoch 18:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7507\n",
        "\n",
        "Train Loss: 1.5270, Train Accuracy: 44.06%\n",
        "\n",
        "Test Loss: 1.5354, Test Accuracy: 44.00%\n",
        "\n",
        "Epoch: 19, Train Loss: 1.5329, Train Accuracy: 44.10%\n",
        "Epoch: 19, Test Loss: 1.5592, Test Accuracy: 43.54%\n",
        "End of Epoch 19:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7998\n",
        "\n",
        "Train Loss: 1.5329, Train Accuracy: 44.10%\n",
        "\n",
        "Test Loss: 1.5592, Test Accuracy: 43.54%\n",
        "\n",
        "Epoch: 20, Train Loss: 1.5248, Train Accuracy: 44.20%\n",
        "Epoch: 20, Test Loss: 1.5413, Test Accuracy: 44.64%\n",
        "Averaging model parameters...\n",
        "\n",
        "End of Epoch 20:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8479\n",
        "\n",
        "Train Loss: 1.5248, Train Accuracy: 44.20%\n",
        "\n",
        "Test Loss: 1.5413, Test Accuracy: 44.64%\n",
        "\n",
        "Epoch: 21, Train Loss: 1.5124, Train Accuracy: 44.72%\n",
        "Epoch: 21, Test Loss: 1.5306, Test Accuracy: 44.66%\n",
        "End of Epoch 21:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8949\n",
        "\n",
        "Train Loss: 1.5124, Train Accuracy: 44.72%\n",
        "\n",
        "Test Loss: 1.5306, Test Accuracy: 44.66%\n",
        "\n",
        "Epoch: 22, Train Loss: 1.5166, Train Accuracy: 45.16%\n",
        "Epoch: 22, Test Loss: 1.5520, Test Accuracy: 43.68%\n",
        "End of Epoch 22:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9410\n",
        "\n",
        "Train Loss: 1.5166, Train Accuracy: 45.16%\n",
        "\n",
        "Test Loss: 1.5520, Test Accuracy: 43.68%\n",
        "\n",
        "Epoch: 23, Train Loss: 1.5130, Train Accuracy: 44.95%\n",
        "Epoch: 23, Test Loss: 1.5162, Test Accuracy: 45.63%\n",
        "End of Epoch 23:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9863\n",
        "\n",
        "Train Loss: 1.5130, Train Accuracy: 44.95%\n",
        "\n",
        "Test Loss: 1.5162, Test Accuracy: 45.63%\n",
        "\n",
        "Epoch: 24, Train Loss: 1.5086, Train Accuracy: 45.36%\n",
        "Epoch: 24, Test Loss: 1.5213, Test Accuracy: 45.35%\n",
        "End of Epoch 24:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0307\n",
        "\n",
        "Train Loss: 1.5086, Train Accuracy: 45.36%\n",
        "\n",
        "Test Loss: 1.5213, Test Accuracy: 45.35%\n",
        "\n",
        "Epoch: 25, Train Loss: 1.4998, Train Accuracy: 45.69%\n",
        "Epoch: 25, Test Loss: 1.5179, Test Accuracy: 45.69%\n",
        "End of Epoch 25:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0743\n",
        "\n",
        "Train Loss: 1.4998, Train Accuracy: 45.69%\n",
        "\n",
        "Test Loss: 1.5179, Test Accuracy: 45.69%\n",
        "\n",
        "Epoch: 26, Train Loss: 1.5084, Train Accuracy: 45.75%\n",
        "Epoch: 26, Test Loss: 1.5063, Test Accuracy: 45.81%\n",
        "End of Epoch 26:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1172\n",
        "\n",
        "Train Loss: 1.5084, Train Accuracy: 45.75%\n",
        "\n",
        "Test Loss: 1.5063, Test Accuracy: 45.81%\n",
        "\n",
        "Epoch: 27, Train Loss: 1.5057, Train Accuracy: 45.61%\n",
        "Epoch: 27, Test Loss: 1.5222, Test Accuracy: 45.56%\n",
        "End of Epoch 27:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1594\n",
        "\n",
        "Train Loss: 1.5057, Train Accuracy: 45.61%\n",
        "\n",
        "Test Loss: 1.5222, Test Accuracy: 45.56%\n",
        "\n",
        "Epoch: 28, Train Loss: 1.5023, Train Accuracy: 46.16%\n",
        "Epoch: 28, Test Loss: 1.5227, Test Accuracy: 45.65%\n",
        "End of Epoch 28:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2010\n",
        "\n",
        "Train Loss: 1.5023, Train Accuracy: 46.16%\n",
        "\n",
        "Test Loss: 1.5227, Test Accuracy: 45.65%\n",
        "\n",
        "Epoch: 29, Train Loss: 1.5129, Train Accuracy: 45.77%\n",
        "Epoch: 29, Test Loss: 1.5326, Test Accuracy: 45.57%\n",
        "End of Epoch 29:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2420\n",
        "\n",
        "Train Loss: 1.5129, Train Accuracy: 45.77%\n",
        "\n",
        "Test Loss: 1.5326, Test Accuracy: 45.57%\n",
        "\n",
        "Epoch: 30, Train Loss: 1.5024, Train Accuracy: 46.42%\n",
        "Epoch: 30, Test Loss: 1.5324, Test Accuracy: 46.20%\n",
        "Averaging model parameters...\n",
        "\n",
        "End of Epoch 30:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2823\n",
        "\n",
        "Train Loss: 1.5024, Train Accuracy: 46.42%\n",
        "\n",
        "Test Loss: 1.5324, Test Accuracy: 46.20%\n",
        "\n",
        "Epoch: 31, Train Loss: 1.5011, Train Accuracy: 46.52%\n",
        "Epoch: 31, Test Loss: 1.5103, Test Accuracy: 46.60%\n",
        "End of Epoch 31:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3222\n",
        "\n",
        "Train Loss: 1.5011, Train Accuracy: 46.52%\n",
        "\n",
        "Test Loss: 1.5103, Test Accuracy: 46.60%\n",
        "\n",
        "Epoch: 32, Train Loss: 1.4985, Train Accuracy: 46.79%\n",
        "Epoch: 32, Test Loss: 1.5118, Test Accuracy: 46.51%\n",
        "End of Epoch 32:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3614\n",
        "\n",
        "Train Loss: 1.4985, Train Accuracy: 46.79%\n",
        "\n",
        "Test Loss: 1.5118, Test Accuracy: 46.51%\n",
        "\n",
        "Epoch: 33, Train Loss: 1.5039, Train Accuracy: 46.77%\n",
        "Epoch: 33, Test Loss: 1.5345, Test Accuracy: 45.65%\n",
        "End of Epoch 33:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4002\n",
        "\n",
        "Train Loss: 1.5039, Train Accuracy: 46.77%\n",
        "\n",
        "Test Loss: 1.5345, Test Accuracy: 45.65%\n",
        "\n",
        "Epoch: 34, Train Loss: 1.4961, Train Accuracy: 46.84%\n",
        "Epoch: 34, Test Loss: 1.5206, Test Accuracy: 46.73%\n",
        "End of Epoch 34:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4385\n",
        "\n",
        "Train Loss: 1.4961, Train Accuracy: 46.84%\n",
        "\n",
        "Test Loss: 1.5206, Test Accuracy: 46.73%\n",
        "\n",
        "Epoch: 35, Train Loss: 1.4949, Train Accuracy: 47.12%\n",
        "Epoch: 35, Test Loss: 1.5377, Test Accuracy: 45.98%\n",
        "End of Epoch 35:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4763\n",
        "\n",
        "Train Loss: 1.4949, Train Accuracy: 47.12%\n",
        "\n",
        "Test Loss: 1.5377, Test Accuracy: 45.98%\n",
        "\n",
        "Epoch: 36, Train Loss: 1.4840, Train Accuracy: 47.02%\n",
        "Epoch: 36, Test Loss: 1.5434, Test Accuracy: 45.75%\n",
        "End of Epoch 36:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5137\n",
        "\n",
        "Train Loss: 1.4840, Train Accuracy: 47.02%\n",
        "\n",
        "Test Loss: 1.5434, Test Accuracy: 45.75%\n",
        "\n",
        "Epoch: 37, Train Loss: 1.4900, Train Accuracy: 47.14%\n",
        "Epoch: 37, Test Loss: 1.5276, Test Accuracy: 46.88%\n",
        "End of Epoch 37:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5507\n",
        "\n",
        "Train Loss: 1.4900, Train Accuracy: 47.14%\n",
        "\n",
        "Test Loss: 1.5276, Test Accuracy: 46.88%\n",
        "\n",
        "Epoch: 38, Train Loss: 1.4937, Train Accuracy: 47.13%\n",
        "Epoch: 38, Test Loss: 1.5133, Test Accuracy: 47.15%\n",
        "End of Epoch 38:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5872\n",
        "\n",
        "Train Loss: 1.4937, Train Accuracy: 47.13%\n",
        "\n",
        "Test Loss: 1.5133, Test Accuracy: 47.15%\n",
        "\n",
        "Epoch: 39, Train Loss: 1.4936, Train Accuracy: 47.28%\n",
        "Epoch: 39, Test Loss: 1.5017, Test Accuracy: 46.82%\n",
        "End of Epoch 39:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6233\n",
        "\n",
        "Train Loss: 1.4936, Train Accuracy: 47.28%\n",
        "\n",
        "Test Loss: 1.5017, Test Accuracy: 46.82%\n",
        "\n",
        "Epoch: 40, Train Loss: 1.4939, Train Accuracy: 47.55%\n",
        "Epoch: 40, Test Loss: 1.5395, Test Accuracy: 46.01%\n",
        "Averaging model parameters...\n",
        "\n",
        "End of Epoch 40:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6591\n",
        "\n",
        "Train Loss: 1.4939, Train Accuracy: 47.55%\n",
        "\n",
        "Test Loss: 1.5395, Test Accuracy: 46.01%\n",
        "\n",
        "Epoch: 41, Train Loss: 1.4973, Train Accuracy: 47.50%\n",
        "Epoch: 41, Test Loss: 1.5036, Test Accuracy: 47.48%\n",
        "End of Epoch 41:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6945\n",
        "\n",
        "Train Loss: 1.4973, Train Accuracy: 47.50%\n",
        "\n",
        "Test Loss: 1.5036, Test Accuracy: 47.48%\n",
        "\n",
        "Epoch: 42, Train Loss: 1.4920, Train Accuracy: 47.76%\n",
        "Epoch: 42, Test Loss: 1.5241, Test Accuracy: 47.16%\n",
        "End of Epoch 42:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7295\n",
        "\n",
        "Train Loss: 1.4920, Train Accuracy: 47.76%\n",
        "\n",
        "Test Loss: 1.5241, Test Accuracy: 47.16%\n",
        "\n",
        "Epoch: 43, Train Loss: 1.4929, Train Accuracy: 47.73%\n",
        "Epoch: 43, Test Loss: 1.5543, Test Accuracy: 47.28%\n",
        "End of Epoch 43:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7642\n",
        "\n",
        "Train Loss: 1.4929, Train Accuracy: 47.73%\n",
        "\n",
        "Test Loss: 1.5543, Test Accuracy: 47.28%\n",
        "\n",
        "Epoch: 44, Train Loss: 1.4950, Train Accuracy: 47.66%\n",
        "Epoch: 44, Test Loss: 1.5229, Test Accuracy: 46.97%\n",
        "End of Epoch 44:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7986\n",
        "\n",
        "Train Loss: 1.4950, Train Accuracy: 47.66%\n",
        "\n",
        "Test Loss: 1.5229, Test Accuracy: 46.97%\n",
        "\n",
        "Epoch: 45, Train Loss: 1.4940, Train Accuracy: 47.84%\n",
        "Epoch: 45, Test Loss: 1.5100, Test Accuracy: 47.15%\n",
        "End of Epoch 45:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8327\n",
        "\n",
        "Train Loss: 1.4940, Train Accuracy: 47.84%\n",
        "\n",
        "Test Loss: 1.5100, Test Accuracy: 47.15%\n",
        "\n",
        "Epoch: 46, Train Loss: 1.5048, Train Accuracy: 47.58%\n",
        "Epoch: 46, Test Loss: 1.5387, Test Accuracy: 46.64%\n",
        "End of Epoch 46:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8664\n",
        "\n",
        "Train Loss: 1.5048, Train Accuracy: 47.58%\n",
        "\n",
        "Test Loss: 1.5387, Test Accuracy: 46.64%\n",
        "\n",
        "Epoch: 47, Train Loss: 1.4996, Train Accuracy: 48.03%\n",
        "Epoch: 47, Test Loss: 1.5237, Test Accuracy: 47.14%\n",
        "End of Epoch 47:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8998\n",
        "\n",
        "Train Loss: 1.4996, Train Accuracy: 48.03%\n",
        "\n",
        "Test Loss: 1.5237, Test Accuracy: 47.14%\n",
        "\n",
        "Epoch: 48, Train Loss: 1.4999, Train Accuracy: 47.80%\n",
        "Epoch: 48, Test Loss: 1.5191, Test Accuracy: 47.54%\n",
        "End of Epoch 48:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9330\n",
        "\n",
        "Train Loss: 1.4999, Train Accuracy: 47.80%\n",
        "\n",
        "Test Loss: 1.5191, Test Accuracy: 47.54%\n",
        "\n",
        "Epoch: 49, Train Loss: 1.5010, Train Accuracy: 48.23%\n",
        "Epoch: 49, Test Loss: 1.5093, Test Accuracy: 47.89%\n",
        "End of Epoch 49:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9659\n",
        "\n",
        "Train Loss: 1.5010, Train Accuracy: 48.23%\n",
        "\n",
        "Test Loss: 1.5093, Test Accuracy: 47.89%\n",
        "\n",
        "[sg27405@csci-cscuda final]$\n"
      ],
      "metadata": {
        "id": "OeCJ5g2Q7i1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Extracting the data from the given logs\n",
        "epochs = np.arange(1, 51)\n",
        "train_accuracy_vals = [\n",
        "    20.05, 27.74, 31.05, 32.75, 33.91, 35.61, 36.98, 39.02, 39.97, 40.83,\n",
        "    41.31, 41.72, 42.09, 43.09, 42.61, 43.40, 43.31, 44.06, 44.10, 44.20,\n",
        "    44.72, 45.16, 44.95, 45.36, 45.69, 45.75, 45.61, 46.16, 45.77, 46.42,\n",
        "    46.52, 46.79, 46.77, 46.84, 47.12, 47.02, 47.14, 47.13, 47.28, 47.55,\n",
        "    47.50, 47.76, 47.73, 47.66, 47.84, 47.58, 48.03, 47.80, 48.23\n",
        "]\n",
        "test_accuracy_vals = [\n",
        "    26.04, 30.65, 31.61, 33.70, 34.39, 35.68, 37.98, 39.02, 40.11, 40.57,\n",
        "    41.37, 41.50, 43.01, 41.96, 42.69, 43.26, 42.03, 44.00, 43.54, 44.64,\n",
        "    44.66, 43.68, 45.63, 45.35, 45.69, 45.81, 45.56, 45.65, 45.57, 46.20,\n",
        "    46.60, 46.51, 45.65, 46.73, 45.98, 45.75, 46.88, 47.15, 46.82, 46.01,\n",
        "    47.48, 47.16, 47.28, 46.97, 47.15, 46.64, 47.14, 47.54, 47.89\n",
        "]\n",
        "\n",
        "# Extracting privacy budget epsilon values\n",
        "epsilon_vals = [\n",
        "    0.4951, 0.6385, 0.7527, 0.8518, 0.9411, 1.0231, 1.0995, 1.1715, 1.2397, 1.3049,\n",
        "    1.3673, 1.4274, 1.4854, 1.5415, 1.5959, 1.6489, 1.7004, 1.7507, 1.7998, 1.8479,\n",
        "    1.8949, 1.9410, 1.9863, 2.0307, 2.0743, 2.1172, 2.1594, 2.2010, 2.2420, 2.2823,\n",
        "    2.3222, 2.3614, 2.4002, 2.4385, 2.4763, 2.5137, 2.5507, 2.5872, 2.6233, 2.6591,\n",
        "    2.6945, 2.7295, 2.7642, 2.7986, 2.8327, 2.8664, 2.8998, 2.9330, 2.9659\n",
        "]\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(epochs)  # Set x-ticks to show all epochs\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='o')\n",
        "plt.xlabel('Privacy Budget (Epsilon)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yqIpAKaCQmkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LR scheduler added\n",
        "\n",
        " T_max:\n",
        "Definition: The number of epochs required for the learning rate to complete one full cosine cycle.\n",
        "Effect: Determines how gradually the learning rate decays.\n",
        "Usage in your case:\n",
        "Since your training loop runs for 50 epochs, setting T_max=50 ensures the learning rate decreases smoothly to the minimum (eta_min) by the end of training.\n",
        "2. eta_min:\n",
        "Definition: The minimum learning rate at the end of the cosine decay.\n",
        "Effect: Acts as a floor for the learning rate, preventing it from becoming too small (which can stall training).\n",
        "Usage in your case:\n",
        "eta_min=0.0001 ensures the learning rate does not drop below 0.0001, even at the last epoch.\n",
        "\n",
        "\n",
        "LR=0.01. BATCH SIZE=256\n",
        "\n"
      ],
      "metadata": {
        "id": "wfolgEazFVzt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dn6eBl9H4C40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the Cosine Annealing Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
        "\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\\n\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()  # Adjusts the learning rate based on the cosine schedule\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}: \\n')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f} \\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% \\n')\n",
        "    # Log current learning rate (optional)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Learning Rate: {current_lr:.6f} \\n')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RocLNtRaLnOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train Loss: 2.1210, Train Accuracy: 20.06%\n",
        "\n",
        "Epoch: 1, Test Loss: 1.9898, Test Accuracy: 26.24%\n",
        "\n",
        "End of Epoch 1:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.4951\n",
        "\n",
        "Train Loss: 2.1210, Train Accuracy: 20.06%\n",
        "\n",
        "Test Loss: 1.9898, Test Accuracy: 26.24%\n",
        "\n",
        "Learning Rate: 0.009990\n",
        "\n",
        "Epoch: 2, Train Loss: 1.9328, Train Accuracy: 27.77%\n",
        "\n",
        "Epoch: 2, Test Loss: 1.8692, Test Accuracy: 30.06%\n",
        "\n",
        "End of Epoch 2:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.6385\n",
        "\n",
        "Train Loss: 1.9328, Train Accuracy: 27.77%\n",
        "\n",
        "Test Loss: 1.8692, Test Accuracy: 30.06%\n",
        "\n",
        "Learning Rate: 0.009961\n",
        "\n",
        "Epoch: 3, Train Loss: 1.8274, Train Accuracy: 30.89%\n",
        "\n",
        "Epoch: 3, Test Loss: 1.7854, Test Accuracy: 31.52%\n",
        "\n",
        "End of Epoch 3:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.7527\n",
        "\n",
        "Train Loss: 1.8274, Train Accuracy: 30.89%\n",
        "\n",
        "Test Loss: 1.7854, Test Accuracy: 31.52%\n",
        "\n",
        "Learning Rate: 0.009912\n",
        "\n",
        "Epoch: 4, Train Loss: 1.7694, Train Accuracy: 32.05%\n",
        "\n",
        "Epoch: 4, Test Loss: 1.7510, Test Accuracy: 33.21%\n",
        "\n",
        "End of Epoch 4:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.8518\n",
        "\n",
        "Train Loss: 1.7694, Train Accuracy: 32.05%\n",
        "\n",
        "Test Loss: 1.7510, Test Accuracy: 33.21%\n",
        "\n",
        "Learning Rate: 0.009844\n",
        "\n",
        "Epoch: 5, Train Loss: 1.7416, Train Accuracy: 33.78%\n",
        "\n",
        "Epoch: 5, Test Loss: 1.7326, Test Accuracy: 34.48%\n",
        "\n",
        "End of Epoch 5:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.9411\n",
        "\n",
        "Train Loss: 1.7416, Train Accuracy: 33.78%\n",
        "\n",
        "Test Loss: 1.7326, Test Accuracy: 34.48%\n",
        "\n",
        "Learning Rate: 0.009758\n",
        "\n",
        "Epoch: 6, Train Loss: 1.7219, Train Accuracy: 35.44%\n",
        "\n",
        "Epoch: 6, Test Loss: 1.7259, Test Accuracy: 34.98%\n",
        "\n",
        "End of Epoch 6:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0231\n",
        "\n",
        "Train Loss: 1.7219, Train Accuracy: 35.44%\n",
        "\n",
        "Test Loss: 1.7259, Test Accuracy: 34.98%\n",
        "\n",
        "Learning Rate: 0.009652\n",
        "\n",
        "Epoch: 7, Train Loss: 1.7199, Train Accuracy: 36.10%\n",
        "\n",
        "Epoch: 7, Test Loss: 1.7071, Test Accuracy: 36.67%\n",
        "\n",
        "End of Epoch 7:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0995\n",
        "\n",
        "Train Loss: 1.7199, Train Accuracy: 36.10%\n",
        "\n",
        "Test Loss: 1.7071, Test Accuracy: 36.67%\n",
        "\n",
        "Learning Rate: 0.009529\n",
        "\n",
        "Epoch: 8, Train Loss: 1.6872, Train Accuracy: 36.87%\n",
        "\n",
        "Epoch: 8, Test Loss: 1.6810, Test Accuracy: 37.11%\n",
        "\n",
        "End of Epoch 8:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.1715\n",
        "\n",
        "Train Loss: 1.6872, Train Accuracy: 36.87%\n",
        "\n",
        "Test Loss: 1.6810, Test Accuracy: 37.11%\n",
        "\n",
        "Learning Rate: 0.009388\n",
        "\n",
        "Epoch: 9, Train Loss: 1.6766, Train Accuracy: 37.21%\n",
        "\n",
        "Epoch: 9, Test Loss: 1.6630, Test Accuracy: 38.22%\n",
        "\n",
        "End of Epoch 9:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2397\n",
        "\n",
        "Train Loss: 1.6766, Train Accuracy: 37.21%\n",
        "\n",
        "Test Loss: 1.6630, Test Accuracy: 38.22%\n",
        "\n",
        "Learning Rate: 0.009229\n",
        "\n",
        "Epoch: 10, Train Loss: 1.6547, Train Accuracy: 38.17%\n",
        "\n",
        "Epoch: 10, Test Loss: 1.6745, Test Accuracy: 37.73%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 10:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3049\n",
        "\n",
        "Train Loss: 1.6547, Train Accuracy: 38.17%\n",
        "\n",
        "Test Loss: 1.6745, Test Accuracy: 37.73%\n",
        "\n",
        "Learning Rate: 0.009055\n",
        "\n",
        "Epoch: 11, Train Loss: 1.6288, Train Accuracy: 38.90%\n",
        "\n",
        "Epoch: 11, Test Loss: 1.6262, Test Accuracy: 38.95%\n",
        "\n",
        "End of Epoch 11:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3673\n",
        "\n",
        "Train Loss: 1.6288, Train Accuracy: 38.90%\n",
        "\n",
        "Test Loss: 1.6262, Test Accuracy: 38.95%\n",
        "\n",
        "Learning Rate: 0.008864\n",
        "\n",
        "Epoch: 12, Train Loss: 1.6225, Train Accuracy: 39.96%\n",
        "\n",
        "Epoch: 12, Test Loss: 1.6122, Test Accuracy: 40.24%\n",
        "\n",
        "End of Epoch 12:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4274\n",
        "\n",
        "Train Loss: 1.6225, Train Accuracy: 39.96%\n",
        "\n",
        "Test Loss: 1.6122, Test Accuracy: 40.24%\n",
        "\n",
        "Learning Rate: 0.008658\n",
        "\n",
        "Epoch: 13, Train Loss: 1.6043, Train Accuracy: 40.54%\n",
        "\n",
        "Epoch: 13, Test Loss: 1.6022, Test Accuracy: 40.79%\n",
        "\n",
        "End of Epoch 13:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4854\n",
        "\n",
        "Train Loss: 1.6043, Train Accuracy: 40.54%\n",
        "\n",
        "Test Loss: 1.6022, Test Accuracy: 40.79%\n",
        "\n",
        "Learning Rate: 0.008439\n",
        "\n",
        "Epoch: 14, Train Loss: 1.5921, Train Accuracy: 41.07%\n",
        "\n",
        "Epoch: 14, Test Loss: 1.5942, Test Accuracy: 40.78%\n",
        "\n",
        "End of Epoch 14:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5415\n",
        "\n",
        "Train Loss: 1.5921, Train Accuracy: 41.07%\n",
        "\n",
        "Test Loss: 1.5942, Test Accuracy: 40.78%\n",
        "\n",
        "Learning Rate: 0.008205\n",
        "\n",
        "Epoch: 15, Train Loss: 1.5885, Train Accuracy: 41.49%\n",
        "\n",
        "Epoch: 15, Test Loss: 1.5778, Test Accuracy: 41.84%\n",
        "\n",
        "End of Epoch 15:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5959\n",
        "\n",
        "Train Loss: 1.5885, Train Accuracy: 41.49%\n",
        "\n",
        "Test Loss: 1.5778, Test Accuracy: 41.84%\n",
        "\n",
        "Learning Rate: 0.007960\n",
        "\n",
        "Epoch: 16, Train Loss: 1.5708, Train Accuracy: 42.55%\n",
        "\n",
        "Epoch: 16, Test Loss: 1.5629, Test Accuracy: 43.02%\n",
        "\n",
        "End of Epoch 16:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6489\n",
        "\n",
        "Train Loss: 1.5708, Train Accuracy: 42.55%\n",
        "\n",
        "Test Loss: 1.5629, Test Accuracy: 43.02%\n",
        "\n",
        "Learning Rate: 0.007702\n",
        "\n",
        "Epoch: 17, Train Loss: 1.5494, Train Accuracy: 43.43%\n",
        "\n",
        "Epoch: 17, Test Loss: 1.5537, Test Accuracy: 43.44%\n",
        "\n",
        "End of Epoch 17:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7004\n",
        "\n",
        "Train Loss: 1.5494, Train Accuracy: 43.43%\n",
        "\n",
        "Test Loss: 1.5537, Test Accuracy: 43.44%\n",
        "\n",
        "Learning Rate: 0.007435\n",
        "\n",
        "Epoch: 18, Train Loss: 1.5358, Train Accuracy: 44.05%\n",
        "\n",
        "Epoch: 18, Test Loss: 1.5505, Test Accuracy: 43.52%\n",
        "\n",
        "End of Epoch 18:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7507\n",
        "\n",
        "Train Loss: 1.5358, Train Accuracy: 44.05%\n",
        "\n",
        "Test Loss: 1.5505, Test Accuracy: 43.52%\n",
        "\n",
        "Learning Rate: 0.007158\n",
        "\n",
        "Epoch: 19, Train Loss: 1.5367, Train Accuracy: 44.06%\n",
        "\n",
        "Epoch: 19, Test Loss: 1.5719, Test Accuracy: 43.02%\n",
        "\n",
        "End of Epoch 19:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7998\n",
        "\n",
        "Train Loss: 1.5367, Train Accuracy: 44.06%\n",
        "\n",
        "Test Loss: 1.5719, Test Accuracy: 43.02%\n",
        "\n",
        "Learning Rate: 0.006872\n",
        "\n",
        "Epoch: 20, Train Loss: 1.5293, Train Accuracy: 44.66%\n",
        "\n",
        "Epoch: 20, Test Loss: 1.5375, Test Accuracy: 44.53%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 20:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8479\n",
        "\n",
        "Train Loss: 1.5293, Train Accuracy: 44.66%\n",
        "\n",
        "Test Loss: 1.5375, Test Accuracy: 44.53%\n",
        "\n",
        "Learning Rate: 0.006580\n",
        "\n",
        "Epoch: 21, Train Loss: 1.5242, Train Accuracy: 44.64%\n",
        "\n",
        "Epoch: 21, Test Loss: 1.5271, Test Accuracy: 44.78%\n",
        "\n",
        "End of Epoch 21:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8949\n",
        "\n",
        "Train Loss: 1.5242, Train Accuracy: 44.64%\n",
        "\n",
        "Test Loss: 1.5271, Test Accuracy: 44.78%\n",
        "\n",
        "Learning Rate: 0.006281\n",
        "\n",
        "Epoch: 22, Train Loss: 1.5089, Train Accuracy: 45.12%\n",
        "\n",
        "Epoch: 22, Test Loss: 1.5211, Test Accuracy: 45.24%\n",
        "\n",
        "End of Epoch 22:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9410\n",
        "\n",
        "Train Loss: 1.5089, Train Accuracy: 45.12%\n",
        "\n",
        "Test Loss: 1.5211, Test Accuracy: 45.24%\n",
        "\n",
        "Learning Rate: 0.005978\n",
        "\n",
        "Epoch: 23, Train Loss: 1.5079, Train Accuracy: 45.96%\n",
        "\n",
        "Epoch: 23, Test Loss: 1.5169, Test Accuracy: 45.54%\n",
        "\n",
        "End of Epoch 23:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9863\n",
        "\n",
        "Train Loss: 1.5079, Train Accuracy: 45.96%\n",
        "\n",
        "Test Loss: 1.5169, Test Accuracy: 45.54%\n",
        "\n",
        "Learning Rate: 0.005670\n",
        "\n",
        "Epoch: 24, Train Loss: 1.5030, Train Accuracy: 46.17%\n",
        "\n",
        "Epoch: 24, Test Loss: 1.5087, Test Accuracy: 45.65%\n",
        "\n",
        "End of Epoch 24:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0307\n",
        "\n",
        "Train Loss: 1.5030, Train Accuracy: 46.17%\n",
        "\n",
        "Test Loss: 1.5087, Test Accuracy: 45.65%\n",
        "\n",
        "Learning Rate: 0.005361\n",
        "\n",
        "Epoch: 25, Train Loss: 1.5038, Train Accuracy: 46.26%\n",
        "\n",
        "Epoch: 25, Test Loss: 1.5287, Test Accuracy: 45.80%\n",
        "\n",
        "End of Epoch 25:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0743\n",
        "\n",
        "Train Loss: 1.5038, Train Accuracy: 46.26%\n",
        "\n",
        "Test Loss: 1.5287, Test Accuracy: 45.80%\n",
        "\n",
        "Learning Rate: 0.005050\n",
        "\n",
        "Epoch: 26, Train Loss: 1.4834, Train Accuracy: 47.00%\n",
        "\n",
        "Epoch: 26, Test Loss: 1.5073, Test Accuracy: 46.55%\n",
        "\n",
        "End of Epoch 26:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1172\n",
        "\n",
        "Train Loss: 1.4834, Train Accuracy: 47.00%\n",
        "\n",
        "Test Loss: 1.5073, Test Accuracy: 46.55%\n",
        "\n",
        "Learning Rate: 0.004739\n",
        "\n",
        "Epoch: 27, Train Loss: 1.4799, Train Accuracy: 46.94%\n",
        "\n",
        "Epoch: 27, Test Loss: 1.4946, Test Accuracy: 45.96%\n",
        "\n",
        "End of Epoch 27:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1594\n",
        "\n",
        "Train Loss: 1.4799, Train Accuracy: 46.94%\n",
        "\n",
        "Test Loss: 1.4946, Test Accuracy: 45.96%\n",
        "\n",
        "Learning Rate: 0.004430\n",
        "\n",
        "Epoch: 28, Train Loss: 1.4780, Train Accuracy: 46.95%\n",
        "\n",
        "Epoch: 28, Test Loss: 1.5009, Test Accuracy: 46.23%\n",
        "\n",
        "End of Epoch 28:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2010\n",
        "\n",
        "Train Loss: 1.4780, Train Accuracy: 46.95%\n",
        "\n",
        "Test Loss: 1.5009, Test Accuracy: 46.23%\n",
        "\n",
        "Learning Rate: 0.004122\n",
        "\n",
        "Epoch: 29, Train Loss: 1.4687, Train Accuracy: 47.29%\n",
        "\n",
        "Epoch: 29, Test Loss: 1.4831, Test Accuracy: 47.21%\n",
        "\n",
        "End of Epoch 29:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2420\n",
        "\n",
        "Train Loss: 1.4687, Train Accuracy: 47.29%\n",
        "\n",
        "Test Loss: 1.4831, Test Accuracy: 47.21%\n",
        "\n",
        "Learning Rate: 0.003819\n",
        "\n",
        "Epoch: 30, Train Loss: 1.4746, Train Accuracy: 47.52%\n",
        "\n",
        "Epoch: 30, Test Loss: 1.4844, Test Accuracy: 46.75%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 30:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2823\n",
        "\n",
        "Train Loss: 1.4746, Train Accuracy: 47.52%\n",
        "\n",
        "Test Loss: 1.4844, Test Accuracy: 46.75%\n",
        "\n",
        "Learning Rate: 0.003520\n",
        "\n",
        "Epoch: 31, Train Loss: 1.4749, Train Accuracy: 47.28%\n",
        "\n",
        "Epoch: 31, Test Loss: 1.4866, Test Accuracy: 46.62%\n",
        "\n",
        "End of Epoch 31:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3222\n",
        "\n",
        "Train Loss: 1.4749, Train Accuracy: 47.28%\n",
        "\n",
        "Test Loss: 1.4866, Test Accuracy: 46.62%\n",
        "\n",
        "Learning Rate: 0.003228\n",
        "\n",
        "Epoch: 32, Train Loss: 1.4650, Train Accuracy: 47.88%\n",
        "\n",
        "Epoch: 32, Test Loss: 1.4752, Test Accuracy: 47.08%\n",
        "\n",
        "End of Epoch 32:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3614\n",
        "\n",
        "Train Loss: 1.4650, Train Accuracy: 47.88%\n",
        "\n",
        "Test Loss: 1.4752, Test Accuracy: 47.08%\n",
        "\n",
        "Learning Rate: 0.002942\n",
        "\n",
        "Epoch: 33, Train Loss: 1.4634, Train Accuracy: 48.01%\n",
        "\n",
        "Epoch: 33, Test Loss: 1.4939, Test Accuracy: 47.18%\n",
        "\n",
        "End of Epoch 33:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4002\n",
        "\n",
        "Train Loss: 1.4634, Train Accuracy: 48.01%\n",
        "\n",
        "Test Loss: 1.4939, Test Accuracy: 47.18%\n",
        "\n",
        "Learning Rate: 0.002665\n",
        "\n",
        "Epoch: 34, Train Loss: 1.4659, Train Accuracy: 48.17%\n",
        "\n",
        "Epoch: 34, Test Loss: 1.4857, Test Accuracy: 47.99%\n",
        "\n",
        "End of Epoch 34:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4385\n",
        "\n",
        "Train Loss: 1.4659, Train Accuracy: 48.17%\n",
        "\n",
        "Test Loss: 1.4857, Test Accuracy: 47.99%\n",
        "\n",
        "Learning Rate: 0.002398\n",
        "\n",
        "Epoch: 35, Train Loss: 1.4711, Train Accuracy: 48.07%\n",
        "\n",
        "Epoch: 35, Test Loss: 1.4840, Test Accuracy: 47.46%\n",
        "\n",
        "End of Epoch 35:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4763\n",
        "\n",
        "Train Loss: 1.4711, Train Accuracy: 48.07%\n",
        "\n",
        "Test Loss: 1.4840, Test Accuracy: 47.46%\n",
        "\n",
        "Learning Rate: 0.002140\n",
        "\n",
        "Epoch: 36, Train Loss: 1.4639, Train Accuracy: 48.46%\n",
        "\n",
        "Epoch: 36, Test Loss: 1.4883, Test Accuracy: 47.31%\n",
        "\n",
        "End of Epoch 36:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5137\n",
        "\n",
        "Train Loss: 1.4639, Train Accuracy: 48.46%\n",
        "\n",
        "Test Loss: 1.4883, Test Accuracy: 47.31%\n",
        "\n",
        "Learning Rate: 0.001895\n",
        "\n",
        "Epoch: 37, Train Loss: 1.4601, Train Accuracy: 48.70%\n",
        "\n",
        "Epoch: 37, Test Loss: 1.4865, Test Accuracy: 47.75%\n",
        "\n",
        "End of Epoch 37:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5507\n",
        "\n",
        "Train Loss: 1.4601, Train Accuracy: 48.70%\n",
        "\n",
        "Test Loss: 1.4865, Test Accuracy: 47.75%\n",
        "\n",
        "Learning Rate: 0.001661\n",
        "\n",
        "Epoch: 38, Train Loss: 1.4631, Train Accuracy: 48.54%\n",
        "\n",
        "Epoch: 38, Test Loss: 1.4774, Test Accuracy: 48.44%\n",
        "\n",
        "End of Epoch 38:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5872\n",
        "\n",
        "Train Loss: 1.4631, Train Accuracy: 48.54%\n",
        "\n",
        "Test Loss: 1.4774, Test Accuracy: 48.44%\n",
        "\n",
        "Learning Rate: 0.001442\n",
        "\n",
        "Epoch: 39, Train Loss: 1.4624, Train Accuracy: 48.49%\n",
        "\n",
        "Epoch: 39, Test Loss: 1.4780, Test Accuracy: 47.67%\n",
        "\n",
        "End of Epoch 39:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6233\n",
        "\n",
        "Train Loss: 1.4624, Train Accuracy: 48.49%\n",
        "\n",
        "Test Loss: 1.4780, Test Accuracy: 47.67%\n",
        "\n",
        "Learning Rate: 0.001236\n",
        "\n",
        "Epoch: 40, Train Loss: 1.4651, Train Accuracy: 48.40%\n",
        "\n",
        "Epoch: 40, Test Loss: 1.4861, Test Accuracy: 48.08%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 40:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6591\n",
        "\n",
        "Train Loss: 1.4651, Train Accuracy: 48.40%\n",
        "\n",
        "Test Loss: 1.4861, Test Accuracy: 48.08%\n",
        "\n",
        "Learning Rate: 0.001045\n",
        "\n",
        "Epoch: 41, Train Loss: 1.4616, Train Accuracy: 48.73%\n",
        "\n",
        "Epoch: 41, Test Loss: 1.4785, Test Accuracy: 48.45%\n",
        "\n",
        "End of Epoch 41:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6945\n",
        "\n",
        "Train Loss: 1.4616, Train Accuracy: 48.73%\n",
        "\n",
        "Test Loss: 1.4785, Test Accuracy: 48.45%\n",
        "\n",
        "Learning Rate: 0.000871\n",
        "\n",
        "Epoch: 42, Train Loss: 1.4584, Train Accuracy: 48.65%\n",
        "\n",
        "Epoch: 42, Test Loss: 1.4870, Test Accuracy: 48.02%\n",
        "\n",
        "End of Epoch 42:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7295\n",
        "\n",
        "Train Loss: 1.4584, Train Accuracy: 48.65%\n",
        "\n",
        "Test Loss: 1.4870, Test Accuracy: 48.02%\n",
        "\n",
        "Learning Rate: 0.000712\n",
        "\n",
        "Epoch: 43, Train Loss: 1.4639, Train Accuracy: 48.74%\n",
        "\n",
        "Epoch: 43, Test Loss: 1.4808, Test Accuracy: 48.51%\n",
        "\n",
        "End of Epoch 43:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7642\n",
        "\n",
        "Train Loss: 1.4639, Train Accuracy: 48.74%\n",
        "\n",
        "Test Loss: 1.4808, Test Accuracy: 48.51%\n",
        "\n",
        "Learning Rate: 0.000571\n",
        "\n",
        "Epoch: 44, Train Loss: 1.4569, Train Accuracy: 48.59%\n",
        "\n",
        "Epoch: 44, Test Loss: 1.4850, Test Accuracy: 48.30%\n",
        "\n",
        "End of Epoch 44:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7986\n",
        "\n",
        "Train Loss: 1.4569, Train Accuracy: 48.59%\n",
        "\n",
        "Test Loss: 1.4850, Test Accuracy: 48.30%\n",
        "\n",
        "Learning Rate: 0.000448\n",
        "\n",
        "Epoch: 45, Train Loss: 1.4567, Train Accuracy: 48.92%\n",
        "\n",
        "Epoch: 45, Test Loss: 1.4794, Test Accuracy: 47.93%\n",
        "\n",
        "End of Epoch 45:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8327\n",
        "\n",
        "Train Loss: 1.4567, Train Accuracy: 48.92%\n",
        "\n",
        "Test Loss: 1.4794, Test Accuracy: 47.93%\n",
        "\n",
        "Learning Rate: 0.000342\n",
        "\n",
        "Epoch: 46, Train Loss: 1.4549, Train Accuracy: 48.82%\n",
        "\n",
        "Epoch: 46, Test Loss: 1.4790, Test Accuracy: 48.31%\n",
        "\n",
        "End of Epoch 46:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8664\n",
        "\n",
        "Train Loss: 1.4549, Train Accuracy: 48.82%\n",
        "\n",
        "Test Loss: 1.4790, Test Accuracy: 48.31%\n",
        "\n",
        "Learning Rate: 0.000256\n",
        "\n",
        "Epoch: 47, Train Loss: 1.4617, Train Accuracy: 48.86%\n",
        "\n",
        "Epoch: 47, Test Loss: 1.4842, Test Accuracy: 48.11%\n",
        "\n",
        "End of Epoch 47:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8998\n",
        "\n",
        "Train Loss: 1.4617, Train Accuracy: 48.86%\n",
        "\n",
        "Test Loss: 1.4842, Test Accuracy: 48.11%\n",
        "\n",
        "Learning Rate: 0.000188\n",
        "\n",
        "Epoch: 48, Train Loss: 1.4571, Train Accuracy: 48.82%\n",
        "\n",
        "Epoch: 48, Test Loss: 1.4810, Test Accuracy: 48.47%\n",
        "\n",
        "End of Epoch 48:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9330\n",
        "\n",
        "Train Loss: 1.4571, Train Accuracy: 48.82%\n",
        "\n",
        "Test Loss: 1.4810, Test Accuracy: 48.47%\n",
        "\n",
        "Learning Rate: 0.000139\n"
      ],
      "metadata": {
        "id": "FTCZ0603KF8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oq-RbRbxShWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xfz84EMs-ygq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LR=0.1- GROUPED GRAD CLIPPING, PARAM AVERAGING, LR SCHEDULER, BATCH SIZE=256"
      ],
      "metadata": {
        "id": "rNQ6Xy2v4Gqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the Cosine Annealing Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
        "\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\\n\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()  # Adjusts the learning rate based on the cosine schedule\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}: \\n')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f} \\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% \\n')\n",
        "    # Log current learning rate (optional)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Learning Rate: {current_lr:.6f} \\n')"
      ],
      "metadata": {
        "id": "M9M4LV5t4GY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_accuracy_vals)+1), train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(range(1, len(test_accuracy_vals)+1), test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_loss_vals)+1), train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(range(1, len(test_loss_vals)+1), test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EObwDud09aoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LR=0.1, batch_size= 128---- LR_batch_dec"
      ],
      "metadata": {
        "id": "6-ysuIy16yZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the Cosine Annealing Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
        "\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\\n\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()  # Adjusts the learning rate based on the cosine schedule\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}: \\n')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f} \\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% \\n')\n",
        "    # Log current learning rate (optional)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Learning Rate: {current_lr:.6f} \\n')\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z8VI7W0l6yMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train Loss: 1.9180, Train Accuracy: 26.37%\n",
        "\n",
        "Epoch: 1, Test Loss: 1.8123, Test Accuracy: 28.80%\n",
        "\n",
        "End of Epoch 1:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.6033\n",
        "\n",
        "Train Loss: 1.9180, Train Accuracy: 26.37%\n",
        "\n",
        "Test Loss: 1.8123, Test Accuracy: 28.80%\n",
        "\n",
        "Learning Rate: 0.099901\n",
        "\n",
        "Epoch: 2, Train Loss: 1.7807, Train Accuracy: 32.29%\n",
        "\n",
        "Epoch: 2, Test Loss: 1.7698, Test Accuracy: 32.87%\n",
        "\n",
        "End of Epoch 2:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.7433\n",
        "\n",
        "Train Loss: 1.7807, Train Accuracy: 32.29%\n",
        "\n",
        "Test Loss: 1.7698, Test Accuracy: 32.87%\n",
        "\n",
        "Learning Rate: 0.099606\n",
        "\n",
        "Epoch: 3, Train Loss: 1.7819, Train Accuracy: 32.29%\n",
        "\n",
        "Epoch: 3, Test Loss: 1.7588, Test Accuracy: 33.50%\n",
        "\n",
        "End of Epoch 3:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.8495\n",
        "\n",
        "Train Loss: 1.7819, Train Accuracy: 32.29%\n",
        "\n",
        "Test Loss: 1.7588, Test Accuracy: 33.50%\n",
        "\n",
        "Learning Rate: 0.099115\n",
        "\n",
        "Epoch: 4, Train Loss: 1.7699, Train Accuracy: 32.86%\n",
        "\n",
        "Epoch: 4, Test Loss: 1.8094, Test Accuracy: 32.56%\n",
        "\n",
        "End of Epoch 4:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.9406\n",
        "\n",
        "Train Loss: 1.7699, Train Accuracy: 32.86%\n",
        "\n",
        "Test Loss: 1.8094, Test Accuracy: 32.56%\n",
        "\n",
        "Learning Rate: 0.098431\n",
        "\n",
        "Epoch: 5, Train Loss: 1.8145, Train Accuracy: 31.76%\n",
        "\n",
        "Epoch: 5, Test Loss: 1.8102, Test Accuracy: 31.65%\n",
        "\n",
        "End of Epoch 5:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0225\n",
        "\n",
        "Train Loss: 1.8145, Train Accuracy: 31.76%\n",
        "\n",
        "Test Loss: 1.8102, Test Accuracy: 31.65%\n",
        "\n",
        "Learning Rate: 0.097555\n",
        "\n",
        "Epoch: 6, Train Loss: 1.8232, Train Accuracy: 30.63%\n",
        "\n",
        "Epoch: 6, Test Loss: 1.8604, Test Accuracy: 31.77%\n",
        "\n",
        "End of Epoch 6:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0981\n",
        "\n",
        "Train Loss: 1.8232, Train Accuracy: 30.63%\n",
        "\n",
        "Test Loss: 1.8604, Test Accuracy: 31.77%\n",
        "\n",
        "Learning Rate: 0.096492\n",
        "\n",
        "Epoch: 7, Train Loss: 1.8230, Train Accuracy: 31.23%\n",
        "\n",
        "Epoch: 7, Test Loss: 1.7955, Test Accuracy: 33.30%\n",
        "\n",
        "End of Epoch 7:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.1690\n",
        "\n",
        "Train Loss: 1.8230, Train Accuracy: 31.23%\n",
        "\n",
        "Test Loss: 1.7955, Test Accuracy: 33.30%\n",
        "\n",
        "Learning Rate: 0.095246\n",
        "\n",
        "Epoch: 8, Train Loss: 1.8380, Train Accuracy: 31.19%\n",
        "\n",
        "Epoch: 8, Test Loss: 1.7774, Test Accuracy: 33.16%\n",
        "\n",
        "End of Epoch 8:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2360\n",
        "\n",
        "Train Loss: 1.8380, Train Accuracy: 31.19%\n",
        "\n",
        "Test Loss: 1.7774, Test Accuracy: 33.16%\n",
        "\n",
        "Learning Rate: 0.093822\n",
        "\n",
        "Epoch: 9, Train Loss: 1.8463, Train Accuracy: 30.99%\n",
        "\n",
        "Epoch: 9, Test Loss: 1.7571, Test Accuracy: 32.41%\n",
        "\n",
        "End of Epoch 9:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2998\n",
        "\n",
        "Train Loss: 1.8463, Train Accuracy: 30.99%\n",
        "\n",
        "Test Loss: 1.7571, Test Accuracy: 32.41%\n",
        "\n",
        "Learning Rate: 0.092224\n",
        "\n",
        "Epoch: 10, Train Loss: 1.8464, Train Accuracy: 30.75%\n",
        "\n",
        "Epoch: 10, Test Loss: 1.7636, Test Accuracy: 32.50%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 10:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3610\n",
        "\n",
        "Train Loss: 1.8464, Train Accuracy: 30.75%\n",
        "\n",
        "Test Loss: 1.7636, Test Accuracy: 32.50%\n",
        "\n",
        "Learning Rate: 0.090460\n",
        "\n",
        "Epoch: 11, Train Loss: 1.8515, Train Accuracy: 31.07%\n",
        "\n",
        "Epoch: 11, Test Loss: 1.9341, Test Accuracy: 28.63%\n",
        "\n",
        "End of Epoch 11:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4199\n",
        "\n",
        "Train Loss: 1.8515, Train Accuracy: 31.07%\n",
        "\n",
        "Test Loss: 1.9341, Test Accuracy: 28.63%\n",
        "\n",
        "Learning Rate: 0.088537\n",
        "\n",
        "Epoch: 12, Train Loss: 1.8476, Train Accuracy: 30.44%\n",
        "\n",
        "Epoch: 12, Test Loss: 1.8382, Test Accuracy: 31.64%\n",
        "\n",
        "End of Epoch 12:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4767\n",
        "\n",
        "Train Loss: 1.8476, Train Accuracy: 30.44%\n",
        "\n",
        "Test Loss: 1.8382, Test Accuracy: 31.64%\n",
        "\n",
        "Learning Rate: 0.086462\n",
        "\n",
        "Epoch: 13, Train Loss: 1.8858, Train Accuracy: 29.93%\n",
        "\n",
        "Epoch: 13, Test Loss: 1.8415, Test Accuracy: 31.81%\n",
        "\n",
        "End of Epoch 13:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5316\n",
        "\n",
        "Train Loss: 1.8858, Train Accuracy: 29.93%\n",
        "\n",
        "Test Loss: 1.8415, Test Accuracy: 31.81%\n",
        "\n",
        "Learning Rate: 0.084243\n",
        "\n",
        "Epoch: 14, Train Loss: 1.8735, Train Accuracy: 30.34%\n",
        "\n",
        "Epoch: 14, Test Loss: 1.8017, Test Accuracy: 31.69%\n",
        "\n",
        "End of Epoch 14:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5850\n",
        "\n",
        "Train Loss: 1.8735, Train Accuracy: 30.34%\n",
        "\n",
        "Test Loss: 1.8017, Test Accuracy: 31.69%\n",
        "\n",
        "Learning Rate: 0.081889\n",
        "\n",
        "Epoch: 15, Train Loss: 1.8551, Train Accuracy: 30.87%\n",
        "\n",
        "Epoch: 15, Test Loss: 1.9331, Test Accuracy: 29.93%\n",
        "\n",
        "End of Epoch 15:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6369\n",
        "\n",
        "Train Loss: 1.8551, Train Accuracy: 30.87%\n",
        "\n",
        "Test Loss: 1.9331, Test Accuracy: 29.93%\n",
        "\n",
        "Learning Rate: 0.079410\n",
        "\n",
        "Epoch: 16, Train Loss: 1.8570, Train Accuracy: 30.68%\n",
        "\n",
        "Epoch: 16, Test Loss: 1.7779, Test Accuracy: 32.82%\n",
        "\n",
        "End of Epoch 16:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6874\n",
        "\n",
        "Train Loss: 1.8570, Train Accuracy: 30.68%\n",
        "\n",
        "Test Loss: 1.7779, Test Accuracy: 32.82%\n",
        "\n",
        "Learning Rate: 0.076815\n",
        "\n",
        "Epoch: 17, Train Loss: 1.8527, Train Accuracy: 30.49%\n",
        "\n",
        "Epoch: 17, Test Loss: 1.8579, Test Accuracy: 32.33%\n",
        "\n",
        "End of Epoch 17:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7367\n",
        "\n",
        "Train Loss: 1.8527, Train Accuracy: 30.49%\n",
        "\n",
        "Test Loss: 1.8579, Test Accuracy: 32.33%\n",
        "\n",
        "Learning Rate: 0.074114\n",
        "\n",
        "Epoch: 18, Train Loss: 1.8671, Train Accuracy: 30.63%\n",
        "\n",
        "Epoch: 18, Test Loss: 1.8824, Test Accuracy: 30.37%\n",
        "\n",
        "End of Epoch 18:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7849\n",
        "\n",
        "Train Loss: 1.8671, Train Accuracy: 30.63%\n",
        "\n",
        "Test Loss: 1.8824, Test Accuracy: 30.37%\n",
        "\n",
        "Learning Rate: 0.071318\n",
        "\n",
        "Epoch: 19, Train Loss: 1.8510, Train Accuracy: 31.60%\n",
        "\n",
        "Epoch: 19, Test Loss: 1.8019, Test Accuracy: 33.34%\n",
        "\n",
        "End of Epoch 19:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8320\n",
        "\n",
        "Train Loss: 1.8510, Train Accuracy: 31.60%\n",
        "\n",
        "Test Loss: 1.8019, Test Accuracy: 33.34%\n",
        "\n",
        "Learning Rate: 0.068438\n",
        "\n",
        "Epoch: 20, Train Loss: 1.8582, Train Accuracy: 31.57%\n",
        "\n",
        "Epoch: 20, Test Loss: 1.8466, Test Accuracy: 31.46%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 20:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8782\n",
        "\n",
        "Train Loss: 1.8582, Train Accuracy: 31.57%\n",
        "\n",
        "Test Loss: 1.8466, Test Accuracy: 31.46%\n",
        "\n",
        "Learning Rate: 0.065485\n",
        "\n",
        "Epoch: 21, Train Loss: 1.8447, Train Accuracy: 31.48%\n",
        "\n",
        "Epoch: 21, Test Loss: 2.1607, Test Accuracy: 26.61%\n",
        "\n",
        "End of Epoch 21:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9235\n",
        "\n",
        "Train Loss: 1.8447, Train Accuracy: 31.48%\n",
        "\n",
        "Test Loss: 2.1607, Test Accuracy: 26.61%\n",
        "\n",
        "Learning Rate: 0.062472\n",
        "\n",
        "Epoch: 22, Train Loss: 1.8450, Train Accuracy: 31.48%\n",
        "\n",
        "Epoch: 22, Test Loss: 1.7801, Test Accuracy: 32.73%\n",
        "\n",
        "End of Epoch 22:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9678\n",
        "\n",
        "Train Loss: 1.8450, Train Accuracy: 31.48%\n",
        "\n",
        "Test Loss: 1.7801, Test Accuracy: 32.73%\n",
        "\n",
        "Learning Rate: 0.059410\n",
        "\n",
        "Epoch: 23, Train Loss: 1.8523, Train Accuracy: 31.77%\n",
        "\n",
        "Epoch: 23, Test Loss: 1.9541, Test Accuracy: 28.76%\n",
        "\n",
        "End of Epoch 23:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0114\n",
        "\n",
        "Train Loss: 1.8523, Train Accuracy: 31.77%\n",
        "\n",
        "Test Loss: 1.9541, Test Accuracy: 28.76%\n",
        "\n",
        "Learning Rate: 0.056310\n",
        "\n",
        "Epoch: 24, Train Loss: 1.8268, Train Accuracy: 31.99%\n",
        "\n",
        "Epoch: 24, Test Loss: 1.8233, Test Accuracy: 32.06%\n",
        "\n",
        "End of Epoch 24:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0543\n",
        "\n",
        "Train Loss: 1.8268, Train Accuracy: 31.99%\n",
        "\n",
        "Test Loss: 1.8233, Test Accuracy: 32.06%\n",
        "\n",
        "Learning Rate: 0.053186\n",
        "\n",
        "Epoch: 25, Train Loss: 1.8165, Train Accuracy: 32.44%\n",
        "\n",
        "Epoch: 25, Test Loss: 1.8189, Test Accuracy: 32.27%\n",
        "\n",
        "End of Epoch 25:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0964\n",
        "\n",
        "Train Loss: 1.8165, Train Accuracy: 32.44%\n",
        "\n",
        "Test Loss: 1.8189, Test Accuracy: 32.27%\n",
        "\n",
        "Learning Rate: 0.050050\n",
        "\n",
        "Epoch: 26, Train Loss: 1.8090, Train Accuracy: 32.74%\n",
        "\n",
        "Epoch: 26, Test Loss: 1.8026, Test Accuracy: 33.53%\n",
        "\n",
        "End of Epoch 26:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1379\n",
        "\n",
        "Train Loss: 1.8090, Train Accuracy: 32.74%\n",
        "\n",
        "Test Loss: 1.8026, Test Accuracy: 33.53%\n",
        "\n",
        "Learning Rate: 0.046914\n",
        "\n",
        "Epoch: 27, Train Loss: 1.8241, Train Accuracy: 32.66%\n",
        "\n",
        "Epoch: 27, Test Loss: 1.8548, Test Accuracy: 32.61%\n",
        "\n",
        "End of Epoch 27:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1787\n",
        "\n",
        "Train Loss: 1.8241, Train Accuracy: 32.66%\n",
        "\n",
        "Test Loss: 1.8548, Test Accuracy: 32.61%\n",
        "\n",
        "Learning Rate: 0.043790\n",
        "\n",
        "Epoch: 28, Train Loss: 1.8100, Train Accuracy: 32.84%\n",
        "\n",
        "Epoch: 28, Test Loss: 1.7679, Test Accuracy: 35.34%\n",
        "\n",
        "End of Epoch 28:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2190\n",
        "\n",
        "Train Loss: 1.8100, Train Accuracy: 32.84%\n",
        "\n",
        "Test Loss: 1.7679, Test Accuracy: 35.34%\n",
        "\n",
        "Learning Rate: 0.040690\n",
        "\n",
        "Epoch: 29, Train Loss: 1.8250, Train Accuracy: 32.72%\n",
        "\n",
        "Epoch: 29, Test Loss: 1.8393, Test Accuracy: 33.23%\n",
        "\n",
        "End of Epoch 29:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2586\n",
        "\n",
        "Train Loss: 1.8250, Train Accuracy: 32.72%\n",
        "\n",
        "Test Loss: 1.8393, Test Accuracy: 33.23%\n",
        "\n",
        "Learning Rate: 0.037628\n",
        "\n",
        "Epoch: 30, Train Loss: 1.8170, Train Accuracy: 32.97%\n",
        "\n",
        "Epoch: 30, Test Loss: 1.8097, Test Accuracy: 34.50%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 30:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2978\n",
        "\n",
        "Train Loss: 1.8170, Train Accuracy: 32.97%\n",
        "\n",
        "Test Loss: 1.8097, Test Accuracy: 34.50%\n",
        "\n",
        "Learning Rate: 0.034615\n",
        "\n",
        "Epoch: 31, Train Loss: 1.8205, Train Accuracy: 33.29%\n",
        "\n",
        "Epoch: 31, Test Loss: 1.7701, Test Accuracy: 33.79%\n",
        "\n",
        "End of Epoch 31:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3364\n",
        "\n",
        "Train Loss: 1.8205, Train Accuracy: 33.29%\n",
        "\n",
        "Test Loss: 1.7701, Test Accuracy: 33.79%\n",
        "\n",
        "Learning Rate: 0.031662\n",
        "\n",
        "Epoch: 32, Train Loss: 1.8012, Train Accuracy: 33.87%\n",
        "\n",
        "Epoch: 32, Test Loss: 1.8103, Test Accuracy: 32.51%\n",
        "\n",
        "End of Epoch 32:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3745\n",
        "\n",
        "Train Loss: 1.8012, Train Accuracy: 33.87%\n",
        "\n",
        "Test Loss: 1.8103, Test Accuracy: 32.51%\n",
        "\n",
        "Learning Rate: 0.028782\n",
        "\n",
        "Epoch: 33, Train Loss: 1.7929, Train Accuracy: 34.10%\n",
        "\n",
        "Epoch: 33, Test Loss: 1.7582, Test Accuracy: 35.62%\n",
        "\n",
        "End of Epoch 33:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4121\n",
        "\n",
        "Train Loss: 1.7929, Train Accuracy: 34.10%\n",
        "\n",
        "Test Loss: 1.7582, Test Accuracy: 35.62%\n",
        "\n",
        "Learning Rate: 0.025986\n",
        "\n",
        "Epoch: 34, Train Loss: 1.7837, Train Accuracy: 34.75%\n",
        "\n",
        "Epoch: 34, Test Loss: 1.7791, Test Accuracy: 35.37%\n",
        "\n",
        "End of Epoch 34:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4492\n",
        "\n",
        "Train Loss: 1.7837, Train Accuracy: 34.75%\n",
        "\n",
        "Test Loss: 1.7791, Test Accuracy: 35.37%\n",
        "\n",
        "Learning Rate: 0.023285\n",
        "\n",
        "Epoch: 35, Train Loss: 1.7857, Train Accuracy: 34.93%\n",
        "\n",
        "Epoch: 35, Test Loss: 1.7411, Test Accuracy: 35.55%\n",
        "\n",
        "End of Epoch 35:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4860\n",
        "\n",
        "Train Loss: 1.7857, Train Accuracy: 34.93%\n",
        "\n",
        "Test Loss: 1.7411, Test Accuracy: 35.55%\n",
        "\n",
        "Learning Rate: 0.020690\n",
        "\n",
        "Epoch: 36, Train Loss: 1.7746, Train Accuracy: 35.35%\n",
        "\n",
        "Epoch: 36, Test Loss: 1.7544, Test Accuracy: 36.21%\n",
        "\n",
        "End of Epoch 36:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5223\n",
        "\n",
        "Train Loss: 1.7746, Train Accuracy: 35.35%\n",
        "\n",
        "Test Loss: 1.7544, Test Accuracy: 36.21%\n",
        "\n",
        "Learning Rate: 0.018211\n",
        "\n",
        "Epoch: 37, Train Loss: 1.7757, Train Accuracy: 34.88%\n",
        "\n",
        "Epoch: 37, Test Loss: 1.7911, Test Accuracy: 35.47%\n",
        "\n",
        "End of Epoch 37:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5582\n",
        "\n",
        "Train Loss: 1.7757, Train Accuracy: 34.88%\n",
        "\n",
        "Test Loss: 1.7911, Test Accuracy: 35.47%\n",
        "\n",
        "Learning Rate: 0.015857\n",
        "\n",
        "Epoch: 38, Train Loss: 1.7627, Train Accuracy: 35.75%\n",
        "\n",
        "Epoch: 38, Test Loss: 1.7289, Test Accuracy: 37.60%\n",
        "\n",
        "End of Epoch 38:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5937\n",
        "\n",
        "Train Loss: 1.7627, Train Accuracy: 35.75%\n",
        "\n",
        "Test Loss: 1.7289, Test Accuracy: 37.60%\n",
        "\n",
        "Learning Rate: 0.013638\n",
        "\n",
        "Epoch: 39, Train Loss: 1.7612, Train Accuracy: 36.12%\n",
        "\n",
        "Epoch: 39, Test Loss: 1.7700, Test Accuracy: 36.82%\n",
        "\n",
        "End of Epoch 39:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6289\n",
        "\n",
        "Train Loss: 1.7612, Train Accuracy: 36.12%\n",
        "\n",
        "Test Loss: 1.7700, Test Accuracy: 36.82%\n",
        "\n",
        "Learning Rate: 0.011563\n",
        "\n",
        "Epoch: 40, Train Loss: 1.7469, Train Accuracy: 36.44%\n",
        "\n",
        "Epoch: 40, Test Loss: 1.7467, Test Accuracy: 37.21%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 40:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6637\n",
        "\n",
        "Train Loss: 1.7469, Train Accuracy: 36.44%\n",
        "\n",
        "Test Loss: 1.7467, Test Accuracy: 37.21%\n",
        "\n",
        "Learning Rate: 0.009640\n",
        "\n",
        "Epoch: 41, Train Loss: 1.7467, Train Accuracy: 36.72%\n",
        "\n",
        "Epoch: 41, Test Loss: 1.7222, Test Accuracy: 38.28%\n",
        "\n",
        "End of Epoch 41:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6981\n",
        "\n",
        "Train Loss: 1.7467, Train Accuracy: 36.72%\n",
        "\n",
        "Test Loss: 1.7222, Test Accuracy: 38.28%\n",
        "\n",
        "Learning Rate: 0.007876\n",
        "\n",
        "Epoch: 42, Train Loss: 1.7384, Train Accuracy: 37.00%\n",
        "\n",
        "Epoch: 42, Test Loss: 1.7755, Test Accuracy: 37.00%\n",
        "\n",
        "End of Epoch 42:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7322\n",
        "\n",
        "Train Loss: 1.7384, Train Accuracy: 37.00%\n",
        "\n",
        "Test Loss: 1.7755, Test Accuracy: 37.00%\n",
        "\n",
        "Learning Rate: 0.006278\n",
        "\n",
        "Epoch: 43, Train Loss: 1.7228, Train Accuracy: 37.70%\n",
        "\n",
        "Epoch: 43, Test Loss: 1.7159, Test Accuracy: 38.25%\n",
        "\n",
        "End of Epoch 43:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7660\n",
        "\n",
        "Train Loss: 1.7228, Train Accuracy: 37.70%\n",
        "\n",
        "Test Loss: 1.7159, Test Accuracy: 38.25%\n",
        "\n",
        "Learning Rate: 0.004854\n",
        "\n",
        "Epoch: 44, Train Loss: 1.7181, Train Accuracy: 37.82%\n",
        "\n",
        "Epoch: 44, Test Loss: 1.7201, Test Accuracy: 38.22%\n",
        "\n",
        "End of Epoch 44:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7995\n",
        "\n",
        "Train Loss: 1.7181, Train Accuracy: 37.82%\n",
        "\n",
        "Test Loss: 1.7201, Test Accuracy: 38.22%\n",
        "\n",
        "Learning Rate: 0.003608\n",
        "\n",
        "Epoch: 45, Train Loss: 1.7198, Train Accuracy: 37.90%\n",
        "\n",
        "Epoch: 45, Test Loss: 1.7203, Test Accuracy: 38.46%\n",
        "\n",
        "End of Epoch 45:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8326\n",
        "\n",
        "Train Loss: 1.7198, Train Accuracy: 37.90%\n",
        "\n",
        "Test Loss: 1.7203, Test Accuracy: 38.46%\n",
        "\n",
        "Learning Rate: 0.002545\n",
        "\n",
        "Epoch: 46, Train Loss: 1.7162, Train Accuracy: 38.16%\n",
        "\n",
        "Epoch: 46, Test Loss: 1.7223, Test Accuracy: 38.98%\n",
        "\n",
        "End of Epoch 46:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8655\n",
        "\n",
        "Train Loss: 1.7162, Train Accuracy: 38.16%\n",
        "\n",
        "Test Loss: 1.7223, Test Accuracy: 38.98%\n",
        "\n",
        "Learning Rate: 0.001669\n",
        "\n",
        "Epoch: 47, Train Loss: 1.7230, Train Accuracy: 38.31%\n",
        "\n",
        "Epoch: 47, Test Loss: 1.7128, Test Accuracy: 39.18%\n",
        "\n",
        "End of Epoch 47:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8981\n",
        "\n",
        "Train Loss: 1.7230, Train Accuracy: 38.31%\n",
        "\n",
        "Test Loss: 1.7128, Test Accuracy: 39.18%\n",
        "\n",
        "Learning Rate: 0.000985\n",
        "\n",
        "Epoch: 48, Train Loss: 1.7080, Train Accuracy: 38.59%\n",
        "\n",
        "Epoch: 48, Test Loss: 1.7153, Test Accuracy: 38.81%\n",
        "\n",
        "End of Epoch 48:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9304\n",
        "\n",
        "Train Loss: 1.7080, Train Accuracy: 38.59%\n",
        "\n",
        "Test Loss: 1.7153, Test Accuracy: 38.81%\n",
        "\n",
        "Learning Rate: 0.000494\n",
        "\n",
        "Epoch: 49, Train Loss: 1.7177, Train Accuracy: 38.17%\n",
        "\n",
        "Epoch: 49, Test Loss: 1.7103, Test Accuracy: 39.13%\n",
        "\n",
        "End of Epoch 49:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9625\n",
        "\n",
        "Train Loss: 1.7177, Train Accuracy: 38.17%\n",
        "\n",
        "Test Loss: 1.7103, Test Accuracy: 39.13%\n",
        "\n",
        "Learning Rate: 0.000199"
      ],
      "metadata": {
        "id": "WSxIvy6KYDuy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1lJpfkXsYDeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AENZiy7G6x81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEARNING RATE=0.001"
      ],
      "metadata": {
        "id": "14bjACFw3YRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=8, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the Cosine Annealing Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
        "\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\\n\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()  # Adjusts the learning rate based on the cosine schedule\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}: \\n')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f} \\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% \\n')\n",
        "    # Log current learning rate (optional)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Learning Rate: {current_lr:.6f} \\n')"
      ],
      "metadata": {
        "id": "VIZwSuifKsGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train Loss: 2.2874, Train Accuracy: 14.78%\n",
        "\n",
        "Epoch: 1, Test Loss: 2.2334, Test Accuracy: 18.96%\n",
        "\n",
        "End of Epoch 1:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.4951\n",
        "\n",
        "Train Loss: 2.2874, Train Accuracy: 14.78%\n",
        "\n",
        "Test Loss: 2.2334, Test Accuracy: 18.96%\n",
        "\n",
        "Learning Rate: 0.000999\n",
        "\n",
        "Epoch: 2, Train Loss: 2.1694, Train Accuracy: 19.82%\n",
        "\n",
        "Epoch: 2, Test Loss: 2.1123, Test Accuracy: 19.66%\n",
        "\n",
        "End of Epoch 2:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.6385\n",
        "\n",
        "Train Loss: 2.1694, Train Accuracy: 19.82%\n",
        "\n",
        "Test Loss: 2.1123, Test Accuracy: 19.66%\n",
        "\n",
        "Learning Rate: 0.000996\n",
        "\n",
        "Epoch: 3, Train Loss: 2.0807, Train Accuracy: 20.35%\n",
        "\n",
        "Epoch: 3, Test Loss: 2.0540, Test Accuracy: 21.50%\n",
        "\n",
        "End of Epoch 3:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.7527\n",
        "\n",
        "Train Loss: 2.0807, Train Accuracy: 20.35%\n",
        "\n",
        "Test Loss: 2.0540, Test Accuracy: 21.50%\n",
        "\n",
        "Learning Rate: 0.000992\n",
        "\n",
        "Epoch: 4, Train Loss: 2.0389, Train Accuracy: 22.25%\n",
        "\n",
        "Epoch: 4, Test Loss: 2.0201, Test Accuracy: 23.56%\n",
        "\n",
        "End of Epoch 4:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.8518\n",
        "\n",
        "Train Loss: 2.0389, Train Accuracy: 22.25%\n",
        "\n",
        "Test Loss: 2.0201, Test Accuracy: 23.56%\n",
        "\n",
        "Learning Rate: 0.000986\n",
        "\n",
        "Epoch: 5, Train Loss: 2.0073, Train Accuracy: 24.25%\n",
        "\n",
        "Epoch: 5, Test Loss: 1.9902, Test Accuracy: 25.17%\n",
        "\n",
        "End of Epoch 5:\n",
        "\n",
        "Privacy Budget: Epsilon = 0.9411\n",
        "\n",
        "Train Loss: 2.0073, Train Accuracy: 24.25%\n",
        "\n",
        "Test Loss: 1.9902, Test Accuracy: 25.17%\n",
        "\n",
        "Learning Rate: 0.000978\n",
        "\n",
        "Epoch: 6, Train Loss: 1.9798, Train Accuracy: 25.92%\n",
        "\n",
        "Epoch: 6, Test Loss: 1.9604, Test Accuracy: 26.13%\n",
        "\n",
        "End of Epoch 6:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0231\n",
        "\n",
        "Train Loss: 1.9798, Train Accuracy: 25.92%\n",
        "\n",
        "Test Loss: 1.9604, Test Accuracy: 26.13%\n",
        "\n",
        "Learning Rate: 0.000968\n",
        "\n",
        "Epoch: 7, Train Loss: 1.9463, Train Accuracy: 26.69%\n",
        "\n",
        "Epoch: 7, Test Loss: 1.9327, Test Accuracy: 28.26%\n",
        "\n",
        "End of Epoch 7:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0995\n",
        "\n",
        "Train Loss: 1.9463, Train Accuracy: 26.69%\n",
        "\n",
        "Test Loss: 1.9327, Test Accuracy: 28.26%\n",
        "\n",
        "Learning Rate: 0.000957\n",
        "\n",
        "Epoch: 8, Train Loss: 1.9198, Train Accuracy: 28.85%\n",
        "\n",
        "Epoch: 8, Test Loss: 1.9055, Test Accuracy: 29.92%\n",
        "\n",
        "End of Epoch 8:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.1715\n",
        "\n",
        "Train Loss: 1.9198, Train Accuracy: 28.85%\n",
        "\n",
        "Test Loss: 1.9055, Test Accuracy: 29.92%\n",
        "\n",
        "Learning Rate: 0.000944\n",
        "\n",
        "Epoch: 9, Train Loss: 1.8999, Train Accuracy: 30.20%\n",
        "\n",
        "Epoch: 9, Test Loss: 1.8823, Test Accuracy: 31.61%\n",
        "\n",
        "End of Epoch 9:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2397\n",
        "\n",
        "Train Loss: 1.8999, Train Accuracy: 30.20%\n",
        "\n",
        "Test Loss: 1.8823, Test Accuracy: 31.61%\n",
        "\n",
        "Learning Rate: 0.000930\n",
        "\n",
        "Epoch: 10, Train Loss: 1.8697, Train Accuracy: 31.17%\n",
        "\n",
        "Epoch: 10, Test Loss: 1.8622, Test Accuracy: 31.11%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 10:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3049\n",
        "\n",
        "Train Loss: 1.8697, Train Accuracy: 31.17%\n",
        "\n",
        "Test Loss: 1.8622, Test Accuracy: 31.11%\n",
        "\n",
        "Learning Rate: 0.000914\n",
        "\n",
        "Epoch: 11, Train Loss: 1.8475, Train Accuracy: 31.35%\n",
        "\n",
        "Epoch: 11, Test Loss: 1.8435, Test Accuracy: 32.31%\n",
        "\n",
        "End of Epoch 11:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3673\n",
        "\n",
        "Train Loss: 1.8475, Train Accuracy: 31.35%\n",
        "\n",
        "Test Loss: 1.8435, Test Accuracy: 32.31%\n",
        "\n",
        "Learning Rate: 0.000897\n",
        "\n",
        "Epoch: 12, Train Loss: 1.8368, Train Accuracy: 31.87%\n",
        "\n",
        "Epoch: 12, Test Loss: 1.8273, Test Accuracy: 32.48%\n",
        "\n",
        "End of Epoch 12:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4274\n",
        "\n",
        "Train Loss: 1.8368, Train Accuracy: 31.87%\n",
        "\n",
        "Test Loss: 1.8273, Test Accuracy: 32.48%\n",
        "\n",
        "Learning Rate: 0.000878\n",
        "\n",
        "Epoch: 13, Train Loss: 1.8245, Train Accuracy: 32.49%\n",
        "\n",
        "Epoch: 13, Test Loss: 1.8146, Test Accuracy: 33.37%\n",
        "\n",
        "End of Epoch 13:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4854\n",
        "\n",
        "Train Loss: 1.8245, Train Accuracy: 32.49%\n",
        "\n",
        "Test Loss: 1.8146, Test Accuracy: 33.37%\n",
        "\n",
        "Learning Rate: 0.000858\n",
        "\n",
        "Epoch: 14, Train Loss: 1.8103, Train Accuracy: 32.94%\n",
        "\n",
        "Epoch: 14, Test Loss: 1.8014, Test Accuracy: 33.62%\n",
        "\n",
        "End of Epoch 14:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5415\n",
        "\n",
        "Train Loss: 1.8103, Train Accuracy: 32.94%\n",
        "\n",
        "Test Loss: 1.8014, Test Accuracy: 33.62%\n",
        "\n",
        "Learning Rate: 0.000837\n",
        "\n",
        "Epoch: 15, Train Loss: 1.7910, Train Accuracy: 33.99%\n",
        "\n",
        "Epoch: 15, Test Loss: 1.7915, Test Accuracy: 33.67%\n",
        "\n",
        "End of Epoch 15:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5959\n",
        "\n",
        "Train Loss: 1.7910, Train Accuracy: 33.99%\n",
        "\n",
        "Test Loss: 1.7915, Test Accuracy: 33.67%\n",
        "\n",
        "Learning Rate: 0.000815\n",
        "\n",
        "Epoch: 16, Train Loss: 1.7821, Train Accuracy: 33.77%\n",
        "\n",
        "Epoch: 16, Test Loss: 1.7806, Test Accuracy: 34.00%\n",
        "\n",
        "End of Epoch 16:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6489\n",
        "\n",
        "Train Loss: 1.7821, Train Accuracy: 33.77%\n",
        "\n",
        "Test Loss: 1.7806, Test Accuracy: 34.00%\n",
        "\n",
        "Learning Rate: 0.000791\n",
        "\n",
        "Epoch: 17, Train Loss: 1.7722, Train Accuracy: 34.14%\n",
        "\n",
        "Epoch: 17, Test Loss: 1.7714, Test Accuracy: 34.34%\n",
        "\n",
        "End of Epoch 17:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7004\n",
        "\n",
        "Train Loss: 1.7722, Train Accuracy: 34.14%\n",
        "\n",
        "Test Loss: 1.7714, Test Accuracy: 34.34%\n",
        "\n",
        "Learning Rate: 0.000767\n",
        "\n",
        "Epoch: 18, Train Loss: 1.7644, Train Accuracy: 34.03%\n",
        "\n",
        "Epoch: 18, Test Loss: 1.7621, Test Accuracy: 34.72%\n",
        "\n",
        "End of Epoch 18:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7507\n",
        "\n",
        "Train Loss: 1.7644, Train Accuracy: 34.03%\n",
        "\n",
        "Test Loss: 1.7621, Test Accuracy: 34.72%\n",
        "\n",
        "Learning Rate: 0.000742\n",
        "\n",
        "Epoch: 19, Train Loss: 1.7632, Train Accuracy: 33.96%\n",
        "\n",
        "Epoch: 19, Test Loss: 1.7560, Test Accuracy: 34.70%\n",
        "\n",
        "End of Epoch 19:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7998\n",
        "\n",
        "Train Loss: 1.7632, Train Accuracy: 33.96%\n",
        "\n",
        "Test Loss: 1.7560, Test Accuracy: 34.70%\n",
        "\n",
        "Learning Rate: 0.000716\n",
        "\n",
        "Epoch: 20, Train Loss: 1.7543, Train Accuracy: 34.75%\n",
        "\n",
        "Epoch: 20, Test Loss: 1.7479, Test Accuracy: 35.07%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 20:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8479\n",
        "\n",
        "Train Loss: 1.7543, Train Accuracy: 34.75%\n",
        "\n",
        "Test Loss: 1.7479, Test Accuracy: 35.07%\n",
        "\n",
        "Learning Rate: 0.000689\n",
        "\n",
        "Epoch: 21, Train Loss: 1.7444, Train Accuracy: 34.54%\n",
        "\n",
        "Epoch: 21, Test Loss: 1.7407, Test Accuracy: 35.30%\n",
        "\n",
        "End of Epoch 21:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8949\n",
        "\n",
        "Train Loss: 1.7444, Train Accuracy: 34.54%\n",
        "\n",
        "Test Loss: 1.7407, Test Accuracy: 35.30%\n",
        "\n",
        "Learning Rate: 0.000662\n",
        "\n",
        "Epoch: 22, Train Loss: 1.7266, Train Accuracy: 35.16%\n",
        "\n",
        "Epoch: 22, Test Loss: 1.7373, Test Accuracy: 35.44%\n",
        "\n",
        "End of Epoch 22:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9410\n",
        "\n",
        "Train Loss: 1.7266, Train Accuracy: 35.16%\n",
        "\n",
        "Test Loss: 1.7373, Test Accuracy: 35.44%\n",
        "\n",
        "Learning Rate: 0.000634\n",
        "\n",
        "Epoch: 23, Train Loss: 1.7303, Train Accuracy: 35.08%\n",
        "\n",
        "Epoch: 23, Test Loss: 1.7299, Test Accuracy: 35.60%\n",
        "\n",
        "End of Epoch 23:\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9863\n",
        "\n",
        "Train Loss: 1.7303, Train Accuracy: 35.08%\n",
        "\n",
        "Test Loss: 1.7299, Test Accuracy: 35.60%\n",
        "\n",
        "Learning Rate: 0.000606\n",
        "\n",
        "Epoch: 24, Train Loss: 1.7229, Train Accuracy: 35.24%\n",
        "\n",
        "Epoch: 24, Test Loss: 1.7259, Test Accuracy: 35.84%\n",
        "\n",
        "End of Epoch 24:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0307\n",
        "\n",
        "Train Loss: 1.7229, Train Accuracy: 35.24%\n",
        "\n",
        "Test Loss: 1.7259, Test Accuracy: 35.84%\n",
        "\n",
        "Learning Rate: 0.000578\n",
        "\n",
        "Epoch: 25, Train Loss: 1.7187, Train Accuracy: 35.56%\n",
        "\n",
        "Epoch: 25, Test Loss: 1.7213, Test Accuracy: 36.13%\n",
        "\n",
        "End of Epoch 25:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0743\n",
        "\n",
        "Train Loss: 1.7187, Train Accuracy: 35.56%\n",
        "\n",
        "Test Loss: 1.7213, Test Accuracy: 36.13%\n",
        "\n",
        "Learning Rate: 0.000550\n",
        "\n",
        "Epoch: 26, Train Loss: 1.7124, Train Accuracy: 35.76%\n",
        "\n",
        "Epoch: 26, Test Loss: 1.7165, Test Accuracy: 36.17%\n",
        "\n",
        "End of Epoch 26:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1172\n",
        "\n",
        "Train Loss: 1.7124, Train Accuracy: 35.76%\n",
        "\n",
        "Test Loss: 1.7165, Test Accuracy: 36.17%\n",
        "\n",
        "Learning Rate: 0.000522\n",
        "\n",
        "Epoch: 27, Train Loss: 1.7072, Train Accuracy: 35.95%\n",
        "\n",
        "Epoch: 27, Test Loss: 1.7127, Test Accuracy: 36.23%\n",
        "\n",
        "End of Epoch 27:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1594\n",
        "\n",
        "Train Loss: 1.7072, Train Accuracy: 35.95%\n",
        "\n",
        "Test Loss: 1.7127, Test Accuracy: 36.23%\n",
        "\n",
        "Learning Rate: 0.000494\n",
        "\n",
        "Epoch: 28, Train Loss: 1.7044, Train Accuracy: 36.06%\n",
        "\n",
        "Epoch: 28, Test Loss: 1.7094, Test Accuracy: 36.38%\n",
        "\n",
        "End of Epoch 28:\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2010\n",
        "\n",
        "Train Loss: 1.7044, Train Accuracy: 36.06%\n",
        "\n",
        "Test Loss: 1.7094, Test Accuracy: 36.38%\n",
        "\n",
        "Learning Rate: 0.000466"
      ],
      "metadata": {
        "id": "wmGNo_7srl0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jszZXKNrwsAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import opacus\n",
        "print(opacus.__version__)\n"
      ],
      "metadata": {
        "id": "ri17cZDp9op0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LR=0.01, BATCH SIZE: 512"
      ],
      "metadata": {
        "id": "Pl7CoXIY_7Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the Cosine Annealing Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
        "\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\\n\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()  # Adjusts the learning rate based on the cosine schedule\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}: \\n')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f} \\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% \\n')\n",
        "    # Log current learning rate (optional)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Learning Rate: {current_lr:.6f} \\n')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xmJ-oKqP-cL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADD DYNAMIC NOISE SCALING TO LR=0.01, BATCH SIZE=**256**"
      ],
      "metadata": {
        "id": "eI8HASydfotX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "def update_noise_multiplier(loss, base_noise, max_noise, min_noise, scale_factor):\n",
        "    \"\"\"\n",
        "    Dynamically update noise multiplier based on training loss.\n",
        "\n",
        "    Args:\n",
        "    - loss (float): Current training loss.\n",
        "    - base_noise (float): Base noise multiplier.\n",
        "    - max_noise (float): Maximum noise multiplier.\n",
        "    - min_noise (float): Minimum noise multiplier.\n",
        "    - scale_factor (float): Sensitivity of noise adjustment to loss.\n",
        "\n",
        "    Returns:\n",
        "    - float: Updated noise multiplier.\n",
        "    \"\"\"\n",
        "    # Adjust noise dynamically (higher loss -> higher noise, lower loss -> lower noise)\n",
        "    noise_multiplier = base_noise + scale_factor * loss\n",
        "    return max(min_noise, min(noise_multiplier, max_noise))  # Clamp to [min_noise, max_noise]\n",
        "\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the Cosine Annealing Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
        "\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\\n\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Parameters for dynamic noise adjustment\n",
        "base_noise = 1.0  # Initial noise multiplier\n",
        "max_noise = 3.0   # Maximum noise allowed\n",
        "min_noise = 0.5   # Minimum noise allowed\n",
        "scale_factor = 0.1  # Scaling factor to adjust sensitivity\n",
        "\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "    # Dynamically adjust noise multiplier based on training loss\n",
        "    new_noise = update_noise_multiplier(\n",
        "        loss=train_loss,\n",
        "        base_noise=base_noise,\n",
        "        max_noise=max_noise,\n",
        "        min_noise=min_noise,\n",
        "        scale_factor=scale_factor\n",
        "    )\n",
        "    optimizer.noise_multiplier = new_noise  # Update the noise multiplier\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()  # Adjusts the learning rate based on the cosine schedule\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}: \\n')\n",
        "    print(f\"Epoch {epoch}, Updated Noise Multiplier: {new_noise:.4f}\\n\")\n",
        "\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f} \\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% \\n')\n",
        "    # Log current learning rate (optional)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Learning Rate: {current_lr:.6f} \\n')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JKw3Qkst-cF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Files already downloaded and verified\n",
        "Files already downloaded and verified\n",
        "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
        "  warnings.warn(\n",
        "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
        "  warnings.warn(\n",
        "WARNING:opacus.data_loader:Ignoring drop_last as it is not compatible with DPDataLoader.\n",
        "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
        "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
        "Using sigma=0.97412109375\n",
        "\n",
        "Epoch: 1, Train Loss: 2.1153, Train Accuracy: 19.92%\n",
        "\n",
        "Epoch: 1, Test Loss: 1.9779, Test Accuracy: 24.84%\n",
        "\n",
        "End of Epoch 1:\n",
        "\n",
        "Epoch 1, Updated Noise Multiplier: 1.2115\n",
        "\n",
        "Privacy Budget: Epsilon = 0.4951\n",
        "\n",
        "Train Loss: 2.1153, Train Accuracy: 19.92%\n",
        "\n",
        "Test Loss: 1.9779, Test Accuracy: 24.84%\n",
        "\n",
        "Learning Rate: 0.009990\n",
        "\n",
        "Epoch: 2, Train Loss: 1.9210, Train Accuracy: 27.82%\n",
        "\n",
        "Epoch: 2, Test Loss: 1.8645, Test Accuracy: 29.92%\n",
        "\n",
        "End of Epoch 2:\n",
        "\n",
        "Epoch 2, Updated Noise Multiplier: 1.1921\n",
        "\n",
        "Privacy Budget: Epsilon = 0.5486\n",
        "\n",
        "Train Loss: 1.9210, Train Accuracy: 27.82%\n",
        "\n",
        "Test Loss: 1.8645, Test Accuracy: 29.92%\n",
        "\n",
        "Learning Rate: 0.009961\n",
        "\n",
        "Epoch: 3, Train Loss: 1.8224, Train Accuracy: 31.34%\n",
        "\n",
        "Epoch: 3, Test Loss: 1.7874, Test Accuracy: 32.33%\n",
        "\n",
        "End of Epoch 3:\n",
        "\n",
        "Epoch 3, Updated Noise Multiplier: 1.1822\n",
        "\n",
        "Privacy Budget: Epsilon = 0.6057\n",
        "\n",
        "Train Loss: 1.8224, Train Accuracy: 31.34%\n",
        "\n",
        "Test Loss: 1.7874, Test Accuracy: 32.33%\n",
        "\n",
        "Learning Rate: 0.009912\n",
        "\n",
        "Epoch: 4, Train Loss: 1.7728, Train Accuracy: 32.52%\n",
        "\n",
        "Epoch: 4, Test Loss: 1.7578, Test Accuracy: 33.34%\n",
        "\n",
        "End of Epoch 4:\n",
        "\n",
        "Epoch 4, Updated Noise Multiplier: 1.1773\n",
        "\n",
        "Privacy Budget: Epsilon = 0.6629\n",
        "\n",
        "Train Loss: 1.7728, Train Accuracy: 32.52%\n",
        "\n",
        "Test Loss: 1.7578, Test Accuracy: 33.34%\n",
        "\n",
        "Learning Rate: 0.009844\n",
        "\n",
        "Epoch: 5, Train Loss: 1.7492, Train Accuracy: 32.94%\n",
        "\n",
        "Epoch: 5, Test Loss: 1.7329, Test Accuracy: 34.56%\n",
        "\n",
        "End of Epoch 5:\n",
        "\n",
        "Epoch 5, Updated Noise Multiplier: 1.1749\n",
        "\n",
        "Privacy Budget: Epsilon = 0.7186\n",
        "\n",
        "Train Loss: 1.7492, Train Accuracy: 32.94%\n",
        "\n",
        "Test Loss: 1.7329, Test Accuracy: 34.56%\n",
        "\n",
        "Learning Rate: 0.009758\n",
        "\n",
        "Epoch: 6, Train Loss: 1.7304, Train Accuracy: 34.58%\n",
        "\n",
        "Epoch: 6, Test Loss: 1.7299, Test Accuracy: 33.83%\n",
        "\n",
        "End of Epoch 6:\n",
        "\n",
        "Epoch 6, Updated Noise Multiplier: 1.1730\n",
        "\n",
        "Privacy Budget: Epsilon = 0.7722\n",
        "\n",
        "Train Loss: 1.7304, Train Accuracy: 34.58%\n",
        "\n",
        "Test Loss: 1.7299, Test Accuracy: 33.83%\n",
        "\n",
        "Learning Rate: 0.009652\n",
        "\n",
        "Epoch: 7, Train Loss: 1.7146, Train Accuracy: 34.55%\n",
        "\n",
        "Epoch: 7, Test Loss: 1.6956, Test Accuracy: 34.49%\n",
        "\n",
        "End of Epoch 7:\n",
        "\n",
        "Epoch 7, Updated Noise Multiplier: 1.1715\n",
        "\n",
        "Privacy Budget: Epsilon = 0.8236\n",
        "\n",
        "Train Loss: 1.7146, Train Accuracy: 34.55%\n",
        "\n",
        "Test Loss: 1.6956, Test Accuracy: 34.49%\n",
        "\n",
        "Learning Rate: 0.009529\n",
        "\n",
        "Epoch: 8, Train Loss: 1.6959, Train Accuracy: 35.15%\n",
        "\n",
        "Epoch: 8, Test Loss: 1.6836, Test Accuracy: 36.72%\n",
        "\n",
        "End of Epoch 8:\n",
        "\n",
        "Epoch 8, Updated Noise Multiplier: 1.1696\n",
        "\n",
        "Privacy Budget: Epsilon = 0.8729\n",
        "\n",
        "Train Loss: 1.6959, Train Accuracy: 35.15%\n",
        "\n",
        "Test Loss: 1.6836, Test Accuracy: 36.72%\n",
        "\n",
        "Learning Rate: 0.009388\n",
        "\n",
        "Epoch: 9, Train Loss: 1.6780, Train Accuracy: 36.34%\n",
        "\n",
        "Epoch: 9, Test Loss: 1.6821, Test Accuracy: 36.01%\n",
        "\n",
        "End of Epoch 9:\n",
        "\n",
        "Epoch 9, Updated Noise Multiplier: 1.1678\n",
        "\n",
        "Privacy Budget: Epsilon = 0.9205\n",
        "\n",
        "Train Loss: 1.6780, Train Accuracy: 36.34%\n",
        "\n",
        "Test Loss: 1.6821, Test Accuracy: 36.01%\n",
        "\n",
        "Learning Rate: 0.009229\n",
        "\n",
        "Epoch: 10, Train Loss: 1.6630, Train Accuracy: 36.73%\n",
        "\n",
        "Epoch: 10, Test Loss: 1.6590, Test Accuracy: 37.35%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 10:\n",
        "\n",
        "Epoch 10, Updated Noise Multiplier: 1.1663\n",
        "\n",
        "Privacy Budget: Epsilon = 0.9666\n",
        "\n",
        "Train Loss: 1.6630, Train Accuracy: 36.73%\n",
        "\n",
        "Test Loss: 1.6590, Test Accuracy: 37.35%\n",
        "\n",
        "Learning Rate: 0.009055\n",
        "\n",
        "Epoch: 11, Train Loss: 1.6456, Train Accuracy: 37.35%\n",
        "\n",
        "Epoch: 11, Test Loss: 1.6543, Test Accuracy: 37.73%\n",
        "\n",
        "End of Epoch 11:\n",
        "\n",
        "Epoch 11, Updated Noise Multiplier: 1.1646\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0111\n",
        "\n",
        "Train Loss: 1.6456, Train Accuracy: 37.35%\n",
        "\n",
        "Test Loss: 1.6543, Test Accuracy: 37.73%\n",
        "\n",
        "Learning Rate: 0.008864\n",
        "\n",
        "Epoch: 12, Train Loss: 1.6464, Train Accuracy: 38.35%\n",
        "\n",
        "Epoch: 12, Test Loss: 1.6420, Test Accuracy: 38.99%\n",
        "\n",
        "End of Epoch 12:\n",
        "\n",
        "Epoch 12, Updated Noise Multiplier: 1.1646\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0543\n",
        "\n",
        "Train Loss: 1.6464, Train Accuracy: 38.35%\n",
        "\n",
        "Test Loss: 1.6420, Test Accuracy: 38.99%\n",
        "\n",
        "Learning Rate: 0.008658\n",
        "\n",
        "Epoch: 13, Train Loss: 1.6373, Train Accuracy: 38.95%\n",
        "\n",
        "Epoch: 13, Test Loss: 1.6222, Test Accuracy: 38.63%\n",
        "\n",
        "End of Epoch 13:\n",
        "\n",
        "Epoch 13, Updated Noise Multiplier: 1.1637\n",
        "\n",
        "Privacy Budget: Epsilon = 1.0961\n",
        "\n",
        "Train Loss: 1.6373, Train Accuracy: 38.95%\n",
        "\n",
        "Test Loss: 1.6222, Test Accuracy: 38.63%\n",
        "\n",
        "Learning Rate: 0.008439\n",
        "\n",
        "Epoch: 14, Train Loss: 1.6161, Train Accuracy: 40.54%\n",
        "\n",
        "Epoch: 14, Test Loss: 1.6037, Test Accuracy: 41.04%\n",
        "\n",
        "End of Epoch 14:\n",
        "\n",
        "Epoch 14, Updated Noise Multiplier: 1.1616\n",
        "\n",
        "Privacy Budget: Epsilon = 1.1368\n",
        "\n",
        "Train Loss: 1.6161, Train Accuracy: 40.54%\n",
        "\n",
        "Test Loss: 1.6037, Test Accuracy: 41.04%\n",
        "\n",
        "Learning Rate: 0.008205\n",
        "\n",
        "Epoch: 15, Train Loss: 1.6033, Train Accuracy: 40.62%\n",
        "\n",
        "Epoch: 15, Test Loss: 1.6085, Test Accuracy: 40.67%\n",
        "\n",
        "End of Epoch 15:\n",
        "\n",
        "Epoch 15, Updated Noise Multiplier: 1.1603\n",
        "\n",
        "Privacy Budget: Epsilon = 1.1766\n",
        "\n",
        "Train Loss: 1.6033, Train Accuracy: 40.62%\n",
        "\n",
        "Test Loss: 1.6085, Test Accuracy: 40.67%\n",
        "\n",
        "Learning Rate: 0.007960\n",
        "\n",
        "Epoch: 16, Train Loss: 1.5768, Train Accuracy: 41.57%\n",
        "\n",
        "Epoch: 16, Test Loss: 1.5976, Test Accuracy: 40.87%\n",
        "\n",
        "End of Epoch 16:\n",
        "\n",
        "Epoch 16, Updated Noise Multiplier: 1.1577\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2155\n",
        "\n",
        "Train Loss: 1.5768, Train Accuracy: 41.57%\n",
        "\n",
        "Test Loss: 1.5976, Test Accuracy: 40.87%\n",
        "\n",
        "Learning Rate: 0.007702\n",
        "\n",
        "Epoch: 17, Train Loss: 1.5654, Train Accuracy: 42.40%\n",
        "\n",
        "Epoch: 17, Test Loss: 1.5654, Test Accuracy: 42.41%\n",
        "\n",
        "End of Epoch 17:\n",
        "\n",
        "Epoch 17, Updated Noise Multiplier: 1.1565\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2536\n",
        "\n",
        "Train Loss: 1.5654, Train Accuracy: 42.40%\n",
        "\n",
        "Test Loss: 1.5654, Test Accuracy: 42.41%\n",
        "\n",
        "Learning Rate: 0.007435\n",
        "\n",
        "Epoch: 18, Train Loss: 1.5519, Train Accuracy: 43.11%\n",
        "\n",
        "Epoch: 18, Test Loss: 1.5509, Test Accuracy: 43.21%\n",
        "\n",
        "End of Epoch 18:\n",
        "\n",
        "Epoch 18, Updated Noise Multiplier: 1.1552\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2909\n",
        "\n",
        "Train Loss: 1.5519, Train Accuracy: 43.11%\n",
        "\n",
        "Test Loss: 1.5509, Test Accuracy: 43.21%\n",
        "\n",
        "Learning Rate: 0.007158\n",
        "\n",
        "Epoch: 19, Train Loss: 1.5315, Train Accuracy: 43.62%\n",
        "\n",
        "Epoch: 19, Test Loss: 1.5392, Test Accuracy: 44.10%\n",
        "\n",
        "End of Epoch 19:\n",
        "\n",
        "Epoch 19, Updated Noise Multiplier: 1.1532\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3275\n",
        "\n",
        "Train Loss: 1.5315, Train Accuracy: 43.62%\n",
        "\n",
        "Test Loss: 1.5392, Test Accuracy: 44.10%\n",
        "\n",
        "Learning Rate: 0.006872\n",
        "\n",
        "Epoch: 20, Train Loss: 1.5292, Train Accuracy: 43.84%\n",
        "\n",
        "Epoch: 20, Test Loss: 1.5533, Test Accuracy: 42.88%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 20:\n",
        "\n",
        "Epoch 20, Updated Noise Multiplier: 1.1529\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3635\n",
        "\n",
        "Train Loss: 1.5292, Train Accuracy: 43.84%\n",
        "\n",
        "Test Loss: 1.5533, Test Accuracy: 42.88%\n",
        "\n",
        "Learning Rate: 0.006580\n",
        "\n",
        "Epoch: 21, Train Loss: 1.5175, Train Accuracy: 44.67%\n",
        "\n",
        "Epoch: 21, Test Loss: 1.5302, Test Accuracy: 44.13%\n",
        "\n",
        "End of Epoch 21:\n",
        "\n",
        "Epoch 21, Updated Noise Multiplier: 1.1518\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3988\n",
        "\n",
        "Train Loss: 1.5175, Train Accuracy: 44.67%\n",
        "\n",
        "Test Loss: 1.5302, Test Accuracy: 44.13%\n",
        "\n",
        "Learning Rate: 0.006281\n",
        "\n",
        "Epoch: 22, Train Loss: 1.5118, Train Accuracy: 44.73%\n",
        "\n",
        "Epoch: 22, Test Loss: 1.5117, Test Accuracy: 44.91%\n",
        "\n",
        "End of Epoch 22:\n",
        "\n",
        "Epoch 22, Updated Noise Multiplier: 1.1512\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4334\n",
        "\n",
        "Train Loss: 1.5118, Train Accuracy: 44.73%\n",
        "\n",
        "Test Loss: 1.5117, Test Accuracy: 44.91%\n",
        "\n",
        "Learning Rate: 0.005978\n",
        "\n",
        "Epoch: 23, Train Loss: 1.5013, Train Accuracy: 45.22%\n",
        "\n",
        "Epoch: 23, Test Loss: 1.5121, Test Accuracy: 45.11%\n",
        "\n",
        "End of Epoch 23:\n",
        "\n",
        "Epoch 23, Updated Noise Multiplier: 1.1501\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4675\n",
        "\n",
        "Train Loss: 1.5013, Train Accuracy: 45.22%\n",
        "\n",
        "Test Loss: 1.5121, Test Accuracy: 45.11%\n",
        "\n",
        "Learning Rate: 0.005670\n",
        "\n",
        "Epoch: 24, Train Loss: 1.4869, Train Accuracy: 45.66%\n",
        "\n",
        "Epoch: 24, Test Loss: 1.4892, Test Accuracy: 45.40%\n",
        "\n",
        "End of Epoch 24:\n",
        "\n",
        "Epoch 24, Updated Noise Multiplier: 1.1487\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5010\n",
        "\n",
        "Train Loss: 1.4869, Train Accuracy: 45.66%\n",
        "\n",
        "Test Loss: 1.4892, Test Accuracy: 45.40%\n",
        "\n",
        "Learning Rate: 0.005361\n",
        "\n",
        "Epoch: 25, Train Loss: 1.4820, Train Accuracy: 46.14%\n",
        "\n",
        "Epoch: 25, Test Loss: 1.5126, Test Accuracy: 45.46%\n",
        "\n",
        "End of Epoch 25:\n",
        "\n",
        "Epoch 25, Updated Noise Multiplier: 1.1482\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5340\n",
        "\n",
        "Train Loss: 1.4820, Train Accuracy: 46.14%\n",
        "\n",
        "Test Loss: 1.5126, Test Accuracy: 45.46%\n",
        "\n",
        "Learning Rate: 0.005050\n",
        "\n",
        "Epoch: 26, Train Loss: 1.4866, Train Accuracy: 45.88%\n",
        "\n",
        "Epoch: 26, Test Loss: 1.4838, Test Accuracy: 46.05%\n",
        "\n",
        "End of Epoch 26:\n",
        "\n",
        "Epoch 26, Updated Noise Multiplier: 1.1487\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5665\n",
        "\n",
        "Train Loss: 1.4866, Train Accuracy: 45.88%\n",
        "\n",
        "Test Loss: 1.4838, Test Accuracy: 46.05%\n",
        "\n",
        "Learning Rate: 0.004739\n",
        "\n",
        "Epoch: 27, Train Loss: 1.4692, Train Accuracy: 46.65%\n",
        "\n",
        "Epoch: 27, Test Loss: 1.4921, Test Accuracy: 45.66%\n",
        "\n",
        "End of Epoch 27:\n",
        "\n",
        "Epoch 27, Updated Noise Multiplier: 1.1469\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5985\n",
        "\n",
        "Train Loss: 1.4692, Train Accuracy: 46.65%\n",
        "\n",
        "Test Loss: 1.4921, Test Accuracy: 45.66%\n",
        "\n",
        "Learning Rate: 0.004430\n",
        "\n",
        "Epoch: 28, Train Loss: 1.4718, Train Accuracy: 46.81%\n",
        "\n",
        "Epoch: 28, Test Loss: 1.4787, Test Accuracy: 46.73%\n",
        "\n",
        "End of Epoch 28:\n",
        "\n",
        "Epoch 28, Updated Noise Multiplier: 1.1472\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6301\n",
        "\n",
        "Train Loss: 1.4718, Train Accuracy: 46.81%\n",
        "\n",
        "Test Loss: 1.4787, Test Accuracy: 46.73%\n",
        "\n",
        "Learning Rate: 0.004122\n",
        "\n",
        "Epoch: 29, Train Loss: 1.4737, Train Accuracy: 46.83%\n",
        "\n",
        "Epoch: 29, Test Loss: 1.4688, Test Accuracy: 46.74%\n",
        "\n",
        "End of Epoch 29:\n",
        "\n",
        "Epoch 29, Updated Noise Multiplier: 1.1474\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6611\n",
        "\n",
        "Train Loss: 1.4737, Train Accuracy: 46.83%\n",
        "\n",
        "Test Loss: 1.4688, Test Accuracy: 46.74%\n",
        "\n",
        "Learning Rate: 0.003819\n",
        "\n",
        "Epoch: 30, Train Loss: 1.4572, Train Accuracy: 47.49%\n",
        "\n",
        "Epoch: 30, Test Loss: 1.4697, Test Accuracy: 46.74%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 30:\n",
        "\n",
        "Epoch 30, Updated Noise Multiplier: 1.1457\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6917\n",
        "\n",
        "Train Loss: 1.4572, Train Accuracy: 47.49%\n",
        "\n",
        "Test Loss: 1.4697, Test Accuracy: 46.74%\n",
        "\n",
        "Learning Rate: 0.003520\n",
        "\n",
        "Epoch: 31, Train Loss: 1.4657, Train Accuracy: 46.99%\n",
        "\n",
        "Epoch: 31, Test Loss: 1.4685, Test Accuracy: 47.72%\n",
        "\n",
        "End of Epoch 31:\n",
        "\n",
        "Epoch 31, Updated Noise Multiplier: 1.1466\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7220\n",
        "\n",
        "Train Loss: 1.4657, Train Accuracy: 46.99%\n",
        "\n",
        "Test Loss: 1.4685, Test Accuracy: 47.72%\n",
        "\n",
        "Learning Rate: 0.003228\n",
        "\n",
        "Epoch: 32, Train Loss: 1.4448, Train Accuracy: 48.11%\n",
        "\n",
        "Epoch: 32, Test Loss: 1.4755, Test Accuracy: 46.95%\n",
        "\n",
        "End of Epoch 32:\n",
        "\n",
        "Epoch 32, Updated Noise Multiplier: 1.1445\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7518\n",
        "\n",
        "Train Loss: 1.4448, Train Accuracy: 48.11%\n",
        "\n",
        "Test Loss: 1.4755, Test Accuracy: 46.95%\n",
        "\n",
        "Learning Rate: 0.002942\n",
        "\n",
        "Epoch: 33, Train Loss: 1.4597, Train Accuracy: 47.29%\n",
        "\n",
        "Epoch: 33, Test Loss: 1.4572, Test Accuracy: 47.46%\n",
        "\n",
        "End of Epoch 33:\n",
        "\n",
        "Epoch 33, Updated Noise Multiplier: 1.1460\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7814\n",
        "\n",
        "Train Loss: 1.4597, Train Accuracy: 47.29%\n",
        "\n",
        "Test Loss: 1.4572, Test Accuracy: 47.46%\n",
        "\n",
        "Learning Rate: 0.002665\n",
        "\n",
        "Epoch: 34, Train Loss: 1.4486, Train Accuracy: 48.04%\n",
        "\n",
        "Epoch: 34, Test Loss: 1.4675, Test Accuracy: 47.40%\n",
        "\n",
        "End of Epoch 34:\n",
        "\n",
        "Epoch 34, Updated Noise Multiplier: 1.1449\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8104\n",
        "\n",
        "Train Loss: 1.4486, Train Accuracy: 48.04%\n",
        "\n",
        "Test Loss: 1.4675, Test Accuracy: 47.40%\n",
        "\n",
        "Learning Rate: 0.002398\n",
        "\n",
        "Epoch: 35, Train Loss: 1.4559, Train Accuracy: 47.85%\n",
        "\n",
        "Epoch: 35, Test Loss: 1.4682, Test Accuracy: 47.04%\n",
        "\n",
        "End of Epoch 35:\n",
        "\n",
        "Epoch 35, Updated Noise Multiplier: 1.1456\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8392\n",
        "\n",
        "Train Loss: 1.4559, Train Accuracy: 47.85%\n",
        "\n",
        "Test Loss: 1.4682, Test Accuracy: 47.04%\n",
        "\n",
        "Learning Rate: 0.002140\n",
        "\n",
        "Epoch: 36, Train Loss: 1.4577, Train Accuracy: 48.18%\n",
        "\n",
        "Epoch: 36, Test Loss: 1.4600, Test Accuracy: 47.69%\n",
        "\n",
        "End of Epoch 36:\n",
        "\n",
        "Epoch 36, Updated Noise Multiplier: 1.1458\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8676\n",
        "\n",
        "Train Loss: 1.4577, Train Accuracy: 48.18%\n",
        "\n",
        "Test Loss: 1.4600, Test Accuracy: 47.69%\n",
        "\n",
        "Learning Rate: 0.001895\n",
        "\n",
        "Epoch: 37, Train Loss: 1.4509, Train Accuracy: 48.21%\n",
        "\n",
        "Epoch: 37, Test Loss: 1.4713, Test Accuracy: 47.49%\n",
        "\n",
        "End of Epoch 37:\n",
        "\n",
        "Epoch 37, Updated Noise Multiplier: 1.1451\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8956\n",
        "\n",
        "Train Loss: 1.4509, Train Accuracy: 48.21%\n",
        "\n",
        "Test Loss: 1.4713, Test Accuracy: 47.49%\n",
        "\n",
        "Learning Rate: 0.001661\n",
        "\n",
        "Epoch: 38, Train Loss: 1.4476, Train Accuracy: 48.32%\n",
        "\n",
        "Epoch: 38, Test Loss: 1.4780, Test Accuracy: 47.85%\n",
        "\n",
        "End of Epoch 38:\n",
        "\n",
        "Epoch 38, Updated Noise Multiplier: 1.1448\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9234\n",
        "\n",
        "Train Loss: 1.4476, Train Accuracy: 48.32%\n",
        "\n",
        "Test Loss: 1.4780, Test Accuracy: 47.85%\n",
        "\n",
        "Learning Rate: 0.001442\n",
        "\n",
        "Epoch: 39, Train Loss: 1.4463, Train Accuracy: 48.45%\n",
        "\n",
        "Epoch: 39, Test Loss: 1.4668, Test Accuracy: 48.02%\n",
        "\n",
        "End of Epoch 39:\n",
        "\n",
        "Epoch 39, Updated Noise Multiplier: 1.1446\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9508\n",
        "\n",
        "Train Loss: 1.4463, Train Accuracy: 48.45%\n",
        "\n",
        "Test Loss: 1.4668, Test Accuracy: 48.02%\n",
        "\n",
        "Learning Rate: 0.001236\n",
        "\n",
        "Epoch: 40, Train Loss: 1.4444, Train Accuracy: 48.27%\n",
        "\n",
        "Epoch: 40, Test Loss: 1.4593, Test Accuracy: 47.87%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 40:\n",
        "\n",
        "Epoch 40, Updated Noise Multiplier: 1.1444\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9780\n",
        "\n",
        "Train Loss: 1.4444, Train Accuracy: 48.27%\n",
        "\n",
        "Test Loss: 1.4593, Test Accuracy: 47.87%\n",
        "\n",
        "Learning Rate: 0.001045\n",
        "\n",
        "Epoch: 41, Train Loss: 1.4457, Train Accuracy: 48.78%\n",
        "\n",
        "Epoch: 41, Test Loss: 1.4608, Test Accuracy: 48.10%\n",
        "\n",
        "End of Epoch 41:\n",
        "\n",
        "Epoch 41, Updated Noise Multiplier: 1.1446\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0049\n",
        "\n",
        "Train Loss: 1.4457, Train Accuracy: 48.78%\n",
        "\n",
        "Test Loss: 1.4608, Test Accuracy: 48.10%\n",
        "\n",
        "Learning Rate: 0.000871\n",
        "\n",
        "Epoch: 42, Train Loss: 1.4377, Train Accuracy: 48.60%\n",
        "\n",
        "Epoch: 42, Test Loss: 1.4671, Test Accuracy: 48.29%\n",
        "\n",
        "End of Epoch 42:\n",
        "\n",
        "Epoch 42, Updated Noise Multiplier: 1.1438\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0315\n",
        "\n",
        "Train Loss: 1.4377, Train Accuracy: 48.60%\n",
        "\n",
        "Test Loss: 1.4671, Test Accuracy: 48.29%\n",
        "\n",
        "Learning Rate: 0.000712\n",
        "\n",
        "Epoch: 43, Train Loss: 1.4347, Train Accuracy: 48.84%\n",
        "\n",
        "Epoch: 43, Test Loss: 1.4610, Test Accuracy: 48.37%\n",
        "\n",
        "End of Epoch 43:\n",
        "\n",
        "Epoch 43, Updated Noise Multiplier: 1.1435\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0579\n",
        "\n",
        "Train Loss: 1.4347, Train Accuracy: 48.84%\n",
        "\n",
        "Test Loss: 1.4610, Test Accuracy: 48.37%\n",
        "\n",
        "Learning Rate: 0.000571\n",
        "\n",
        "Epoch: 44, Train Loss: 1.4465, Train Accuracy: 48.41%\n",
        "\n",
        "Epoch: 44, Test Loss: 1.4665, Test Accuracy: 48.06%\n",
        "\n",
        "End of Epoch 44:\n",
        "\n",
        "Epoch 44, Updated Noise Multiplier: 1.1446\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0840\n",
        "\n",
        "Train Loss: 1.4465, Train Accuracy: 48.41%\n",
        "\n",
        "Test Loss: 1.4665, Test Accuracy: 48.06%\n",
        "\n",
        "Learning Rate: 0.000448\n",
        "\n",
        "Epoch: 45, Train Loss: 1.4445, Train Accuracy: 48.55%\n",
        "\n",
        "Epoch: 45, Test Loss: 1.4637, Test Accuracy: 48.16%\n",
        "\n",
        "End of Epoch 45:\n",
        "\n",
        "Epoch 45, Updated Noise Multiplier: 1.1445\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1098\n",
        "\n",
        "Train Loss: 1.4445, Train Accuracy: 48.55%\n",
        "\n",
        "Test Loss: 1.4637, Test Accuracy: 48.16%\n",
        "\n",
        "Learning Rate: 0.000342\n",
        "\n",
        "Epoch: 46, Train Loss: 1.4348, Train Accuracy: 49.09%\n",
        "\n",
        "Epoch: 46, Test Loss: 1.4678, Test Accuracy: 48.34%\n",
        "\n",
        "End of Epoch 46:\n",
        "\n",
        "Epoch 46, Updated Noise Multiplier: 1.1435\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1354\n",
        "\n",
        "Train Loss: 1.4348, Train Accuracy: 49.09%\n",
        "\n",
        "Test Loss: 1.4678, Test Accuracy: 48.34%\n",
        "\n",
        "Learning Rate: 0.000256\n",
        "\n",
        "Epoch: 47, Train Loss: 1.4422, Train Accuracy: 48.59%\n",
        "\n",
        "Epoch: 47, Test Loss: 1.4601, Test Accuracy: 48.39%\n",
        "\n",
        "End of Epoch 47:\n",
        "\n",
        "Epoch 47, Updated Noise Multiplier: 1.1442\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1608\n",
        "\n",
        "Train Loss: 1.4422, Train Accuracy: 48.59%\n",
        "\n",
        "Test Loss: 1.4601, Test Accuracy: 48.39%\n",
        "\n",
        "Learning Rate: 0.000188\n",
        "\n",
        "Epoch: 48, Train Loss: 1.4511, Train Accuracy: 48.51%\n",
        "\n",
        "Epoch: 48, Test Loss: 1.4590, Test Accuracy: 48.40%\n",
        "\n",
        "End of Epoch 48:\n",
        "\n",
        "Epoch 48, Updated Noise Multiplier: 1.1451\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1859\n",
        "\n",
        "Train Loss: 1.4511, Train Accuracy: 48.51%\n",
        "\n",
        "Test Loss: 1.4590, Test Accuracy: 48.40%\n",
        "\n",
        "Learning Rate: 0.000139\n",
        "\n",
        "Epoch: 49, Train Loss: 1.4484, Train Accuracy: 48.44%\n",
        "\n",
        "Epoch: 49, Test Loss: 1.4665, Test Accuracy: 48.50%\n",
        "\n",
        "End of Epoch 49:\n",
        "\n",
        "Epoch 49, Updated Noise Multiplier: 1.1448\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2108\n",
        "\n",
        "Train Loss: 1.4484, Train Accuracy: 48.44%\n",
        "\n",
        "Test Loss: 1.4665, Test Accuracy: 48.50%\n",
        "\n",
        "Learning Rate: 0.000110\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zg4ShC8cLM8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T-JUb4tmvzeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dynamic noise addition- reverse"
      ],
      "metadata": {
        "id": "uVPhOxZjwPF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "def update_noise_multiplier(loss, base_noise, max_noise, min_noise, scale_factor):\n",
        "    \"\"\"\n",
        "    Dynamically update noise multiplier based on training loss.\n",
        "\n",
        "    Args:\n",
        "    - loss (float): Current training loss.\n",
        "    - base_noise (float): Base noise multiplier.\n",
        "    - max_noise (float): Maximum noise multiplier.\n",
        "    - min_noise (float): Minimum noise multiplier.\n",
        "    - scale_factor (float): Sensitivity of noise adjustment to loss.\n",
        "\n",
        "    Returns:\n",
        "    - float: Updated noise multiplier.\n",
        "    \"\"\"\n",
        "    # Adjust noise dynamically (higher loss -> lower noise, lower loss -> higher noise)\n",
        "    noise_multiplier = base_noise - scale_factor * loss\n",
        "    return max(min_noise, min(noise_multiplier, max_noise))  # Clamp to [min_noise, max_noise]\n",
        "\n",
        "\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the Cosine Annealing Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
        "\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\\n\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Parameters for dynamic noise adjustment\n",
        "base_noise = 1.0  # Initial noise multiplier\n",
        "max_noise = 3.0   # Maximum noise allowed\n",
        "min_noise = 0.5   # Minimum noise allowed\n",
        "scale_factor = 0.1  # Scaling factor to adjust sensitivity\n",
        "\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "    # Dynamically adjust noise multiplier based on training loss\n",
        "    new_noise = update_noise_multiplier(\n",
        "        loss=train_loss,\n",
        "        base_noise=base_noise,\n",
        "        max_noise=max_noise,\n",
        "        min_noise=min_noise,\n",
        "        scale_factor=scale_factor\n",
        "    )\n",
        "    optimizer.noise_multiplier = new_noise  # Update the noise multiplier\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()  # Adjusts the learning rate based on the cosine schedule\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}: \\n')\n",
        "    print(f\"Epoch {epoch}, Updated Noise Multiplier: {new_noise:.4f}\\n\")\n",
        "\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f} \\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% \\n')\n",
        "    # Log current learning rate (optional)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Learning Rate: {current_lr:.6f} \\n')\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XalPz5_Iv0w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: 1, Train Loss: 2.1077, Train Accuracy: 19.90%\n",
        "\n",
        "Epoch: 1, Test Loss: 1.9668, Test Accuracy: 25.45%\n",
        "\n",
        "End of Epoch 1:\n",
        "\n",
        "Epoch 1, Updated Noise Multiplier: 0.7892\n",
        "\n",
        "Privacy Budget: Epsilon = 0.4951\n",
        "\n",
        "Train Loss: 2.1077, Train Accuracy: 19.90%\n",
        "\n",
        "Test Loss: 1.9668, Test Accuracy: 25.45%\n",
        "\n",
        "Learning Rate: 0.009990\n",
        "\n",
        "Epoch: 2, Train Loss: 1.9019, Train Accuracy: 28.59%\n",
        "\n",
        "Epoch: 2, Test Loss: 1.8318, Test Accuracy: 30.41%\n",
        "\n",
        "End of Epoch 2:\n",
        "\n",
        "Epoch 2, Updated Noise Multiplier: 0.8098\n",
        "\n",
        "Privacy Budget: Epsilon = 1.1101\n",
        "\n",
        "Train Loss: 1.9019, Train Accuracy: 28.59%\n",
        "\n",
        "Test Loss: 1.8318, Test Accuracy: 30.41%\n",
        "\n",
        "Learning Rate: 0.009961\n",
        "\n",
        "Epoch: 3, Train Loss: 1.7953, Train Accuracy: 32.49%\n",
        "\n",
        "Epoch: 3, Test Loss: 1.7563, Test Accuracy: 35.05%\n",
        "\n",
        "End of Epoch 3:\n",
        "\n",
        "Epoch 3, Updated Noise Multiplier: 0.8205\n",
        "\n",
        "Privacy Budget: Epsilon = 1.2696\n",
        "\n",
        "Train Loss: 1.7953, Train Accuracy: 32.49%\n",
        "\n",
        "Test Loss: 1.7563, Test Accuracy: 35.05%\n",
        "\n",
        "Learning Rate: 0.009912\n",
        "\n",
        "Epoch: 4, Train Loss: 1.7235, Train Accuracy: 34.78%\n",
        "\n",
        "Epoch: 4, Test Loss: 1.7207, Test Accuracy: 34.60%\n",
        "\n",
        "End of Epoch 4:\n",
        "\n",
        "Epoch 4, Updated Noise Multiplier: 0.8277\n",
        "\n",
        "Privacy Budget: Epsilon = 1.3881\n",
        "\n",
        "Train Loss: 1.7235, Train Accuracy: 34.78%\n",
        "\n",
        "Test Loss: 1.7207, Test Accuracy: 34.60%\n",
        "\n",
        "Learning Rate: 0.009844\n",
        "\n",
        "Epoch: 5, Train Loss: 1.6857, Train Accuracy: 36.08%\n",
        "\n",
        "Epoch: 5, Test Loss: 1.6810, Test Accuracy: 37.01%\n",
        "\n",
        "End of Epoch 5:\n",
        "\n",
        "Epoch 5, Updated Noise Multiplier: 0.8314\n",
        "\n",
        "Privacy Budget: Epsilon = 1.4904\n",
        "\n",
        "Train Loss: 1.6857, Train Accuracy: 36.08%\n",
        "\n",
        "Test Loss: 1.6810, Test Accuracy: 37.01%\n",
        "\n",
        "Learning Rate: 0.009758\n",
        "\n",
        "Epoch: 6, Train Loss: 1.6658, Train Accuracy: 37.18%\n",
        "\n",
        "Epoch: 6, Test Loss: 1.6549, Test Accuracy: 37.98%\n",
        "\n",
        "End of Epoch 6:\n",
        "\n",
        "Epoch 6, Updated Noise Multiplier: 0.8334\n",
        "\n",
        "Privacy Budget: Epsilon = 1.5846\n",
        "\n",
        "Train Loss: 1.6658, Train Accuracy: 37.18%\n",
        "\n",
        "Test Loss: 1.6549, Test Accuracy: 37.98%\n",
        "\n",
        "Learning Rate: 0.009652\n",
        "\n",
        "Epoch: 7, Train Loss: 1.6335, Train Accuracy: 38.65%\n",
        "\n",
        "Epoch: 7, Test Loss: 1.6479, Test Accuracy: 38.41%\n",
        "\n",
        "End of Epoch 7:\n",
        "\n",
        "Epoch 7, Updated Noise Multiplier: 0.8366\n",
        "\n",
        "Privacy Budget: Epsilon = 1.6739\n",
        "\n",
        "Train Loss: 1.6335, Train Accuracy: 38.65%\n",
        "\n",
        "Test Loss: 1.6479, Test Accuracy: 38.41%\n",
        "\n",
        "Learning Rate: 0.009529\n",
        "\n",
        "Epoch: 8, Train Loss: 1.6234, Train Accuracy: 39.25%\n",
        "\n",
        "Epoch: 8, Test Loss: 1.6175, Test Accuracy: 39.30%\n",
        "\n",
        "End of Epoch 8:\n",
        "\n",
        "Epoch 8, Updated Noise Multiplier: 0.8377\n",
        "\n",
        "Privacy Budget: Epsilon = 1.7580\n",
        "\n",
        "Train Loss: 1.6234, Train Accuracy: 39.25%\n",
        "\n",
        "Test Loss: 1.6175, Test Accuracy: 39.30%\n",
        "\n",
        "Learning Rate: 0.009388\n",
        "\n",
        "Epoch: 9, Train Loss: 1.6078, Train Accuracy: 40.00%\n",
        "\n",
        "Epoch: 9, Test Loss: 1.5927, Test Accuracy: 40.68%\n",
        "\n",
        "End of Epoch 9:\n",
        "\n",
        "Epoch 9, Updated Noise Multiplier: 0.8392\n",
        "\n",
        "Privacy Budget: Epsilon = 1.8389\n",
        "\n",
        "Train Loss: 1.6078, Train Accuracy: 40.00%\n",
        "\n",
        "Test Loss: 1.5927, Test Accuracy: 40.68%\n",
        "\n",
        "Learning Rate: 0.009229\n",
        "\n",
        "Epoch: 10, Train Loss: 1.6090, Train Accuracy: 40.40%\n",
        "\n",
        "Epoch: 10, Test Loss: 1.5983, Test Accuracy: 40.76%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 10:\n",
        "\n",
        "Epoch 10, Updated Noise Multiplier: 0.8391\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9168\n",
        "\n",
        "Train Loss: 1.6090, Train Accuracy: 40.40%\n",
        "\n",
        "Test Loss: 1.5983, Test Accuracy: 40.76%\n",
        "\n",
        "Learning Rate: 0.009055\n",
        "\n",
        "Epoch: 11, Train Loss: 1.5915, Train Accuracy: 41.56%\n",
        "\n",
        "Epoch: 11, Test Loss: 1.5742, Test Accuracy: 42.30%\n",
        "\n",
        "End of Epoch 11:\n",
        "\n",
        "Epoch 11, Updated Noise Multiplier: 0.8409\n",
        "\n",
        "Privacy Budget: Epsilon = 1.9926\n",
        "\n",
        "Train Loss: 1.5915, Train Accuracy: 41.56%\n",
        "\n",
        "Test Loss: 1.5742, Test Accuracy: 42.30%\n",
        "\n",
        "Learning Rate: 0.008864\n",
        "\n",
        "Epoch: 12, Train Loss: 1.5794, Train Accuracy: 42.14%\n",
        "\n",
        "Epoch: 12, Test Loss: 1.5754, Test Accuracy: 42.23%\n",
        "\n",
        "End of Epoch 12:\n",
        "\n",
        "Epoch 12, Updated Noise Multiplier: 0.8421\n",
        "\n",
        "Privacy Budget: Epsilon = 2.0656\n",
        "\n",
        "Train Loss: 1.5794, Train Accuracy: 42.14%\n",
        "\n",
        "Test Loss: 1.5754, Test Accuracy: 42.23%\n",
        "\n",
        "Learning Rate: 0.008658\n",
        "\n",
        "Epoch: 13, Train Loss: 1.5583, Train Accuracy: 43.15%\n",
        "\n",
        "Epoch: 13, Test Loss: 1.5606, Test Accuracy: 43.48%\n",
        "\n",
        "End of Epoch 13:\n",
        "\n",
        "Epoch 13, Updated Noise Multiplier: 0.8442\n",
        "\n",
        "Privacy Budget: Epsilon = 2.1363\n",
        "\n",
        "Train Loss: 1.5583, Train Accuracy: 43.15%\n",
        "\n",
        "Test Loss: 1.5606, Test Accuracy: 43.48%\n",
        "\n",
        "Learning Rate: 0.008439\n",
        "\n",
        "Epoch: 14, Train Loss: 1.5572, Train Accuracy: 43.13%\n",
        "\n",
        "Epoch: 14, Test Loss: 1.5447, Test Accuracy: 43.69%\n",
        "\n",
        "End of Epoch 14:\n",
        "\n",
        "Epoch 14, Updated Noise Multiplier: 0.8443\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2045\n",
        "\n",
        "Train Loss: 1.5572, Train Accuracy: 43.13%\n",
        "\n",
        "Test Loss: 1.5447, Test Accuracy: 43.69%\n",
        "\n",
        "Learning Rate: 0.008205\n",
        "\n",
        "Epoch: 15, Train Loss: 1.5608, Train Accuracy: 43.26%\n",
        "\n",
        "Epoch: 15, Test Loss: 1.5583, Test Accuracy: 44.35%\n",
        "\n",
        "End of Epoch 15:\n",
        "\n",
        "Epoch 15, Updated Noise Multiplier: 0.8439\n",
        "\n",
        "Privacy Budget: Epsilon = 2.2712\n",
        "\n",
        "Train Loss: 1.5608, Train Accuracy: 43.26%\n",
        "\n",
        "Test Loss: 1.5583, Test Accuracy: 44.35%\n",
        "\n",
        "Learning Rate: 0.007960\n",
        "\n",
        "Epoch: 16, Train Loss: 1.5282, Train Accuracy: 44.54%\n",
        "\n",
        "Epoch: 16, Test Loss: 1.5413, Test Accuracy: 44.41%\n",
        "\n",
        "End of Epoch 16:\n",
        "\n",
        "Epoch 16, Updated Noise Multiplier: 0.8472\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3367\n",
        "\n",
        "Train Loss: 1.5282, Train Accuracy: 44.54%\n",
        "\n",
        "Test Loss: 1.5413, Test Accuracy: 44.41%\n",
        "\n",
        "Learning Rate: 0.007702\n",
        "\n",
        "Epoch: 17, Train Loss: 1.5254, Train Accuracy: 44.70%\n",
        "\n",
        "Epoch: 17, Test Loss: 1.5262, Test Accuracy: 44.48%\n",
        "\n",
        "End of Epoch 17:\n",
        "\n",
        "Epoch 17, Updated Noise Multiplier: 0.8475\n",
        "\n",
        "Privacy Budget: Epsilon = 2.3997\n",
        "\n",
        "Train Loss: 1.5254, Train Accuracy: 44.70%\n",
        "\n",
        "Test Loss: 1.5262, Test Accuracy: 44.48%\n",
        "\n",
        "Learning Rate: 0.007435\n",
        "\n",
        "Epoch: 18, Train Loss: 1.5318, Train Accuracy: 44.74%\n",
        "\n",
        "Epoch: 18, Test Loss: 1.5281, Test Accuracy: 44.74%\n",
        "\n",
        "End of Epoch 18:\n",
        "\n",
        "Epoch 18, Updated Noise Multiplier: 0.8468\n",
        "\n",
        "Privacy Budget: Epsilon = 2.4614\n",
        "\n",
        "Train Loss: 1.5318, Train Accuracy: 44.74%\n",
        "\n",
        "Test Loss: 1.5281, Test Accuracy: 44.74%\n",
        "\n",
        "Learning Rate: 0.007158\n",
        "\n",
        "Epoch: 19, Train Loss: 1.5252, Train Accuracy: 45.29%\n",
        "\n",
        "Epoch: 19, Test Loss: 1.5331, Test Accuracy: 45.07%\n",
        "\n",
        "End of Epoch 19:\n",
        "\n",
        "Epoch 19, Updated Noise Multiplier: 0.8475\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5222\n",
        "\n",
        "Train Loss: 1.5252, Train Accuracy: 45.29%\n",
        "\n",
        "Test Loss: 1.5331, Test Accuracy: 45.07%\n",
        "\n",
        "Learning Rate: 0.006872\n",
        "\n",
        "Epoch: 20, Train Loss: 1.5287, Train Accuracy: 45.15%\n",
        "\n",
        "Epoch: 20, Test Loss: 1.5188, Test Accuracy: 45.74%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 20:\n",
        "\n",
        "Epoch 20, Updated Noise Multiplier: 0.8471\n",
        "\n",
        "Privacy Budget: Epsilon = 2.5818\n",
        "\n",
        "Train Loss: 1.5287, Train Accuracy: 45.15%\n",
        "\n",
        "Test Loss: 1.5188, Test Accuracy: 45.74%\n",
        "\n",
        "Learning Rate: 0.006580\n",
        "\n",
        "Epoch: 21, Train Loss: 1.5222, Train Accuracy: 45.58%\n",
        "\n",
        "Epoch: 21, Test Loss: 1.5271, Test Accuracy: 46.03%\n",
        "\n",
        "End of Epoch 21:\n",
        "\n",
        "Epoch 21, Updated Noise Multiplier: 0.8478\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6406\n",
        "\n",
        "Train Loss: 1.5222, Train Accuracy: 45.58%\n",
        "\n",
        "Test Loss: 1.5271, Test Accuracy: 46.03%\n",
        "\n",
        "Learning Rate: 0.006281\n",
        "\n",
        "Epoch: 22, Train Loss: 1.5253, Train Accuracy: 45.47%\n",
        "\n",
        "Epoch: 22, Test Loss: 1.5501, Test Accuracy: 45.29%\n",
        "\n",
        "End of Epoch 22:\n",
        "\n",
        "Epoch 22, Updated Noise Multiplier: 0.8475\n",
        "\n",
        "Privacy Budget: Epsilon = 2.6982\n",
        "\n",
        "Train Loss: 1.5253, Train Accuracy: 45.47%\n",
        "\n",
        "Test Loss: 1.5501, Test Accuracy: 45.29%\n",
        "\n",
        "Learning Rate: 0.005978\n",
        "\n",
        "Epoch: 23, Train Loss: 1.5162, Train Accuracy: 46.07%\n",
        "\n",
        "Epoch: 23, Test Loss: 1.5243, Test Accuracy: 46.57%\n",
        "\n",
        "End of Epoch 23:\n",
        "\n",
        "Epoch 23, Updated Noise Multiplier: 0.8484\n",
        "\n",
        "Privacy Budget: Epsilon = 2.7550\n",
        "\n",
        "Train Loss: 1.5162, Train Accuracy: 46.07%\n",
        "\n",
        "Test Loss: 1.5243, Test Accuracy: 46.57%\n",
        "\n",
        "Learning Rate: 0.005670\n",
        "\n",
        "Epoch: 24, Train Loss: 1.5146, Train Accuracy: 46.19%\n",
        "\n",
        "Epoch: 24, Test Loss: 1.5207, Test Accuracy: 46.11%\n",
        "\n",
        "End of Epoch 24:\n",
        "\n",
        "Epoch 24, Updated Noise Multiplier: 0.8485\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8107\n",
        "\n",
        "Train Loss: 1.5146, Train Accuracy: 46.19%\n",
        "\n",
        "Test Loss: 1.5207, Test Accuracy: 46.11%\n",
        "\n",
        "Learning Rate: 0.005361\n",
        "\n",
        "Epoch: 25, Train Loss: 1.5200, Train Accuracy: 46.42%\n",
        "\n",
        "Epoch: 25, Test Loss: 1.5269, Test Accuracy: 46.41%\n",
        "\n",
        "End of Epoch 25:\n",
        "\n",
        "Epoch 25, Updated Noise Multiplier: 0.8480\n",
        "\n",
        "Privacy Budget: Epsilon = 2.8655\n",
        "\n",
        "Train Loss: 1.5200, Train Accuracy: 46.42%\n",
        "\n",
        "Test Loss: 1.5269, Test Accuracy: 46.41%\n",
        "\n",
        "Learning Rate: 0.005050\n",
        "\n",
        "Epoch: 26, Train Loss: 1.5097, Train Accuracy: 46.53%\n",
        "\n",
        "Epoch: 26, Test Loss: 1.5277, Test Accuracy: 46.59%\n",
        "\n",
        "End of Epoch 26:\n",
        "\n",
        "Epoch 26, Updated Noise Multiplier: 0.8490\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9198\n",
        "\n",
        "Train Loss: 1.5097, Train Accuracy: 46.53%\n",
        "\n",
        "Test Loss: 1.5277, Test Accuracy: 46.59%\n",
        "\n",
        "Learning Rate: 0.004739\n",
        "\n",
        "Epoch: 27, Train Loss: 1.5131, Train Accuracy: 47.03%\n",
        "\n",
        "Epoch: 27, Test Loss: 1.5260, Test Accuracy: 46.90%\n",
        "\n",
        "End of Epoch 27:\n",
        "\n",
        "Epoch 27, Updated Noise Multiplier: 0.8487\n",
        "\n",
        "Privacy Budget: Epsilon = 2.9730\n",
        "\n",
        "Train Loss: 1.5131, Train Accuracy: 47.03%\n",
        "\n",
        "Test Loss: 1.5260, Test Accuracy: 46.90%\n",
        "\n",
        "Learning Rate: 0.004430\n",
        "\n",
        "Epoch: 28, Train Loss: 1.5195, Train Accuracy: 46.74%\n",
        "\n",
        "Epoch: 28, Test Loss: 1.5339, Test Accuracy: 47.19%\n",
        "\n",
        "End of Epoch 28:\n",
        "\n",
        "Epoch 28, Updated Noise Multiplier: 0.8481\n",
        "\n",
        "Privacy Budget: Epsilon = 3.0256\n",
        "\n",
        "Train Loss: 1.5195, Train Accuracy: 46.74%\n",
        "\n",
        "Test Loss: 1.5339, Test Accuracy: 47.19%\n",
        "\n",
        "Learning Rate: 0.004122\n",
        "\n",
        "Epoch: 29, Train Loss: 1.5165, Train Accuracy: 47.05%\n",
        "\n",
        "Epoch: 29, Test Loss: 1.5186, Test Accuracy: 47.21%\n",
        "\n",
        "End of Epoch 29:\n",
        "\n",
        "Epoch 29, Updated Noise Multiplier: 0.8484\n",
        "\n",
        "Privacy Budget: Epsilon = 3.0778\n",
        "\n",
        "Train Loss: 1.5165, Train Accuracy: 47.05%\n",
        "\n",
        "Test Loss: 1.5186, Test Accuracy: 47.21%\n",
        "\n",
        "Learning Rate: 0.003819\n",
        "\n",
        "Epoch: 30, Train Loss: 1.5181, Train Accuracy: 47.01%\n",
        "\n",
        "Epoch: 30, Test Loss: 1.5275, Test Accuracy: 47.40%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 30:\n",
        "\n",
        "Epoch 30, Updated Noise Multiplier: 0.8482\n",
        "\n",
        "Privacy Budget: Epsilon = 3.1292\n",
        "\n",
        "Train Loss: 1.5181, Train Accuracy: 47.01%\n",
        "\n",
        "Test Loss: 1.5275, Test Accuracy: 47.40%\n",
        "\n",
        "Learning Rate: 0.003520\n",
        "\n",
        "Epoch: 31, Train Loss: 1.5188, Train Accuracy: 47.12%\n",
        "\n",
        "Epoch: 31, Test Loss: 1.5267, Test Accuracy: 46.77%\n",
        "\n",
        "End of Epoch 31:\n",
        "\n",
        "Epoch 31, Updated Noise Multiplier: 0.8481\n",
        "\n",
        "Privacy Budget: Epsilon = 3.1801\n",
        "\n",
        "Train Loss: 1.5188, Train Accuracy: 47.12%\n",
        "\n",
        "Test Loss: 1.5267, Test Accuracy: 46.77%\n",
        "\n",
        "Learning Rate: 0.003228\n",
        "\n",
        "Epoch: 32, Train Loss: 1.5092, Train Accuracy: 47.23%\n",
        "\n",
        "Epoch: 32, Test Loss: 1.5251, Test Accuracy: 47.46%\n",
        "\n",
        "End of Epoch 32:\n",
        "\n",
        "Epoch 32, Updated Noise Multiplier: 0.8491\n",
        "\n",
        "Privacy Budget: Epsilon = 3.2303\n",
        "\n",
        "Train Loss: 1.5092, Train Accuracy: 47.23%\n",
        "\n",
        "Test Loss: 1.5251, Test Accuracy: 47.46%\n",
        "\n",
        "Learning Rate: 0.002942\n",
        "\n",
        "Epoch: 33, Train Loss: 1.5122, Train Accuracy: 47.45%\n",
        "\n",
        "Epoch: 33, Test Loss: 1.5198, Test Accuracy: 47.74%\n",
        "\n",
        "End of Epoch 33:\n",
        "\n",
        "Epoch 33, Updated Noise Multiplier: 0.8488\n",
        "\n",
        "Privacy Budget: Epsilon = 3.2798\n",
        "\n",
        "Train Loss: 1.5122, Train Accuracy: 47.45%\n",
        "\n",
        "Test Loss: 1.5198, Test Accuracy: 47.74%\n",
        "\n",
        "Learning Rate: 0.002665\n",
        "\n",
        "Epoch: 34, Train Loss: 1.5093, Train Accuracy: 47.85%\n",
        "\n",
        "Epoch: 34, Test Loss: 1.5255, Test Accuracy: 47.77%\n",
        "\n",
        "End of Epoch 34:\n",
        "\n",
        "Epoch 34, Updated Noise Multiplier: 0.8491\n",
        "\n",
        "Privacy Budget: Epsilon = 3.3288\n",
        "\n",
        "Train Loss: 1.5093, Train Accuracy: 47.85%\n",
        "\n",
        "Test Loss: 1.5255, Test Accuracy: 47.77%\n",
        "\n",
        "Learning Rate: 0.002398\n",
        "\n",
        "Epoch: 35, Train Loss: 1.4893, Train Accuracy: 48.25%\n",
        "\n",
        "Epoch: 35, Test Loss: 1.5090, Test Accuracy: 48.29%\n",
        "\n",
        "End of Epoch 35:\n",
        "\n",
        "Epoch 35, Updated Noise Multiplier: 0.8511\n",
        "\n",
        "Privacy Budget: Epsilon = 3.3772\n",
        "\n",
        "Train Loss: 1.4893, Train Accuracy: 48.25%\n",
        "\n",
        "Test Loss: 1.5090, Test Accuracy: 48.29%\n",
        "\n",
        "Learning Rate: 0.002140\n",
        "\n",
        "Epoch: 36, Train Loss: 1.5011, Train Accuracy: 48.21%\n",
        "\n",
        "Epoch: 36, Test Loss: 1.5235, Test Accuracy: 48.06%\n",
        "\n",
        "End of Epoch 36:\n",
        "\n",
        "Epoch 36, Updated Noise Multiplier: 0.8499\n",
        "\n",
        "Privacy Budget: Epsilon = 3.4247\n",
        "\n",
        "Train Loss: 1.5011, Train Accuracy: 48.21%\n",
        "\n",
        "Test Loss: 1.5235, Test Accuracy: 48.06%\n",
        "\n",
        "Learning Rate: 0.001895\n",
        "\n",
        "Epoch: 37, Train Loss: 1.4957, Train Accuracy: 48.28%\n",
        "\n",
        "Epoch: 37, Test Loss: 1.5530, Test Accuracy: 47.44%\n",
        "\n",
        "End of Epoch 37:\n",
        "\n",
        "Epoch 37, Updated Noise Multiplier: 0.8504\n",
        "\n",
        "Privacy Budget: Epsilon = 3.4719\n",
        "\n",
        "Train Loss: 1.4957, Train Accuracy: 48.28%\n",
        "\n",
        "Test Loss: 1.5530, Test Accuracy: 47.44%\n",
        "\n",
        "Learning Rate: 0.001661\n",
        "\n",
        "Epoch: 38, Train Loss: 1.5039, Train Accuracy: 48.04%\n",
        "\n",
        "Epoch: 38, Test Loss: 1.5137, Test Accuracy: 48.23%\n",
        "\n",
        "End of Epoch 38:\n",
        "\n",
        "Epoch 38, Updated Noise Multiplier: 0.8496\n",
        "\n",
        "Privacy Budget: Epsilon = 3.5186\n",
        "\n",
        "Train Loss: 1.5039, Train Accuracy: 48.04%\n",
        "\n",
        "Test Loss: 1.5137, Test Accuracy: 48.23%\n",
        "\n",
        "Learning Rate: 0.001442\n",
        "\n",
        "Epoch: 39, Train Loss: 1.5128, Train Accuracy: 48.14%\n",
        "\n",
        "Epoch: 39, Test Loss: 1.5287, Test Accuracy: 47.90%\n",
        "\n",
        "End of Epoch 39:\n",
        "\n",
        "Epoch 39, Updated Noise Multiplier: 0.8487\n",
        "\n",
        "Privacy Budget: Epsilon = 3.5650\n",
        "\n",
        "Train Loss: 1.5128, Train Accuracy: 48.14%\n",
        "\n",
        "Test Loss: 1.5287, Test Accuracy: 47.90%\n",
        "\n",
        "Learning Rate: 0.001236\n",
        "\n",
        "Epoch: 40, Train Loss: 1.5013, Train Accuracy: 48.38%\n",
        "\n",
        "Epoch: 40, Test Loss: 1.5294, Test Accuracy: 48.60%\n",
        "\n",
        "Averaging model parameters...\n",
        "End of Epoch 40:\n",
        "\n",
        "Epoch 40, Updated Noise Multiplier: 0.8499\n",
        "\n",
        "Privacy Budget: Epsilon = 3.6111\n",
        "\n",
        "Train Loss: 1.5013, Train Accuracy: 48.38%\n",
        "\n",
        "Test Loss: 1.5294, Test Accuracy: 48.60%\n",
        "\n",
        "Learning Rate: 0.001045\n",
        "\n",
        "Epoch: 41, Train Loss: 1.5076, Train Accuracy: 48.18%\n",
        "\n",
        "Epoch: 41, Test Loss: 1.5162, Test Accuracy: 48.30%\n",
        "\n",
        "End of Epoch 41:\n",
        "\n",
        "Epoch 41, Updated Noise Multiplier: 0.8492\n",
        "\n",
        "Privacy Budget: Epsilon = 3.6566\n",
        "\n",
        "Train Loss: 1.5076, Train Accuracy: 48.18%\n",
        "\n",
        "Test Loss: 1.5162, Test Accuracy: 48.30%\n",
        "\n",
        "Learning Rate: 0.000871\n",
        "\n",
        "Epoch: 42, Train Loss: 1.4982, Train Accuracy: 48.66%\n",
        "\n",
        "Epoch: 42, Test Loss: 1.5317, Test Accuracy: 48.38%\n",
        "\n",
        "End of Epoch 42:\n",
        "\n",
        "Epoch 42, Updated Noise Multiplier: 0.8502\n",
        "\n",
        "Privacy Budget: Epsilon = 3.7019\n",
        "\n",
        "Train Loss: 1.4982, Train Accuracy: 48.66%\n",
        "\n",
        "Test Loss: 1.5317, Test Accuracy: 48.38%\n",
        "\n",
        "Learning Rate: 0.000712\n",
        "\n",
        "Epoch: 43, Train Loss: 1.4863, Train Accuracy: 49.17%\n",
        "\n",
        "Epoch: 43, Test Loss: 1.5234, Test Accuracy: 48.38%\n",
        "\n",
        "End of Epoch 43:\n",
        "\n",
        "Epoch 43, Updated Noise Multiplier: 0.8514\n",
        "\n",
        "Privacy Budget: Epsilon = 3.7465\n",
        "\n",
        "Train Loss: 1.4863, Train Accuracy: 49.17%\n",
        "\n",
        "Test Loss: 1.5234, Test Accuracy: 48.38%\n",
        "\n",
        "Learning Rate: 0.000571\n",
        "\n",
        "Epoch: 44, Train Loss: 1.5078, Train Accuracy: 48.35%\n",
        "\n",
        "Epoch: 44, Test Loss: 1.5277, Test Accuracy: 47.69%\n",
        "\n",
        "End of Epoch 44:\n",
        "\n",
        "Epoch 44, Updated Noise Multiplier: 0.8492\n",
        "\n",
        "Privacy Budget: Epsilon = 3.7904\n",
        "\n",
        "Train Loss: 1.5078, Train Accuracy: 48.35%\n",
        "\n",
        "Test Loss: 1.5277, Test Accuracy: 47.69%\n",
        "\n",
        "Learning Rate: 0.000448\n",
        "\n",
        "Epoch: 45, Train Loss: 1.5009, Train Accuracy: 48.56%\n",
        "\n",
        "Epoch: 45, Test Loss: 1.5292, Test Accuracy: 48.25%\n",
        "\n",
        "End of Epoch 45:\n",
        "\n",
        "Epoch 45, Updated Noise Multiplier: 0.8499\n",
        "\n",
        "Privacy Budget: Epsilon = 3.8345\n",
        "\n",
        "Train Loss: 1.5009, Train Accuracy: 48.56%\n",
        "\n",
        "Test Loss: 1.5292, Test Accuracy: 48.25%\n",
        "\n",
        "Learning Rate: 0.000342\n",
        "\n",
        "Epoch: 46, Train Loss: 1.4961, Train Accuracy: 48.21%\n",
        "\n",
        "Epoch: 46, Test Loss: 1.5221, Test Accuracy: 48.65%\n",
        "\n",
        "End of Epoch 46:\n",
        "\n",
        "Epoch 46, Updated Noise Multiplier: 0.8504\n",
        "\n",
        "Privacy Budget: Epsilon = 3.8781\n",
        "\n",
        "Train Loss: 1.4961, Train Accuracy: 48.21%\n",
        "\n",
        "Test Loss: 1.5221, Test Accuracy: 48.65%\n",
        "\n",
        "Learning Rate: 0.000256\n",
        "\n",
        "Epoch: 47, Train Loss: 1.4960, Train Accuracy: 48.87%\n",
        "\n",
        "Epoch: 47, Test Loss: 1.5271, Test Accuracy: 48.60%\n",
        "\n",
        "End of Epoch 47:\n",
        "\n",
        "Epoch 47, Updated Noise Multiplier: 0.8504\n",
        "\n",
        "Privacy Budget: Epsilon = 3.9212\n",
        "\n",
        "Train Loss: 1.4960, Train Accuracy: 48.87%\n",
        "\n",
        "Test Loss: 1.5271, Test Accuracy: 48.60%\n",
        "\n",
        "Learning Rate: 0.000188\n",
        "\n",
        "Epoch: 48, Train Loss: 1.5002, Train Accuracy: 48.70%\n",
        "\n",
        "Epoch: 48, Test Loss: 1.5160, Test Accuracy: 48.26%\n",
        "\n",
        "End of Epoch 48:\n",
        "\n",
        "Epoch 48, Updated Noise Multiplier: 0.8500\n",
        "\n",
        "Privacy Budget: Epsilon = 3.9640\n",
        "\n",
        "Train Loss: 1.5002, Train Accuracy: 48.70%\n",
        "\n",
        "Test Loss: 1.5160, Test Accuracy: 48.26%\n",
        "\n",
        "Learning Rate: 0.000139\n",
        "\n",
        "Epoch: 49, Train Loss: 1.5064, Train Accuracy: 48.49%\n",
        "\n",
        "Epoch: 49, Test Loss: 1.5182, Test Accuracy: 48.29%\n",
        "\n",
        "End of Epoch 49:\n",
        "\n",
        "Epoch 49, Updated Noise Multiplier: 0.8494\n",
        "\n",
        "Privacy Budget: Epsilon = 4.0066\n",
        "\n",
        "Train Loss: 1.5064, Train Accuracy: 48.49%\n",
        "\n",
        "Test Loss: 1.5182, Test Accuracy: 48.29%\n",
        "\n",
        "Learning Rate: 0.000110\n",
        "\n",
        "Epochs:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
        "Train Accuracy:  [19.896936156560795, 28.591400211699387, 32.48509582683151, 34.77720040928516, 36.07834498227625, 37.182897296438924, 38.65348586917527, 39.25278795442651, 39.99599158232288, 40.404729878942405, 41.56471862340059, 42.14395282586609, 43.14565971529965, 43.13473840415505, 43.26076050968195, 44.539369208193456, 44.69583308327832, 44.737792400593825, 45.293293293293296, 45.15046807826751, 45.584687468846575, 45.4732428115016, 46.07355864811133, 46.18564147838998, 46.41645760160562, 46.53278819945486, 47.02715635841608, 46.740728963790296, 47.05034819498919, 47.010875007510364, 47.12166766887152, 47.23023371456986, 47.44806411738905, 47.852772961878, 48.24809750416642, 48.20516911295469, 48.276274123200096, 48.041681638519584, 48.14158939870269, 48.37776483114691, 48.17719796710553, 48.65687282107088, 49.16706330318896, 48.34563578122202, 48.55914408559144, 48.20814133649918, 48.86671618718757, 48.69601879178462, 48.489407799711124]\n",
        "Test Accuracy:  [25.450721153846153, 30.408653846153847, 35.046073717948715, 34.59535256410256, 37.009214743589745, 37.98076923076923, 38.411458333333336, 39.30288461538461, 40.67508012820513, 40.755208333333336, 42.297676282051285, 42.2275641025641, 43.47956730769231, 43.68990384615385, 44.35096153846154, 44.41105769230769, 44.48116987179487, 44.74158653846154, 45.07211538461539, 45.7431891025641, 46.03365384615385, 45.29246794871795, 46.57451923076923, 46.11378205128205, 46.41426282051282, 46.594551282051285, 46.89503205128205, 47.185496794871796, 47.20552884615385, 47.395833333333336, 46.774839743589745, 47.45592948717949, 47.736378205128204, 47.766426282051285, 48.28725961538461, 48.05689102564103, 47.43589743589744, 48.22716346153846, 47.89663461538461, 48.59775641025641, 48.29727564102564, 48.37740384615385, 48.37740384615385, 47.68629807692308, 48.24719551282051, 48.64783653846154, 48.59775641025641, 48.25721153846154, 48.28725961538461]\n",
        "Train Loss:  [2.1076787936381804, 1.9018786314206246, 1.7952720397557969, 1.723455936480791, 1.6856605921036159, 1.6658392924528855, 1.6335161288579305, 1.6234411355776666, 1.6077628386326326, 1.6090013180023586, 1.5914724588394165, 1.579412885201283, 1.5583205149723933, 1.5572168729244134, 1.5608264385125576, 1.5281732595883883, 1.5254454930623373, 1.531776931346991, 1.5252344840612166, 1.5286522816389034, 1.522210448827499, 1.525304250839429, 1.516150983174642, 1.5146153388879238, 1.5200155514937181, 1.5097432118195755, 1.5130534269870857, 1.5194646040598552, 1.5164846285795555, 1.5181064049402873, 1.5188043734966181, 1.5091766711993095, 1.5122448921203613, 1.5092860020123995, 1.4893433271310268, 1.5010878355075152, 1.4956645115827902, 1.503944904376299, 1.5127549727757772, 1.5013328509453014, 1.5076176930696537, 1.4981803172673935, 1.486271905899048, 1.5077756105325162, 1.500902644181863, 1.4961287761345887, 1.4959706043585752, 1.5002067804336547, 1.506384230271364]\n",
        "Test Loss:  [1.9668038349885206, 1.831763628201607, 1.756287776506864, 1.720740461960817, 1.6810028614142003, 1.654901046019334, 1.6478800681921153, 1.6175165634888868, 1.5926921306512294, 1.5983238006249452, 1.574175143853212, 1.5753605854816926, 1.560589998196333, 1.5447040643447485, 1.5582656157322419, 1.5413012963074904, 1.5262092565878844, 1.5280716388653486, 1.5330724869018946, 1.5187860360512366, 1.5270907909442217, 1.5500561518546863, 1.5242790472813141, 1.5206821942940736, 1.5268951257069905, 1.527703731487959, 1.5260052405870879, 1.533893624941508, 1.5186187028884888, 1.5274776373154078, 1.5266909354772322, 1.5250893892386022, 1.5197787223718104, 1.5255334346722333, 1.5089592322325096, 1.5235129717068794, 1.5529693028865716, 1.5137199194003375, 1.5287455014693432, 1.5293903411963048, 1.5161583576446924, 1.5316905700243437, 1.5234004045144105, 1.5276745343819642, 1.529157042503357, 1.5221433211595585, 1.5271409108088567, 1.5160065125196407, 1.5181760176634178]\n",
        "Epsilon:  [0.49513272854041585, 1.110054830277703, 1.2695761287087877, 1.388142446736472, 1.4903723958712451, 1.5846235200841585, 1.673868117965625, 1.7579608949355627, 1.8389402672266044, 1.9167562331690153, 1.9925719139717228, 2.0655817846168345, 2.1362881232677715, 2.204483120277118, 2.271193173139369, 2.336689157556696, 2.3996571632268364, 2.461380207490421, 2.5222424924059297, 2.5818407319603827, 2.640575792864219, 2.6981671708189707, 2.7549765983446335, 2.810660434483967, 2.8655057160370223, 2.91975554699084, 2.972980001254774, 3.02561199739297, 3.077762203564096, 3.1291904242183217, 3.1800509543230837, 3.230341141515022, 3.279812902581502, 3.32882533394463, 3.377242186495088, 3.424666521532715, 3.4719065563526477, 3.5185551665283694, 3.564958138989702, 3.6111366447637376, 3.656624489691365, 3.701852723845352, 3.7464713008543606, 3.7904462257915417, 3.834535567089714, 3.8781123300090727, 3.9212378778431236, 3.9640243202815486, 4.006574454879168]"
      ],
      "metadata": {
        "id": "JNA0aRXGQHmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Epochs=  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
        "Train_Accuracy=  [19.896936156560795, 28.591400211699387, 32.48509582683151, 34.77720040928516, 36.07834498227625, 37.182897296438924, 38.65348586917527, 39.25278795442651, 39.99599158232288, 40.404729878942405, 41.56471862340059, 42.14395282586609, 43.14565971529965, 43.13473840415505, 43.26076050968195, 44.539369208193456, 44.69583308327832, 44.737792400593825, 45.293293293293296, 45.15046807826751, 45.584687468846575, 45.4732428115016, 46.07355864811133, 46.18564147838998, 46.41645760160562, 46.53278819945486, 47.02715635841608, 46.740728963790296, 47.05034819498919, 47.010875007510364, 47.12166766887152, 47.23023371456986, 47.44806411738905, 47.852772961878, 48.24809750416642, 48.20516911295469, 48.276274123200096, 48.041681638519584, 48.14158939870269, 48.37776483114691, 48.17719796710553, 48.65687282107088, 49.16706330318896, 48.34563578122202, 48.55914408559144, 48.20814133649918, 48.86671618718757, 48.69601879178462, 48.489407799711124]\n",
        "Test_Accuracy= [25.450721153846153, 30.408653846153847, 35.046073717948715, 34.59535256410256, 37.009214743589745, 37.98076923076923, 38.411458333333336, 39.30288461538461, 40.67508012820513, 40.755208333333336, 42.297676282051285, 42.2275641025641, 43.47956730769231, 43.68990384615385, 44.35096153846154, 44.41105769230769, 44.48116987179487, 44.74158653846154, 45.07211538461539, 45.7431891025641, 46.03365384615385, 45.29246794871795, 46.57451923076923, 46.11378205128205, 46.41426282051282, 46.594551282051285, 46.89503205128205, 47.185496794871796, 47.20552884615385, 47.395833333333336, 46.774839743589745, 47.45592948717949, 47.736378205128204, 47.766426282051285, 48.28725961538461, 48.05689102564103, 47.43589743589744, 48.22716346153846, 47.89663461538461, 48.59775641025641, 48.29727564102564, 48.37740384615385, 48.37740384615385, 47.68629807692308, 48.24719551282051, 48.64783653846154, 48.59775641025641, 48.25721153846154, 48.28725961538461]\n",
        "Train_Loss= [2.1076787936381804, 1.9018786314206246, 1.7952720397557969, 1.723455936480791, 1.6856605921036159, 1.6658392924528855, 1.6335161288579305, 1.6234411355776666, 1.6077628386326326, 1.6090013180023586, 1.5914724588394165, 1.579412885201283, 1.5583205149723933, 1.5572168729244134, 1.5608264385125576, 1.5281732595883883, 1.5254454930623373, 1.531776931346991, 1.5252344840612166, 1.5286522816389034, 1.522210448827499, 1.525304250839429, 1.516150983174642, 1.5146153388879238, 1.5200155514937181, 1.5097432118195755, 1.5130534269870857, 1.5194646040598552, 1.5164846285795555, 1.5181064049402873, 1.5188043734966181, 1.5091766711993095, 1.5122448921203613, 1.5092860020123995, 1.4893433271310268, 1.5010878355075152, 1.4956645115827902, 1.503944904376299, 1.5127549727757772, 1.5013328509453014, 1.5076176930696537, 1.4981803172673935, 1.486271905899048, 1.5077756105325162, 1.500902644181863, 1.4961287761345887, 1.4959706043585752, 1.5002067804336547, 1.506384230271364]\n",
        "Test_Loss= [1.9668038349885206, 1.831763628201607, 1.756287776506864, 1.720740461960817, 1.6810028614142003, 1.654901046019334, 1.6478800681921153, 1.6175165634888868, 1.5926921306512294, 1.5983238006249452, 1.574175143853212, 1.5753605854816926, 1.560589998196333, 1.5447040643447485, 1.5582656157322419, 1.5413012963074904, 1.5262092565878844, 1.5280716388653486, 1.5330724869018946, 1.5187860360512366, 1.5270907909442217, 1.5500561518546863, 1.5242790472813141, 1.5206821942940736, 1.5268951257069905, 1.527703731487959, 1.5260052405870879, 1.533893624941508, 1.5186187028884888, 1.5274776373154078, 1.5266909354772322, 1.5250893892386022, 1.5197787223718104, 1.5255334346722333, 1.5089592322325096, 1.5235129717068794, 1.5529693028865716, 1.5137199194003375, 1.5287455014693432, 1.5293903411963048, 1.5161583576446924, 1.5316905700243437, 1.5234004045144105, 1.5276745343819642, 1.529157042503357, 1.5221433211595585, 1.5271409108088567, 1.5160065125196407, 1.5181760176634178]\n",
        "Epsilon= [0.49513272854041585, 1.110054830277703, 1.2695761287087877, 1.388142446736472, 1.4903723958712451, 1.5846235200841585, 1.673868117965625, 1.7579608949355627, 1.8389402672266044, 1.9167562331690153, 1.9925719139717228, 2.0655817846168345, 2.1362881232677715, 2.204483120277118, 2.271193173139369, 2.336689157556696, 2.3996571632268364, 2.461380207490421, 2.5222424924059297, 2.5818407319603827, 2.640575792864219, 2.6981671708189707, 2.7549765983446335, 2.810660434483967, 2.8655057160370223, 2.91975554699084, 2.972980001254774, 3.02561199739297, 3.077762203564096, 3.1291904242183217, 3.1800509543230837, 3.230341141515022, 3.279812902581502, 3.32882533394463, 3.377242186495088, 3.424666521532715, 3.4719065563526477, 3.5185551665283694, 3.564958138989702, 3.6111366447637376, 3.656624489691365, 3.701852723845352, 3.7464713008543606, 3.7904462257915417, 3.834535567089714, 3.8781123300090727, 3.9212378778431236, 3.9640243202815486, 4.006574454879168]\n",
        "\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(Epochs, Train_Accuracy, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(Epochs, Test_Accuracy, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(Epsilon, Test_Accuracy, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(Epochs, Train_loss, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, Test_loss, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O27ZyuzX97qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SXMIzwbmQobP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ttYrL_UC97ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LR=0.01, BATCH SIZE=256 ----Dynamic noise addition as per entropy"
      ],
      "metadata": {
        "id": "5q7SSjl5AdKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import opacus\n",
        "from opacus import PrivacyEngine\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_accuracy_vals=[]\n",
        "test_accuracy_vals=[]\n",
        "train_loss_vals=[]\n",
        "test_loss_vals=[]\n",
        "epochs=[]\n",
        "epsilon_vals=[]\n",
        "\n",
        "def compute_entropy(outputs):\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    # Compute entropy\n",
        "    entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-10), dim=1)  # Avoid log(0)\n",
        "    return entropy.mean().item()  # Return the average entropy over the batch\n",
        "\n",
        "def update_noise_multiplier_based_on_entropy(entropy, base_noise, max_noise, min_noise, scale_factor):\n",
        "    \"\"\"\n",
        "    Dynamically update noise multiplier based on entropy of model's predictions.\n",
        "\n",
        "    Args:\n",
        "    - entropy (float): The entropy of the model's output.\n",
        "    - base_noise (float): Base noise multiplier.\n",
        "    - max_noise (float): Maximum noise multiplier.\n",
        "    - min_noise (float): Minimum noise multiplier.\n",
        "    - scale_factor (float): Sensitivity of noise adjustment to entropy.\n",
        "\n",
        "    Returns:\n",
        "    - float: Updated noise multiplier.\n",
        "    \"\"\"\n",
        "    # Inverse relationship between entropy and noise multiplier:\n",
        "    # Higher entropy -> Lower noise, Lower entropy -> Higher noise\n",
        "    noise_multiplier = base_noise + scale_factor * (1.0 - entropy)  # Inverse scaling\n",
        "    return max(min_noise, min(noise_multiplier, max_noise))  # Clamp to [min_noise, max_noise]\n",
        "\n",
        "def group_grad_clipping(model, max_norm):\n",
        "    \"\"\"\n",
        "    Applies group-wise gradient clipping to the model's parameters.\n",
        "\n",
        "    Args:\n",
        "    - model: The neural network model.\n",
        "    - max_norm: Maximum allowed norm for gradients in each group.\n",
        "    \"\"\"\n",
        "    # Define groups (you can customize these)\n",
        "    param_groups = {\n",
        "        \"conv_layers\": [],\n",
        "        \"linear_layers\": [],\n",
        "    }\n",
        "\n",
        "    # Assign parameters to groups\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if \"conv\" in name:\n",
        "            param_groups[\"conv_layers\"].append(param)\n",
        "        elif \"linear\" in name:\n",
        "            param_groups[\"linear_layers\"].append(param)\n",
        "        else:\n",
        "            # Add to a default group if needed\n",
        "            pass\n",
        "\n",
        "    # Clip gradients for each group\n",
        "    for group_name, params in param_groups.items():\n",
        "        if params:\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "\n",
        "# Weight Standardization Layer\n",
        "class WeightStandardization(nn.Module):\n",
        "    def __init__(self, conv, eps=1e-5):\n",
        "        super(WeightStandardization, self).__init__()\n",
        "        self.conv = conv\n",
        "        self.eps = eps\n",
        "        self.weight_mean = nn.Parameter(torch.zeros(1, conv.in_channels, 1, 1))\n",
        "        self.weight_std = nn.Parameter(torch.ones(1, conv.in_channels, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.conv.weight\n",
        "        weight = (weight - weight.mean(dim=(1, 2, 3), keepdim=True)) / (weight.std(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
        "        weight = weight * self.weight_std + self.weight_mean\n",
        "        return F.conv2d(x, weight, self.conv.bias, self.conv.stride, self.conv.padding)\n",
        "\n",
        "# Wide Basic Block Definition\n",
        "class WideBasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(WideBasicBlock, self).__init__()\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=in_channels)\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False))\n",
        "        self.group_norm2 = nn.GroupNorm(num_groups=4, num_channels=out_channels)\n",
        "        self.conv2 = WeightStandardization(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.group_norm1(x)))\n",
        "        out = self.conv2(F.relu(self.group_norm2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "# Wide ResNet Model Definition\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        assert ((depth - 4) % 6 == 0), \"Depth should be 6n+4\"\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.in_channels = nStages[0]\n",
        "        self.conv1 = WeightStandardization(nn.Conv2d(3, nStages[0], kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.layer1 = self._wide_layer(WideBasicBlock, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasicBlock, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasicBlock, nStages[3], n, stride=2)\n",
        "        self.group_norm1 = nn.GroupNorm(num_groups=4, num_channels=nStages[3])\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "    def _wide_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.group_norm1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, drop_last=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = WideResNet(depth=16, widen_factor=4, num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Initialize the Cosine Annealing Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
        "\n",
        "base_noise = 1.0\n",
        "max_noise = 2.0\n",
        "min_noise = 0.1\n",
        "scale_factor = 0.5\n",
        "\n",
        "# Initialize the PrivacyEngine\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "# Wrap your model, optimizer, and data loader to make them private\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    target_epsilon=3.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=50,\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    #noise_multiplier=1.0,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier}\\n\")\n",
        "\n",
        "# Parameter Averaging Function\n",
        "def average_model_parameters(models):\n",
        "    # Assume models is a list of models (e.g., a list of models from different workers)\n",
        "    state_dict = models[0].state_dict()\n",
        "    for key in state_dict:\n",
        "        state_dict[key] = torch.stack([model.state_dict()[key].float() for model in models], dim=0).mean(dim=0)\n",
        "    return state_dict\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Compute entropy and update noise multiplier\n",
        "        entropy = compute_entropy(outputs)\n",
        "        optimizer.noise_multiplier = update_noise_multiplier_based_on_entropy(\n",
        "            entropy, base_noise, max_noise, min_noise, scale_factor\n",
        "        )\n",
        "\n",
        "        # Apply grouped gradient clipping\n",
        "        group_grad_clipping(model, max_norm=1.0)\n",
        "\n",
        "        # Step with DP-SGD\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Epoch: {epoch}, Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}% \\n')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training Loop with Parameter Averaging\n",
        "for epoch in range(1, 50):\n",
        "    train_loss, train_accuracy = train(epoch)\n",
        "    test_loss, test_accuracy = test(epoch)\n",
        "\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()  # Adjusts the learning rate based on the cosine schedule\n",
        "\n",
        "    # Average parameters if applicable (for example, after each epoch in distributed setup)\n",
        "    # This assumes you have multiple models (e.g., in a federated setting) and want to average them.\n",
        "    if epoch % 10 == 0:  # Every 10 epochs, average parameters (for example)\n",
        "        print(\"Averaging model parameters...\")\n",
        "        # For now, we just average a single model's state_dict (this is for illustration).\n",
        "        # In a real scenario, you'd average across models from different workers.\n",
        "        model_state_dict = average_model_parameters([model])\n",
        "\n",
        "        # Load averaged parameters back into the model\n",
        "        model.load_state_dict(model_state_dict)\n",
        "    # Query and print the privacy budget spent so far\n",
        "    epsilon_spent = privacy_engine.get_epsilon(delta=1e-5)\n",
        "    # Print statistics for the epoch\n",
        "    epochs.append(epoch)\n",
        "    train_accuracy_vals.append(train_accuracy)\n",
        "    test_accuracy_vals.append(test_accuracy)\n",
        "    train_loss_vals.append(train_loss)\n",
        "    test_loss_vals.append(test_loss)\n",
        "    epsilon_vals.append(epsilon_spent)\n",
        "    print(f'End of Epoch {epoch}: \\n')\n",
        "    print(f'Privacy Budget: Epsilon = {epsilon_spent:.4f} \\n')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}% \\n')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% \\n')\n",
        "    # Log current learning rate (optional)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Learning Rate: {current_lr:.6f} \\n')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Epochs: \", epochs)\n",
        "print(\"Train Accuracy: \", train_accuracy_vals)\n",
        "print(\"Test Accuracy: \", test_accuracy_vals)\n",
        "print(\"Train Loss: \", train_loss_vals)\n",
        "print(\"Test Loss: \", test_loss_vals)\n",
        "print(\"Epsilon: \", epsilon_vals)\n",
        "\n",
        "# After the training loop, plot the graphs\n",
        "\n",
        "# 1. Train Accuracy and Test Accuracy vs Epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy_vals, label='Train Accuracy', color='blue', marker='o')\n",
        "plt.plot(epochs, test_accuracy_vals, label='Test Accuracy', color='green', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Epsilon vs Test Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epsilon_vals, test_accuracy_vals, label='Test Accuracy', color='purple', marker='s')\n",
        "plt.xlabel('Epsilon (Privacy Budget)')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Epsilon vs Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: 3. Train Loss and Test Loss vs Epochs (you can add this to track losses)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_loss_vals, label='Train Loss', color='red', marker='o')\n",
        "plt.plot(epochs, test_loss_vals, label='Test Loss', color='orange', marker='x')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss vs Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T0iQaAGA97ex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "outputId": "8c60dc92-77a6-40b8-9d8f-b95ba679f0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "WARNING:opacus.data_loader:Ignoring drop_last as it is not compatible with DPDataLoader.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using sigma=1.22802734375\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.25 GiB is free. Process 4517 has 13.50 GiB memory in use. Of the allocated memory 11.05 GiB is allocated by PyTorch, and 2.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f8c872e3a6ae>\u001b[0m in \u001b[0;36m<cell line: 262>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;31m# Training Loop with Parameter Averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-f8c872e3a6ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Compute entropy and update noise multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are trying to call the hook of a dead Module!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/grad_sample/grad_sample_module.py\u001b[0m in \u001b[0;36mcapture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mgrad_sampler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_compute_per_sample_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mgrad_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_sampler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             create_or_accumulate_grad_sample(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/grad_sample/functorch.py\u001b[0m in \u001b[0;36mft_compute_per_sample_gradient\u001b[0;34m(layer, activations, backprops)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprepare_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     per_sample_grads = layer.ft_compute_sample_grad(\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/apis.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         return vmap_impl(\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# If chunk_size is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     return _flat_vmap(\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mflat_in_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         )\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mbatched_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unwrap_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/apis.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meager_transforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margnums_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_and_value_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_saved_tensors_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36mgrad_and_value_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0;31m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             flat_grad_input = _autograd_grad(\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0mflat_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_diff_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/eager_transforms.py\u001b[0m in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     grad_inputs = torch.autograd.grad(\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mdiff_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n\u001b[1;32m    495\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.26 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.25 GiB is free. Process 4517 has 13.50 GiB memory in use. Of the allocated memory 11.05 GiB is allocated by PyTorch, and 2.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ]
}